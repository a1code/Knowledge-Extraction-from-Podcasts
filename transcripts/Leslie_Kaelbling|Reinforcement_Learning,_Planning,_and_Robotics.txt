The following is a conversation with Leslie Kaling, she is a roboticist and professor at MIT. she is recognized for her work and reinforcement learning, planning, robot navigation and several other topics and A.I. She won the Educate Computers and Thought Award and was the editor in chief of the prestigious journal Machine Learning Research. This conversation is part of the artificial intelligence podcast at MIT and beyond, if you enjoy it, subscribe on YouTube, iTunes or simply connect with me on Twitter at Lex Friedman spelled F.R. I.D..
And now here is my conversation with Leslie Cabling. What made me get excited about I can say that is I read Girdle Esher when I was in high school. That was pretty formative for me. Because it exposed. The interestingness of primitives in combination and how you can make complex things out of simple parts and ideas of A.I. and what kinds of programs might generate intelligent behavior.
So so you first fell in love with A.I. reasoning, logic versus robots?
Yeah, the robots came because my first job. So I finished my undergraduate degree in philosophy at Stanford and was about to finish my master's in computer science. And I got hired at Ezri in their AI lab and they were building a robot. It was a kind of a follow on to Shaki, but all the shaky people were not there anymore. And so my job was to try to get this robot to do stuff. And that is really kind of what got me interested in robots.
So maybe taking a small step back, your bachelor's and Stanford philosophy did Nasserism in computer science, but the master's in philosophy. So what was that journey like? What elements of philosophy do you think you bring to your work in computer science?
So it is surprisingly relevant. So the part of the reason that I did not do a computer science undergraduate degree was that there was not one at Stanford at the time, but that was part of philosophy. And in fact, Stanford has a special sub majoring in something called Now Symbolic Systems, which is logic, model theory, formal semantics of natural language. And so that is actually a perfect preparation for working A.I. and computer science. That that is kind of interesting.
So if you were interested in artificial intelligence, what what kind of majors were people even thinking about taking?
What is in neuroscience was so besides philosophies, what were we supposed to do if you were fascinated by the idea of creating intelligence?
There were not enough people who did that for that even to be a conversation.
I mean, I think probably probably for philosophy. I mean, it is interesting in my class, my graduating class of undergraduate philosophers, probably maybe slightly less than half went on in computer science, slightly less than half went on in law. And like one or two went on in philosophy. So it was a common kind of connection.
Do you think A.I. researchers have a role, be part time philosophers, or should they stick to the solid science and engineering without sort of taking the philosophising tangents? I mean, you work with robots. You think about what it takes to create intelligent beings. are not you the perfect person to think about the big picture philosophy of it all, the parts of philosophy that are closest to A.I., I think, or at least the closest to that.
I think about our stuff like belief and knowledge and denotation and that kind of stuff. And that is, you know, it is quite formal and it is like just one step away from the kinds of computer science work that we do kind of routinely. I think that there are important questions still about. What you can do with a machine and what you can not and so on, although at least my personal view is that I am completely a materialist, and I do not think that there is any reason why we can not make a robot be behaviorally indistinguishable from a human.
And the question of whether it is distinguishable internally, whether it is a zombie or not, in philosophy terms, I actually do not. I do not know and I do not know if I care too much about that, but there is a philosophical notions there, mathematical and philosophical, because we do not know so much of how difficult that is, how difficult the perception problem, how difficult is the planning problem, how difficult is it to operate in this world successfully because our robots are not currently as successful as human beings in many tasks.
The question about the gap between current robots and human beings borders a little bit on philosophy. You know, the the expanse of knowledge that is required to operate in this world and the ability to form common sense knowledge, the ability to reason about uncertainty. Much of the work you have been doing there is there is open questions there that I do not know require to activate a certain big picture of you.
To me, that does not seem like a philosophical gap at all. To me, it is there is a big technical gap. Yes, huge technical gap. But I do not see any reason why it is more than a technical gap. Perfect.
So when you mention I mentioned NECI and maybe can you describe to me when you first fell in love with robotics, with robots or inspired, which so you should mention flaky or shaky, shaky, flaky.
And what was the robot that first captured your imagination? what is possible?
Right. Well, so the first robot I worked with was like Shaki was a robot that the right people had built. But by the time I think when I arrived, it was sitting in a corner of somebody's office dripping hydraulic fluid into a pan. But it is iconic. And really everybody should read the shaky tech report because it has so many good ideas in it. I mean, they invented a star search and symbolic planning and learning macro operators. They had low level kind of configurations based planning for the robot.
They had vision. They had all the basic ideas of a ton of things.
Estaba shaky have arms. That was the job would go.
Genki was a mobile robot, but it could push objects and so it would move things around with which actuators itself with its base.
OK, so it could but and they had painted the baseboards black. So it used it used vision to localize itself in a map. It detected objects, it could detect objects that were surprising to it. It would plan and replan based on what it saw. It reasoned about whether to look and take pictures. I mean, it really had the basics of so many of the things that we think about now.
How did it represent the space around it?
So it had representations that a bunch of different levels of abstraction. So it had, I think, a kind of an occupancy grid of some sort at the lowest level. At the high level, it was abstract, symbolic kind of rooms and connectivity. So where does flaky come in?
Yeah, OK, so I showed up, but Azariah and the we were building a brand new robot. As I said, none of the people from the previous project were kind of there involved anymore. So we were kind of starting from scratch. And my advisor was Stan Rosenstein. He ended up being my thesis advisor and he was motivated by this idea of situated computation or situated automata. And the idea was. That the tools of logical reasoning were important, but possibly only for the engineers or designers to use in the analysis of the system, but not necessarily to be manipulated in the head of the system itself.
Right. So I might use logic to prove a theory about the behavior of my robot, even if the robot's not using logic and its head to prove there. So that was kind of the distinction. And so the idea was to kind of use those principles, to make robots do stuff. But a lot of the basic things we had to kind of learn for ourselves because I had zero background in robotics, I did not know anything about control. I did not know anything about sensors.
So we reinvented a lot of wheels on the way to getting that robot to do stuff.
Do you think that was an advantage or hindrance? Oh, no. it is I mean, I am big in favor of wheel reinvention, actually. I mean, I think you learn a lot by doing it. Yes. it is important, though, to eventually have the pointers to so that you can see what is really going on. But I think you can appreciate much better the the good solutions once you have messed around a little bit on your own and found a bad one.
Yeah, I think you mentioned reinventing reinforcement learning and referring to rewards as pleasures. I pleasure.
Yeah, I think yeah. Which I think is a nice name for it seems to me it is more, it is more fun almost.
Do you think you could tell the history of AI machine learning, reinforcement learning how you think about it from the 50s to now.
One thing is that it is oscillates, right? So things become fashionable and then they go out and then something else becomes cool and that goes out and so on. And I think there is so there is some interesting sociological process that actually drives a lot of what is going on. Early days was kind of cybernetics and control. Right. And the idea that of homeostasis, people have made these robots that could, I do not know, try to plug into the wall when they needed power and then come loose and roll around and do stuff.
And then I think over time, the thought, well, that was inspiring. But people said, no, no, we want to get maybe closer to what feels like real intelligence or human intelligence. And then maybe the expert systems people tried to do that. But maybe a little too superficially, right, so, oh, we get the surface understanding of what intelligence is like because I understand how a steel mill works and I can try to explain it to you and you can write it down in logic and then we can make a computer and for that.
And then that did not work out. But what is interesting, I think, is when a thing starts to not be working very well, it is not only do we change methods, we change problems. Right. So it is not like we have better ways of doing the problem of the expert systems people are trying to do. We have no ways of trying to do that problem.
Oh, yeah, I know. I think maybe a few, but we kind of give up on that problem and we switch to a different problem. And we we work that for a while and we make it as a broad community, as a community.
And there is a lot of people who would argue you do not give up on the problem. it is just the decrease the number of people work and you almost kind of like put it on the shelf. So we will come back to this 20 years later.
Yeah, I think that is right. Or you might decide that it is malformed, like you might say. it is wrong to just try to make something that does superficial, symbolic reasoning behave like a doctor, you can not do that until you have had the sensory motor experience of being a doctor or something. So there is arguments that say that that is a problem, is not well formed, or it could be that it is well well-formed. But but we just were not approaching it well.
You mentioned that your favorite part of logic and symbolic systems is that they give short names for large sets. So there is some use to this. They use a symbolic reasoning. So looking at expert systems and symbolic computing, what do you think are the roadblocks that were hit in the 80s and 90s?
OK, so right. So the fact that I am not a fan of expert systems does not mean that I am not a fan of some kinds of symbolic reasoning, so.
let us see roadblocks. Well, the main road block, I think, was that the idea that humans could articulate their knowledge effectively into into, you know, some kind of logical statements.
So it is not just the cost, the effort, but really just the capability of doing it right.
Because we are all experts in vision. Right. But totally do not have introspective access into how we do that. Right. And it is true that. I mean, I think the idea was, well, of course, even people then would know, of course, I would not ask you to please write down the rules that you use for recognizing a water bottle. that is crazy. And everyone understood that. But we might ask you to please write down the rules you use for deciding.
I do not know what time to put on or how to set up a microphone or something like that. But even those things, I think people maybe I think what they found I am not sure about this, but I think what they found was that the so-called experts could give explanations that sort of post hoc explanations for how and why they did things, but they were not necessarily very good. And then they they depended on. Maybe some kinds of perceptual things, which, again, they could not really define very well, so I think I think fundamentally I think that the underlying problem with that was the assumption that people could articulate how and why they make their decisions.
And so it is almost an encoding the knowledge for converting from expert to something that a machine can understand and reason with. No, no, no, no.
Not even just encoding, but getting it out of you. Just not not not writing it. I mean, yes, hard also to write it down for the computer. Yeah. But I do not think that people can produce it. You can tell me a story about why you do stuff, but I am not so sure that is the way.
Great. So there are still on the hierarchical planning side places where symbolic reasoning is very useful. So as you have talked about.
So where so do not was the gap.
Yeah. OK, good. So saying that humans can not provide a description of their reasoning processes, that is OK. Fine, but that does not mean that it is not good to do reasoning of various styles inside a computer. Those are just two orthogonal points. So then the question is what kind of reasoning should you do inside a computer? Right. And the answer is, I think you need to do all different kinds of reasoning inside a computer, depending on what kinds of problems you face.
I guess the question is, what kind of things can you encode, symbolic and reasonable?
I think the idea about and and even symbolic, I do not even like that terminology because I do not know what it means technically and formally. I do believe in abstractions. So abstractions are critical, right. You cannot reason completely fine grained about everything in your life. Right. You can not make a plan at the level of images and talks for getting a Ph.D.. Right. So you have to reduce the size of the state's base and you have to reduce the horizon if you are going to reason about getting a Ph.D. or even buying the ingredients to make dinner.
And so. So how can you reduce the spaces and the horizon of the reasoning you have to do? And the answer is abstraction, spatial abstraction, temporal abstraction. I think abstraction along the lines of goals is also interesting, like you might or, well, abstraction. The decomposition goals is maybe more of a decomposition thing. So I think that is where these kinds of if you want to call it symbolic or discrete models come in. You you talk about a room of your house instead of your Poes.
You talk about, you know, doing something during the afternoon instead of it 254. And you do that because it makes your reasoning problem easier and also because you have you do not have enough information to reason in high fidelity about your pose of your elbow at two thirty five this afternoon anyway.
Right. When you are trying to get a Ph.D. that you are doing anything for at that moment, at that moment, you do have to reason about the puzzle, your elbow maybe, but then you maybe you do that in some continuous joint space kind of model.
And so again, I. My biggest point about all of this is that there should be the dogma is not the thing, right? We should not it should not be that I am in favor or against symbolic reasoning. And you are in favor against neural networks. It should be that just just computer science tells us what the right answer to all these questions is if we are smart enough to figure it out. Well, yeah.
When you try to actually solve the problem with computers, the right answer comes out. You mentioned abstractions. I mean, your networks form abstractions or rather there is there is automated ways to form structures and there is expert driven ways to form factions and exploit human driven ways. And humans just seem to be way better at forming abstractions currently and certain problems.
So when you are referring to two forty five R.P.M. versus afternoon, how do we construct that taxonomy? Is there any room for automated construction of such abstractions?
Oh I think eventually, yeah. I mean I think when we get to be better and machine learning, engineers will build algorithms that build awesome abstractions that are useful in this kind of way that you are describing. Yeah. So let us then step from the the abstraction discussion and let us talk about Palm. I am DP's.
Partially observable Markov decision processes, so uncertainty. So first, what are Markov decision processes?
What about maybe how much of our world could be models and how much when you wake up in the morning in making breakfast, how do you think of yourself as an MDP? So how do you think about MDP and how they relate to our world?
Well, so there is a stance question, right? So a stance is a position that I take with respect to a problem. So I as a researcher or person who designed systems, can decide to make a model of the world around me in some terms. And so I take this messy world and I say I am going to treat it as if it were a problem of this formal kind. And then I can apply solution concepts or algorithms or whatever to solve that formal thing.
Right. So of course, the world is not anything. it is not a MDP or a problem. I do not know what it is, but I can model aspects of it in some way or some other way. And when I model some aspect of it in a certain way, that gives me some set of algorithms I can use.
You can model the world in all kinds of ways. Some have some are more accepting of uncertainty, more easily modeling uncertainty of the world.
Some really force the world to be deterministic. And so certainly ADP's model, the uncertainty of the world.
Yes. Model, some uncertainty, the model not present state uncertainty, but they model uncertainty in the way the future will unfold.
So what are murkoff decision process?
So market decision process is a model. it is a kind of model that you could make that says I know completely the current state of my system and what it means to be a state is that I that all the I have all the information right now that will let me make predictions about the future as well as I can so that remembering anything about my history would make my predictions any better.
And but but then it also says that then I can take some actions that might change the state of the world and that I do not have a deterministic model of those changes.
I have a probabilistic model of how the world might change.
it is a it is a useful model for some kinds of systems. I think it is a I mean, it is certainly not a good model for. Most problems, I think, because for most problems, you do not actually know the state for most problems, you it is partially observed. So that is now a different problem class.
So that is where the poverty is a part of the decision process step.
And so how do they address the fact that you can not observe most incomplete information about most of the world around you? Right.
So now the idea is we still kind of postulate that there exists a state. We think that there is some information about the world out there such that if we knew that we could make good predictions, but we do not know the state. And so then we have to think about how. But we do get observations. Maybe I get images right here, things or I feel things, and those might be local or noisy. And so therefore they do not tell me everything about what is going on.
And then I have to reason about, given the history of actions I have taken and observations I have gotten, what do I think is going on in the world? And then given my own kind of uncertainty about what is going on in the world, I can decide what actions to take and saw difficulties.
This problem of planning under uncertainty, in your view and your long experience of modeling the world, trying to deal with this uncertainty in especially in real world systems, optimal planning for even discrete bomb dips can be undecidable depending on how you set it up.
And for so. Lots of people say I do not use palm leaves because they are intractable, and I think that that is a kind of a very funny thing to say because. The problem you have to solve is the problem you have to solve. So if the problem you have to solve is intractable, that is what makes us ehi people, right. So we saw we understand that the problem solving is is wildly intractable, that we can not we will never be able to solve it ultimately.
At least I do not.
Yeah, right. So later we can come back to an idea about bounded optimality something. But anyway, we can not come up with optimal solutions to these problems. So we have to make approximations, approximations in modeling, approximations in solution algorithms and so on. And so I do not have a problem with saying, yeah, my problem actually it is Parmly be in continuous space with continuous observations and it is so computationally complex I can not even think about it is, you know, big or whatever, but that does not prevent me from it helps me, gives me some clarity to think about it that way and to then take steps to make approximation after approximation to get down to something that is like computable in some reasonable time.
When you think about optimality, you know, the community broadly is shifted. And that I think a little bit in how much they value the idea of optimality of chasing as an optimal solution. How is of use of chasing an optimal solution changed over the years?
And when you work with robots, that is interesting. I think we have a little bit of a math for the logical crisis, actually, from the theoretical side. I mean, I do think that theory is important in that right now we are not doing much of it. So there is lots of empirical hacking around and training this and doing that and reporting numbers, but is it good? Is it bad? We do not know. it is very hard to say things. And if you look at like computer science theory, so people talked for a while, everyone was about solving problems optimally or completely, and and then there were interesting relaxations.
So people look at, oh, can I are there regrette browns or can I do some kind of, you know, approximation? Can I prove something that I can approximately solve this problem or that I get closer to the solution as I spend more time and so on?
what is interesting, I think, is that we do not have good approximate solution concepts for. Very difficult problems, right? I like to you know, I like to say that I I am interested in doing a very bad job of very big problems as well.
Right.
So very are very big problems. I like to do that. But I would I wish I could say something. I wish I had a, I do not know, some kind of a of a formal solution concept that I could use to say, oh, this this algorithm actually it gives me something like, I know what I am going to get.
I can do something other than just run it and get out so that that notion is still somewhere deeply compelling to you. The notion that you can say you can drop thing on the table says this. You can expect this is all going to give me some good results.
I hope there is I hope science will. I mean, there is engineering and there is science. I think that they are not exactly the same. And I think right now we are making huge engineering like leaps and bounds. So the engineering is running away ahead of the science, which is cool and often how it goes.
Right. So we are making things and nobody knows how and why they work roughly, but. We need to turn that into science in some form, it is that there is some room for formalizing.
We need to know what the principles are. Why does this work? Why does that not work? I mean, for a while, people build bridges by trying, but now we can often predict whether it is going to work or not without building it. Can we do that for learning systems or for robots?
So your hope is from a materialistic perspective that intelligence, artificial intelligence systems, robots are just more fancier bridges. Believe space. what is the difference between belief, space and state space? You mentioned DP's found DP's reasoning about you since the world. there is a state was this belief space idea?
Yeah, I did get to believe space.
That is instead of thinking about what is the state of the world and trying to control that as a robot, I think about what is the space of beliefs that I can have about the world. If I think of a belief as a probability distribution over ways the world could be a belief state is a distribution. And then my control problem, if I am reasoning about how to move through a world I am uncertain about. My control problem is actually the problem, controlling my beliefs, so I think about taking actions, not just what effect they will have on the world outside, but what effect they will have on my own understanding of the world outside.
And so that might compel me to ask a question or look somewhere to gather information which may not really change the world state, but it changes my own belief about the world.
that is a powerful way to have to empower the agent to reason about the world, to explore the world. To what kind of problems does allow you to solve to to consider believe space vs. just state space?
Well, any problem that requires deliberate information gathering. Right. So if in some problems. Like chess, there is no uncertainty or maybe there is uncertainty about the opponent, there is no uncertainty about the state and some problems, there is uncertainty, but you gather information as you go, right? You might say, oh, I am driving my autonomous car down the road and it does not know perfectly where it is, but the light are all going all the time. So I do not have to think about whether to gather information.
But if you are a human driving down the road, you sometimes look over your shoulder to see what is going on behind you in the lane, and you have to decide whether you should do that now. And you have to trade off the fact that you are not seeing in front of you when you are looking behind you. And how valuable is that information and so on. And so to make choices about information gathering, you have to reasonably space. Also also, I mean, also to just.
Take into account your own uncertainty before trying to do things so you might say, if I understand where I am standing relative to the doorjamb pretty accurately, then it is OK for me to go through the door. But if I am really not sure where the door is, then it might be better to not do that.
Right now, the degree of uncertainty about the world is actually part of the thing you are trying to optimize. And for me. that is right. that is right.
So this idea of a long horizon of planning for a party or just even how to get out of the house or how to make breakfast, you show this presentation of the WTF was the fork of a robot looking at the sink.
And can you describe how we plan in this world of this idea of hierarchical planning?
we have mentioned this is.
Yeah, how can a robot hope to plan about something of this with such a long array where the goal is quite far away?
People, since probably reasoning began, have thought about hierarchical reasoning, the temporal hierarchy and their spatial hierarchy. But let us talk about temporal hierarchy. So you might say, oh, I have this long execution I have to do, but I can divide it into some segments abstractly. Right. So maybe you have to get out of the house. I have to get in the car. I have to drive and so on. And so.
You can plan if you can build abstractions, so this we started out by talking about abstractions that we are back to that now if you can build abstractions in your states face. And abstractions, sort of temporal abstractions, then you can make plans at a high level and you can say, I am going to go to town and then I will have to get gas and then I can go here and I can do this other thing. And you can reason about the dependencies and constraints among these actions.
Again, without thinking about the complete details, what we do in our hierarchical planning work is then say, all right, I make a plan at a high level of abstraction. I have to have some reason to think that it is feasible without working it out in complete detail. And that is actually the interesting step. I always like to talk about walking through an airport. Like you can plan to go to New York and arrive at the airport and then find yourself an office building later.
You can not even tell me in advance what your plan is for walking through the airport, partly because you are too lazy to think about it. Maybe, but partly also because you just do not have the information.
You do not know what get your landing in or what people are going to be in front of you or anything. So there is no point in planning in detail, but you have to have you have to make a leap of faith that you can figure it out once you get there. And it is really interesting to me how you arrive at that.
How do you so you have learned over your lifetime to be able to make some kinds of predictions about how hard it is to achieve some kinds of goals. And that is critical. Like you would never plan to fly somewhere if you could not did not have a model of how hard it was to do some of the intermediate steps. So one of the things we are thinking about now is how do you do this kind of very aggressive generalization? To situations that you have not been in, and I want to predict how long will it take to walk through the Kuala Lumpur airport like you give me an estimate and it would not be crazy and you have to have an estimate of that in order to make plans that involve walking through the Kuala Lumpur airport, even if you do not need to know it in detail.
So I am really interested in these kinds of abstract models and how do we acquire them. But once we have them, we can use them to do hierarchical reasoning, which I think is very important.
Yeah, there is this notion of go regression and premix back changing this idea of starting at the goal and just form these big clouds of states.
I mean, it is almost like saying to the airport, you know, you know, once you show up to the airport that you are like a few steps away from the goal. So, like, think of it this way is kind of interesting.
I do not know if you have sort of further comments on that sort of starting to go with.
Yeah. I mean, it is interesting that Simon Herb Simon, back in the early days way, I did talked a lot about means and the reasoning and reasoning back from the goal. there is a kind of an intuition that people have that. The number of the states space is big, the number of actions you could take is really big. So if you say here I sit and I want to surge forward from where I am, what are all the things I could do that is just overwhelming?
If you say if you can reason at this other level and say, here is what I am hoping to achieve, what can I do to make that true, that somehow the branching is smaller? Now, what is interesting is that, like in the planning community, that has not worked out in the class of problems that they look at in the methods that they tend to use, it has not turned out that it is better to go backward. Um, it is still kind of my intuition that it is.
But I can not prove that to you right now.
Right. And share your intuition, at least for us mere humans. Speaking of which, when you maybe know, take it and take a little step into that philosophy circle, how hard would it, when you think about human life, is you give those examples often, how hard do you think it is to formulate human life as a planning problem or aspects of human life?
So when you look at robots, you are often trying to think about object manipulation tasks, about moving a thing.
When you take a slight step outside the room, let the robot leave and go get lunch or maybe try to pursue more fuzzy goals.
How hard do you think is that problem? If you were to try to maybe put another way, try to form a human life as as a planning problem? Well, that would be a mistake.
I mean, it is not all a planning problem, right? I think it is really, really important that we understand that you have to put together pieces and parts that have different styles of reasoning and representation and learning, I think. I think it is it is probably clear to anybody that that it can not all be this or all be that brains are not all like this are all like that. Right. They have different pieces and parts and substructure and so on.
So I do not think that there is any good reason to think that there is going to be like one true algorithmic thing that is going to do the whole job, just a bunch of pieces together designed to solve a bunch of specific problems, one specific or maybe styles of problems.
I mean, there is probably some reasoning that needs to go on in image space, I think, again. there is this model based versus model free idea. it is own reinforcement learning people talk about, oh, should I learn? I could learn a policy just straight up a way of behaving. I could learn it is popular on a value function. that is some kind of weird intermediate ground. Or I could learn a transition model which tells me something about the dynamics of the world.
If I could imagine that I learn a transition model and I couple it with a planner and I draw a box around that, I have a policy again, it is just stored a different way. Right, right, it is and but it is just as much of a policy as the other policy, it is just I have made I think the way I see it is it is a time space trade off in computation right up, more overt policy representation. Maybe it takes more space, but maybe I can compute quickly what action I should take.
On the other hand, maybe a very compact model of the world dynamics. Plus a planner let us me compute what action to take to just more slowly. there is no I mean, I do not think there is no argument to be had.
it is just like a question of what form of computation is best for us, for the various sub problems.
Right. So and so like learning to do algebra manipulations for some reason is I mean, that is probably going to want naturally a sort of a different representation than riding a unicycle at the time. Constraints on the unicycle or serious spaces, maybe smaller, I do not know.
But so I and I could be the more human size of falling in love, having a relationship that might be another, uh. Yeah, another. No idea how to model that.
Yeah. that is that is first solve the algebra and object manipulation.
So what do you think is harder perception or planning.
Perception.
that is my understanding, that is why. So what do you think is so hard about perception by understanding the world around you?
Well, I mean, I think the big question. Is the representational a hugely the question as representation so.
Perception has made great strides lately, right, and we can classify images and we can play certain kinds of games and predict how to steer the car and all that sort of stuff.
I do not think we have a very good idea of. What perception should deliver, right? So if you if you believe in modularity, OK, there is there is a very strong view which says. We should not build in any modularity, we should make a giant I can not take neural network, train it into and to do the thing and that is the best way for it. And it is hard to argue with that except on a sample complexity basis. Right. So you might say, oh, well, if I wanted to do end to end, first of all, learning on this giant giant neural network, it is going to take a lot of data and a lot of like broken robots.
So. Then the only answer is to say, OK, we have to build something in building some structure or some bias, we know from theory of machine learning the only way to cut down the sample complexity is to kind of cut down somehow cut down the hypothesis space.
You can do that by building in bias. there is all kinds of reason to think that nature built bias into humans. Convolution is a bias, right? it is a very strong bias and it is a very critical bias. So my own view is that we should look for more things that are like convolution, but that address other aspects of reasoning. Right? So convolution helps us a lot with a certain kind of spatial reasoning. that is quite close to the imaging, I think.
there is other ideas like that. Maybe some amount of research, maybe some notions of abstraction, maybe the notion that objects exist, actually, I think that is pretty important and a lot of people will not give you that to start with. Right.
So almost like a convolution in the in the object semantic object space of some kind, some some kind of idea there.
that is right. And people like the graph graph convolutions are an idea that are related to relational representations. And so so I think there are so you I have come far afield from perception, but I think I think the thing that is going to make perception that kind of the next step is actually understanding better what it should produce. Right. What are we going to do with the output of it? Right. it is fine when what we are going to do with the output is stear.
it is less clear when we are just trying to make a one integrated, intelligent agent. what is the output of perception? B, We have no idea. And how should that hook up to the other stuff? We do not know. So I think the pressing question is what kinds of structure can we build in there, like the moral equivalent of convolution that will make a really awesome superstructure that then learning can kind of progress on efficiently?
I agree a very compelling description of actually where we stand with the perception of them. you are teaching a course on a body intelligence. What do you think it takes to build a robot with human level intelligence? I do not know if we knew we would do it if you were to.
I mean, OK, so do you think a robot needs to have, uh, self-awareness, uh, consciousness, the fear of mortality? Or is it is it simpler than that? Or is consciousness a simple thing?
Like do you think about these notions?
I do not think much about consciousness. Even most philosophers who care about it will give you that. You could have robots that are zombies. Right, that behave like humans but are not conscious. And I at this moment would be happy enough with that. So I am not really worried one way or the other.
To the technical side, you are not thinking of the use of self-awareness.
Well, but I mean, OK, but then what is self-awareness mean? I mean that you need to have some part of the system that can observe other parts of the system and tell whether they are working well or not. That seems critical. So does that count? Does I mean, does that kind of self-awareness or not? Well, it depends on whether you think that there is somebody at home who can articulate whether they are self aware. But clearly, if I have, like, you know, some piece of code that is counting how many times this procedure gets executed, that is a kind of self-awareness.
Right. So there is a big spectrum. it is clear you have to have some of it. Right.
You know, quite far away. I many dimensions.
But is the direction of research that is most compelling to you for, you know, trying to achieve human level intelligence in robots?
Well, to me, I guess the thing that seems most compelling to me at the moment is this question of what to build in. I want to learn. I think. we are we do not we are missing a bunch of ideas and and we, you know, people, you know, do not you dare ask me how many years it is going to be until that happens, because I will not even participate in the conversation, because I think we are missing ideas and I do not know how long it is going to take to find them.
So I want to ask you how many years, but maybe I will ask you. What it when you will be sufficiently impressed that we have achieved it, so what is what is a good test of intelligence? Do you like the Turing test, the natural language and the robotic space? Is there something where you would sit back and think, oh, that is that is pretty impressive as a test, as a benchmark? Do you think about these kinds of problems?
No, I resist. I mean, I think all the time that we spend arguing about those kinds of things could be better spent just making the robots work better so you do not value competition.
So I mean, the nature of benchmark AI benchmarks and data sets or Turing test challenges where everybody kind of gets together and tries to build a better robot because they want to outcompete each other like the diaper challenge of the autonomous vehicles. Do you see the value of that? I can get in the way I think you can in the way I mean, some people many people find it motivating and so that is good. I find it a.. Motivating, absolutely.
Yeah. Uh, but I think what I mean, I think you get an interesting cycle where for a contest, a bunch of smart people get super motivated and they hacked their brains out. And much of what gets done is just hacks. But sometimes really cool ideas emerge and then that gives us something to chew on after that. So it is not a thing for me, but I do not I do not regret that other people do it.
Yeah, it is like you said, with everything else in the mix is good. So jumping topics a little bit.
You started the journal Machine Learning Research and served as its editor in chief.
How did the publication come about and what do you think about the current publishing model? Space and machine learning, artificial intelligence distinction?
OK, good. So it came about because there was a journal called Machine Learning, which still exists, which was owned by Kluwer. And there was I was on the editorial board and we used to have these meetings annually where we would complain to Clowery that it was too expensive for the libraries and that people could not publish.
And we would really like to have some kind of relief on those fronts. And they would always sympathize but not do anything. So we just decided to make a new journal. And there was the Journal of AI Research, which has was on the same model which had been in existence for maybe five years or so, and it was going on pretty well. So, uh, we just made in a journal. It was not I mean, I do not know, I guess it was work, but it was not that hard.
So basically, the editorial board, probably 75 percent of the editorial board of the Machine Learning resigned. And we found it then this new journal.
But it was sort of was more open. Yeah, right.
So it is completely open. it is open access, actually. Uh, I had a postdoc, George, candidates who wanted to call these journals free for all.
Uh, because there were I mean, it both has no page charges and has no, uh, access restrictions.
And the reason and so lots of people I mean, there were there were people who are mad about the existence of this journal who thought it was a fraud or something. It would be impossible, they said, to run a journal like this with basically I mean, for a long time, I did not even have a bank account. I paid for the lawyer to incorporate and the IP address and just did it cost a couple hundred dollars a year to run?
it is a little bit more now, but not that much more. But that is because I think computer scientists are competent and autonomous in a way that many scientists in other fields are not. I mean, doing these kinds of things, we already typeset around papers. We all have students and people who can hack a website together in afternoon. So the infrastructure for us was like not a problem, but for other people in other fields, it is a harder thing to do in this kind of open access journals.
And there is nevertheless one of the most prestigious journals. So it is not like prestige and it can be achieved without any paper is not required for prestige.
it is out. Yeah. So on the review process side of actually a long time ago, I do not remember when I reviewed a paper where you were also a reviewer and I remember reading your view of being influenced by it and it was really well written. It influenced how I write teacher reviews.
You disagreed with me actually, and you made it my review, but much better so.
But nevertheless, the review process, you know, has its flaws. And how do you think what do you think works? Well, how can it be improved?
So actually, when I started, Jameelah, I wanted to do something completely different. And I did not because it felt like we needed a traditional journal of record, and so we just made Jama'a be almost like a normal journal except for the open access parts of it.
Basically, increasingly, of course, publication is not even a sensible word. You can publish something by putting it in archives so I can publish everything tomorrow.
So making stuff public is there is no barrier. We still need curation and evaluation. I do not have time to read all of archive.
And you could argue that. Kind of social thumbs up being of articles suffices, right, you might say, oh, heck with this, we do not need journals at all. we will put everything on archives and people will upvote and downvote the articles. And then your CV will say, oh, man, he got a lot of upvotes. So that is good.
Um, but I think there is still. Value in careful reading and commentary of things, and it is hard to tell when people are up voting and down voting or arguing about your paper on Twitter and Reddit, whether they know what they are talking about. Right. So then I have the second order problem of trying to decide whose opinions I should value and such. So I do not know what I if I had infinite time, which I do not and I am not going to do this because I really want to make robots work.
But if I felt inclined to do something more in a publication direction, I would do this other thing which I thought about doing the first time, which is to get together some set of people whose opinions I value and who are pretty articulate. And I guess we would be public, although we could be private, I am not sure. And we would review papers, we would not publish them and you would not submit them. We would just find papers and we would write reviews and we would make those reviews public.
And maybe if you you know, so we are Leslie's friends who review papers and maybe eventually if if we our opinion was sufficiently valued, like the opinion of Jameelah is valued, then you would say in your CV that Leslie's friends gave my paper a five star rating and that would be just as good as saying I got it, you know, accepted into this journal. So I think I think we should have good public commentary and organize it in some way, but I do not really know how to do it.
it is interesting times.
The way the way you describe it actually is really interesting. I mean, we do have movies and TV that there is experts, critics come in, they write reviews, but there is also regular. Yeah, non critics. You must write reviews and they are separated. I like open review.
The, the, the I, I clear process. I think it is interesting.
it is a step in the right direction, but it is still not as compelling as a reviewing movies or video games. I mean sometimes almost it might be silly as this from my perspective to say, but it boils down to the user interface, how fun and easy it is to actually perform the reviews, how efficient, how much you as a reviewer get street cred for being a good reviewer.
Those are those human elements come into play.
Now, it is a big investment to do a good review of a paper and the flood of papers is out of control. Right. So, you know, there are not three thousand I do not know how many movies are there in a year? I do not know. But that is going to be less than how many machine learning papers are in a year now.
And I am worried, you know, I. I am I right. So I am like an old person. So of course I am going to say, rah rah rah rah rah. Things are moving too fast.
I am a stick in the mud so I can say that my particular flavor of that is I think the horizon for researchers has gotten very short, that students want to publish a lot of papers and there is a huge there is value, it is exciting and there is value in that. And you get patted on the head for it and so on. But and some of that is fine. But I am worried that we are driving out. People who would spend two years thinking about something I am back in my day when we worked on our theses, we did not publish papers.
You did your thesis for years. You picked the hard problem and then you worked and chewed on it and did stuff and waste of time and for a long time. And when it was roughly when it was done, you would write papers. And so I I do not know how to answer. And I do not think that everybody has to work in that mode. But I think there is some problems that are hard enough that it is important to have a longer research horizon.
And I am worried that we do not incentivize that at all at this point in this current structure.
Right. So what do you see as what are your hopes and fears about the future of A.I. and continue on this theme?
So I had gone through a few winters and ups and downs, D.C., another winter of I coming. And are you more hopeful about making robots work, as you said?
I think the cycles are inevitable, but I think each time we we get higher. Right. I mean, you know, it is like climbing some kind of landscape with a noisy optimizer. Yeah. So it is clear that the the you know, the deep learning stuff has made deep and important improvements.
And so the high watermark is now higher, there is no question. But of course, I think people are overselling and eventually investors like us and other people will look around and say, well, you are not quite delivering on this grand claim.
And that wild hypothesis is that probably it is going to crash some amount and then it is OK. I mean, but I do not I can not imagine that there is like some awesome monotonic improvement from here to human level AI.
So in you know, I have to ask this question. I probably anticipate answers the answers.
But do you have a worry short term or long term about the existential threats of AI and maybe short term, less existential by more robots taking away jobs?
Hmm.
Well, actually, let me talk a little bit about utility. Actually, I had an interesting conversation with some military ethicists who wanted to talk to me about autonomous weapons and there they were, interesting, smart, well educated guys who did not know too much about AI or machine learning. And the first question they asked me was, has your robot ever done something you did not expect? And I, like, burst out laughing because anybody who is ever done something with the robot.
Right. Knows that they do not do it. And what I realized was that their model of how we program a robot was completely wrong. Their model of how we can program a robot was like Lego Mindstorms like, oh, go for it, a meter turn left, take a picture, do this, do that. And so if you have that model of programming, then it is true. it is kind of weird that your robot would do something that you did not anticipate.
But the fact is actually so now this is my new educational mission. If I have to talk to nonexperts, I try to teach them the idea that we do not operate.
We operate at least one or maybe many levels of abstraction above that. And we say, oh, here is a hypothesis class. Maybe it is a space of plans or maybe it is a space of classifiers or whatever, but there is some set of answers and an objective function.
And then we work on some optimization method that tries to optimize a solution, a solution in that class. And we do not know what solution is going to come out right, so I think it is important to communicate that. So, of course, probably people who are listening to this, they they know that lesson. But I think it is really critical to communicate that lesson. And then lots of people are now talking about, you know, the value alignment problem.
So you want to be sure as robots or software systems get more competent, that their objectives are aligned with your objectives or that our objectives are compatible in some way or we have a good way of mediating when they have different objectives. And so I think it is important to start thinking in terms like you do not have to be freaked out by the robot apocalypse to accept that it is important to think about it. Doctor functions of value alignment. Yes. And that you have to really everyone who is done optimization knows that you have to be careful what you wish for, that, you know, sometimes you get the optimal solution and you realize that was that objective was wrong.
So pragmatically, in the short term, it seems to me that that those are really interesting and critical questions. And the idea that we are going to go from being people who engineer algorithms to being people who engineer objective functions, I think that is that is definitely going to happen. And that is going to change our thinking and methodology.
And so we are going to. You started a Stanford philosophy. that is where I will go back to philosophy maybe. Well, I mean, they are mixed together because because as we also know, as machine learning people. Right. When you are designing in fact, this is the lecture I gave in class today. When you design an objective function, you have to wear both hats. there is the hat that says, what do I want? And there is the hat that says, but I know what my optimizer can do to some degree and I have to take that into account.
So it is it is always a trade off and we have to kind of be mindful of that.
The part about taking people's jobs. I understand that that is important. I do not understand. Sociology or economics or people very well, so I do not know how to think about that, so that is yeah. So there might be a sociological aspect there, the economic aspect that is very difficult to think about.
I mean, I think other people should be thinking about it, but I am just that is not my strength.
So what do you think is the most exciting area of research in the short term for the community and for you? For yourself?
Well, so, I mean, there is the story I have been telling you about. How to engineer intelligent robots. So that is what we want to do. We all kind of want to do well. I mean, some of us want to do this. And the question is, what is the most effective strategy? And we have tried and there is a bunch of different things you could do at the extremes. Right. One super extreme is we do introspection and we write a program, OK, that has not worked out very well.
Another extreme is we take a giant bunch of neural goo and we try to train it up to do something. I do not think that is going to work either. So the question is, what is the middle ground? And and again, this is not a theological question or anything like that. it is just like going how do you just how do we what is the best way to make this work out? And I think it is clear it is a combination of learning. To me, it is clear it is a combination of learning and not learning and lots of that combination.
B and what is the stuff we are building? So to me, that is the most compelling question.
And when you say engineer robots, you mean engineering systems that work in the real world.
Is that that is the emphasis. Last question, which robots or robot is your favorite from science fiction? So you can go with Star Wars, R, R D2, or you can go with more modern, uh, maybe how?
I do not think I have a favorite robot from science fiction. This is this is back to you like to make robots work in the real world here. Not, not.
And I mean, I love the process. I care more about the process.
The process. Yeah. I mean, I do research because it is fun, not because I care about what we produce.
Well, that is that is a beautiful night, actually. And Leslie, thank you so much for talking today. Sure. it is been fun.
