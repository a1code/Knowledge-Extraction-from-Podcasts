The following is a conversation with Melanie Mitchell. she is a professor of computer science at Portland State University and an external professor at Santa Fe Institute. She has worked on and written about artificial intelligence from fascinating perspectives, including adaptive, complex systems, genetic algorithms, and the copycat cognitive architecture, which places the process of analogy making at the core of human cognition. From her doctoral work with her advisers, Douglas Hofstadter and John Holland to today, she has contributed a lot of important ideas to the field of AI, including her recent book simply called Artificial Intelligence A Guide for Thinking Humans.
This is the Artificial Intelligence Podcast. If you enjoy, subscribe on YouTube, give it five stars, an Apple podcast supported on Patrón or simply connect with me on Twitter. Àlex Friedman spelled Afridi's man. I recently started doing ads at the end of the introduction. I will do one or two minutes after introducing the episode and never any ads in the middle. They can break the flow of the conversation. I hope that works for you and does not hurt the listening experience.
I provide timestamps for the start of the conversation, but it helps if you listen to the ad and support this podcast by trying out the product or service being advertised. This show was presented by Kashyap, the number one finance app in the App Store. I personally use cash to send money to friends, but you can also use it to buy, sell and deposit Bitcoin in just seconds. Kashyap also has a new investing feature. You can buy a fraction of a stock, say, one dollars worth, no matter what the stock price is.
Broker's services are provided by cash up investing, a subsidiary of Square and member SIPC. I am excited to be working with cash out to support one of my favorite organizations called First Best known for their first robotics and Lego competitions. They educate and inspire hundreds of thousands of students and over 110 countries and have a perfect rating. And Charity Navigator, which means that donated money is used to maximum effectiveness. When you get cash app from the App Store or Google Play and use Leks podcast, you will get ten dollars in cash.
I will also donate ten dollars. The first, which again is an organization that I have personally seen, inspire girls and boys to dream of engineering a better world. And now here is my conversation with Melanie Mitchell. The name of your new book is Artificial Intelligence subtitle A Guide for Thinking Humans, the name of this podcast is Artificial Intelligence. So let me take a step back and ask the old Shakespeare question about roses.
And what do you think of the term artificial intelligence for our big and complicated and interesting field? I am not crazy about the term.
I think it has a few problems because it means so many different things to different people. And intelligence is one of those words that is not very clearly defined either. there is so many different kinds of intelligence, degrees of intelligence, approaches to intelligence. John McCarthy was the one who came up with the term artificial intelligence.
And from what I read, he called it that to differentiate it from cybernetics, which was another. Related movement at the time, and he later regretted calling it artificial intelligence.
Herbert Simon was pushing for calling it complex information processing, which got nixed.
But, you know, probably is equally vague, I guess.
Is it the intelligence or the artificial in terms of words that I think most problematic, would you say?
Yeah, I think it is a little of both, but, you know, it has some good size because I personally was attracted to the field because I was interested in Fanaa phenomenon of intelligence.
And if it was called complex information processing, maybe I would be doing something wholly different now.
What do you think of I have heard the term used cognitive systems, for example.
So using cognitive. Yeah, I mean, cognitive has certain associations with it and people like to separate things like cognition and perception, which I do not actually think are separate. But often people talk about cognition as being different from sort of other aspects of of intelligence. it is sort of higher level.
So to cognition is this broad, beautiful ness of things that encompasses the whole thing.
Memory? Yeah, I think it is hard to draw lines like that. When I was coming out of grad school in the night in 1990, which is when I graduated, that was during one of the EHI Winters, and I was advised to not put A.I. artificial intelligence on my CV, but instead call it intelligent systems.
So that was kind of a euphemism.
Yes. What about the stick briefly and in terms and words, the idea of artificial general intelligence or or like John Rikoon prefers human level intelligence sort of starting to talk about ideas that. That achieve higher and higher levels of intelligence and somehow artificial intelligence seems to be a term used more for the narrow, very specific applications of AI and sort of the worst set of terms. Appeal to you to describe the thing that perhaps we strive to create. People have been struggling with this for the whole history of the field.
And defining exactly what it is that we are talking about, you know, John Searle had this distinction between strong A.I. and weaker and weaker Heyy could be generally, but his his idea was strong. A.I. was the view that a machine is actually thinking. That as opposed to. Simulating thinking or. Carrying out. Intelligence processes that we would call intelligent. At a high level, if you look at the founding of the field, McCarthy, Searle and so on.
Are we closer to having a better sense of that line between narrow, weak eye and strong and. Yes, I think we are closer to having a better. Idea of what that line is early on, for example, a lot of people thought that playing chess would be. You could not play chess if you did not have sort of general human level intelligence, and of course, once computers were able to play chess better than humans, that revised that view and people said, OK, well, maybe now we have to revise what we think of intelligence as.
And so that is kind of been a.
Theme throughout the history of the field is that once a machine can do some task, we then have to look back and say, oh, well, that changes my understanding of what intelligence is, because I do not think that machine is intelligent. At least that is not what I want to call intelligence.
Do you think that line moves forever or will we eventually really feel as a civilization like we have crossed the line if it is possible?
it is hard to predict, but I do not see any reason why we could not in principle. Create something that we would consider intelligent. I do not know how we will know for sure. Maybe our own view of what intelligence is will be refined more and more until we finally figure out what we mean when we talk about it.
But I I think eventually we will create machines in a sense that have intelligence, they may not be the kinds of machines we have now. And one of the things that that is going to produce is making us sort of understand our own machine like qualities that we, in a sense. Our mechanical in the sense that like cells, cells are kind of mechanical, they they have algorithms, they process information by and somehow out of this massive cells, we get this emergent property that we call intelligence.
But underlying it is. Really just cellular processing and lots and lots and lots of it. Do you think we will be able to do you think it is possible to create intelligence without understanding our own mind? He said sort of in that process will understand more and more. But do you think it is possible to sort of create without really fully understanding from a mechanistic perspective, sort of from a functional perspective, how our mysterious mind works? If I had to bet on it, I would say, no, we we do have to understand our own minds, at least to some significant extent, but I think that is a really big open question.
I have been very surprised at how far kind of brute force approaches based on, say, big data and huge networks can can take us. I would not have expected that. And they have nothing to do with the way our minds work. So that is been surprising to me.
So it could be wrong to explore the psychological and the philosophical. Do you think we are OK as a species with something that is more intelligent than us? Do you think perhaps the reason we are pushing that line further and further is we are afraid of acknowledging that there is something stronger, better, smarter than us humans?
Well, I am not sure we can define intelligence that way because, you know, smarter than is with with respect to what what you know, computers are already smarter than us.
In some areas. They can multiply much better than we can.
They they can figure out driving routes to take much faster and better than we can. They have a lot more information to draw than they know about traffic conditions and all that stuff.
So for any given particular task, sometimes computers are much better than we are and we are totally happy with that. Right? I am totally happy with that. I do not bother me at all. I guess the question is, you know what? Which things about our intelligence would we? Feel very sad or upset that machines had been able to recreate so in the book, I talk about my former adviser, Douglas Hofstetter, who encountered a music generation program, and that was really the line for him, that if a machine could create beautiful music, that would be.
Terrifying for him, because that is something he feels is really at the core of what it is to be human, creating beautiful music, art, literature, I, I do not think. He does not like the fact that. Machines can.
Recognize spoken language really well, like he does not he personally does not like using speech recognition, but I do not think it bothers him to his core because it is like, OK, that is not at the core of humanity, but it may be different for every person. What what really? They feel would usurp their humanity, and I think maybe it is a generational thing also, maybe our children or our children's children will be adapted, that they will adapt to these new devices that can do all these tasks and say, yes, this thing is smarter than me in all these areas.
But, uh, that is great because it helps me.
Looking at the broad history of our species, what do you think so many humans have dreamed of creating artificial life and artificial intelligence throughout the history of our civilization. So not just this century or the 20th century, but really many throughout many centuries that preceded it.
that is a really good question, and I have wondered about that. Because I I myself was driven by curiosity about my own thought processes and thought it would be fantastic to be able to get a computer to mimic some of my thought processes. I am not sure why we are so driven, I think. We want to. Understand ourselves better. And we also want machines to do things for us, but I do not know, there is something more to it because it is so deep in the kind of mythology or the ethos of our our species.
And I do not think other species have this drive.
So I do not know if you were to sort of psychoanalyze yourself in your in your own interest. And I, I you.
What excites you about creating intelligence? You said understanding our own cells. Yeah, I think that is what drives me, particularly I am. Really interested in human intelligence. But I am I am also interested in the sort of the phenomenon of intelligence more generally, and I do not think humans are the only thing with intelligence, you know, and or even animals. But I think intelligence. Is a concept that encompasses a lot of complex systems, and if you think of things like insect colonies or cellular processes or the immune system or all kinds of different biological or even societal processes have as an emergent property some aspects of what we would call intelligence.
You know, they have memory, they do process information, they have goals, they accomplish their goals, etc. And to me, that the question of what is this thing we are talking about here was really fascinating to me. And exploring it, using computers seem to be a good way to approach the question.
So do you think kind of intelligence do you think of our universe as a kind of hierarchy of complex systems and then intelligence is just the property of any you can look at any level and every level has some aspect of intelligence. So we are just like one little speck in that giant hierarchy of complex systems. I do not know if I would say any system like that has intelligence, but I guess what I want to I do not have a good enough definition of intelligence to say that.
So let me let me do sort of multiple-choice, I guess, though, you said and colonies. So are in colonies intelligent. Are the bacteria in our body intelligent? And then look going to the the physics world molecules and the behavior at the quantum level of of electrons and so on, are those kinds of systems, do they possess intelligence like war?
Where is a line that feels compelling to you?
I do not know. I mean, I think intelligence is a continuum and I think that the ability to in some sense have intention, have a goal, have have some kind of self awareness is part of it. So I am not sure if, you know, it is hard to know where to draw that line.
I think that is kind of a mystery. But I would not say that say that, you know, this the planets orbiting the Sun are is an intelligent system. I mean, I would find that maybe not the right term to describe that.
And this is you know, there is all this debate in the field of like what is what is the right way to define intelligence? what is the right way to model intelligence? Should we think about computation? Should we think about dynamics? And should we think about, you know, free energy and all that stuff? And I think that it is it is a fantastic time to be in the field because there is so many questions and so much we do not understand. there is so much work to do.
So are we are we the most special kind of intelligence in this kind of. You said there is a bunch of different elements and characteristics of intelligence systems and colonies, as it are, his human intelligence. The thing in our brain is that the most interesting kind of intelligence in this continuum?
Well, it is interesting to us because because it is us.
I mean, interesting to me. Yes. And because I am part of, you know, human.
But to understanding the fundamentals of intelligence, what I am doing is studying the human is sort of if everything we have talked about, we talk about in your book, what just the AI field, this notion.
Yes, it is hard to define, but it is usually talking about something that is very akin to human intelligence.
Yeah, to me, it is the most interesting because it is the most complex.
I think it is the most self-aware. it is the only system, at least that I know of that reflects on its own intelligence.
And you talk about the history of AI and us in terms of creating artificial intelligence, being terrible at predicting the future with AI, with tech in general. So. Why do you think we are so bad at predicting the future, are we hopelessly bad so no matter what? Well, there is this decade or the next two decades, every time we make a prediction, there is just no way of doing it well or as the field matures, will be better and better at it.
I believe as the field matures, we will be better. And I think the reason that we have had so much trouble is that we have so little understanding of our own intelligence. So there is the famous story about. Marvin Minsky. Assigning computer vision as a summer project to his undergrad students, and I believe that is actually true story.
Yeah, there is a there is a write up that everyone should read. it is like a I think it is like a proposal that describes everything that should be done in that project is hilarious because it I mean, you can explain. But my sort of recollection, it describes basically all the fundamental problems of computer vision, many of which still have not been solved. Yeah.
And I do not know how far they really expect it to get, but I think that and they are really, you know, Marvin Minsky, super smart guy and very sophisticated thinker.
But I think that no one really understands or understood still does not understand how complicated, how complex. The things that we do are because they are so invisible to us, you know, to us vision, being able to look out at the world and describe what we see, that is just immediate.
It feels like it is no work at all. So it did not seem like it would be that hard. But there is so much going on unconsciously, sort of invisible to us that. I think we overestimate how. Easy, it will be to get computers to do it and sort of for me to ask an unfair question. you have done research. You have thought about many different branches of AI through this book, widespread looking at where it has been and where it is today.
If you were to make a prediction, how many years from now would we as a society create something that you would say achieved human level intelligence? Or superhuman level intelligence. That is an unfair question. A prediction that will most likely be wrong, so but it is just your notion because. OK, I will say. I will say more than one hundred years, more than one hundred years, and there I quoted somebody in my book who said that human level intelligence is 100 Nobel Prizes away, which I like because it is a it is a nice way to to sort of it is a nice unit for prediction.
And it is like that many.
Fantastic discoveries have to be made and of course, there is no Nobel Prize in. I right.
Not yet, look at the one hundred years your sense is really the journey to intelligence has to go through something, something more complicated that is akin to our own cognitive systems, understanding them, being able to create them in in artificial systems, as opposed to sort of taking the machine learning approach approaches of today and really scaling them and scaling them and scaling them exponentially with both computer and hardware and and data.
That would be my that would be my guess. You know, I think that in in the sort of going along the narrow A.I. that these current the current approaches will get better. You know, I think there is some fundamental limits to how far they are going to get. I might be wrong, but that is what I think. And there is some fundamental weaknesses that they have that I talk about in the book that just comes from this approach of of supervised learning. we are requiring sort of feed forward networks and so on.
it is just I do not think it is a sustainable approach to understanding the world.
I am personally torn on it, sort of ViiV everything you read about in the book and sort of we are talking about now. I agree. I agree with you, but I am more and more depending on the day. First of all, I am deeply surprised by the success of machine learning and deep learning in general from the very beginning when I was it is really been my focus of work.
I am just surprised how far it gets. And I also think we are really early on in these efforts of these narrow. So I think there will be a lot of surprises of how far it gets, I think will be extremely impressed. Like I my sense is everything I have seen so far and we will talk about autonomous driving and so on, I think we can get really far. But I also have a sense that we will discover, just like you said, is that even though we will get really far to in order to create something like our own intelligence is actually much farther than we realize.
Right. I think these methods are a lot more powerful than people give them credit for, actually. So then, of course, there is the media hype. But I think there is a lot of researchers in the community, especially like not undergrads. Right. But like people who have been and I they are skeptical about how far you can get. And I am more and more thinking that it can actually get farther than them realize. it is certainly possible. One thing that surprised me when I was writing the book is how far apart different people are in the field are using their opinion of how how far the field has come and what is accomplished and what is what is going to happen next.
what is your sense of the different who are the different people, groups, mindsets, thoughts in the community about where I is today?
Yeah, they are all over the place, so there is there is kind of the the Singularity Transhumanism Group, I do not know exactly how to characterize that approach, which says, well, yeah, the sort of exponential, exponential progress we are on, the sort of almost at the the hugely accelerating part of the exponential.
And by in the next 30 years, we are going to see Superintelligent, A.I and all that and we will be able to upload our brains and that.
So there is that kind of extreme view that most I think most people who work in I do not have.
They disagree with that.
But there are people who who are. Maybe, you know, singularity people, but but they are they do think that the current approach of deep learning is going to scale and is going to kind of go all the way basically and take us to true A.I. or human level AI or whatever you want to call it. And there is quite a few of them. And a lot of them. Like a lot of the people I met who work at. Big tech companies in aid groups kind of have this view that we are really not that far, you know, just to linger on that point, sort of give and take as an example, Giannakou, I do not know if you know about his work and viewpoints on this.
I do. He believes that there is a bunch of breakthroughs like fundamental like Nobel Prizes. Yeah, it is still right. But I think he thinks those breakthroughs will be built on top of deep learning. Right.
And then there is some people who think we need to kind of put deep learning to the side a little bit as just one module that is helpful in the bigger cognitive framework.
Right. So so so I think from what I understand, John McCain is. Rightly seen supervised learning is not sustainable, we have to figure out how to do unsupervised learning, but that is going to be the key and. You know, I think that is probably true, I think unsupervised learning is going to be harder than people think.
I mean, the way that we humans do it, then there is the opposing view. You know, there is the the Gary Marcus kind of hybrid view where where deep learning is one part.
But we need to bring back kind of these symbolic approaches and combine them.
Of course, no one knows how to do that very well, which is the more important part, right. To emphasize and how do they how do they fit together? what is what is the foundation? what is the thing that is on top of the cake was the icing. Right?
Then there is people pushing different different things. there is the people, the causality people who say, you know, deep learning as it is formulated today completely lacks any notion of causality and that is dooms it.
And therefore, we have to somehow give it some kind of notion of causality. there is. A lot of. Push from the more cognitive science crowd saying. We have to look at developmental learning. We have to look at how babies learn. We have to look at intuitive physics. All these things we know about physics and somebody kind of quipped, we also have to teach machines intuitive metaphysics, which means like objects exist, the causality exists.
You know, these things that maybe we are born with, I do not know that that they do not have the machines do not have any of that. You know, they look at a group of pixels and they maybe they get. Ten million examples, but they. They can not necessarily learn that there are objects in the world. So there is just a lot of pieces of the puzzle that people are promoting and with different opinions of like how, how, how important they are and how close we are to being able to put them all together to create general intelligence.
Looking at this broad field. What do you take away from it? Who is the most impressive is that the cognitive folks, the Gary Marcus camp, the John camp unsupervised and the supervised, there is the supervisor and then there is the engineers who are actually building systems.
You have sort of the Andre Carpathia Tesla building actual you know, it is not philosophise real systems that operate in the real world.
What what do you take away from all of this?
I mean, I do not know if, you know, these these different views are not necessarily mutually exclusive. And I think people like John McCain. Agrees with the developmental psychology of causality, intuitive physics, etc., but he still thinks that it is learning like and and learning is the way to go. we will take us perhaps all the way. Yeah. And that we do not need. there is no sort of innate stuff that has to get built in.
This is you know, it is because it is a hard problem. I personally, you know, I am very sympathetic to the cognitive science side because that is kind of where I came in to the field. I have become more and more sort of an embodiment, adherent to say that without having a body, it is going to be very hard to learn what we need to learn about the world.
there is something I would love to talk about in a little bit to step into the cognitive world then, if you do not mind, because you have done so many interesting things.
If we look to copycat taking a couple of decades, step back. You, Douglas Hofstadter and others have created and developed copycat more than 30 years ago.
Oh, that is painful. There the what is it what is or what is copycat? it is a program that makes analogies in an idealized domain, idealized world of letter strings.
So as you say, 30 years ago. Wow. So I started working on it when I started grad school in 1984. Wow dates me, and it is based on Doug Hofstadter's ideas that about that analogy is really a core aspect of thinking.
I remember he has a really nice quote in the book by by himself and Emmanuel Sanders called Surfaces and Essence's I do not know if you have seen that book, but it is about analogy.
And he says without concepts, there can be no thought and without analogies, there can be no concepts.
So the view is that analogy is not just this kind of reasoning technique where we go, you know, shoe is to foot as glove is to what you know, these kinds of things that we have on IQ tests or whatever that but that it is much deeper.
it is much more pervasive in every thing we do, in every our language, our our thinking, our perception.
So we so he had a view that was a very active perception idea. So the idea was that instead of having kind of a passive network in which you have input that is being processed through these friede forward layers and then there is an output at the end, that perception is really a dynamic process. You know, we are like our eyes are moving around and they are getting information and that information is feeding back to what we look at next influences, what we look at next and how we look at it.
And so copycat was trying to do that, kind of simulate that kind of idea where you have these.
Agents was kind of an agent based system, and you have these agents that are picking things to look at and deciding whether they were interesting or not, whether they should be looked at more and that would influence other agents.
Now, how do they interact? So they interacted through this global kind of what we call the workspace.
So it was actually inspired by the old Blackbaud Systems where you would have agents that post information on a blackboard, a common blackboard.
This is like all very old fashioned asset that we are talking about, like in physical spaces as a computer programs or programs or agents posting concepts on a blackboard.
Yeah, we called it a workspace. And it it is the workspace is a data structure. The agents are little pieces of code that you can think of them as detect little detectors or little filters that say, I am going to pick this place to look and I am going to look for certain things.
This is the thing I I think is important is that they are so it is almost like, you know, the convolution in a way, except a little bit more general and saying and then highlighting it on the on the in the workspace.
what is it once it is in the workspace, how do the things that are highlighted relate to each other?
So there is different kinds of agents that can build connections between different things. So so just to give you a concrete example, what copycat did was it it made analogies between strings of letters. So here is an example. ABC changes to Abdeh. What does it change to? And the program had some prior knowledge about the alphabet, knew the sequence of the alphabet. It had a concept of letter, successor of letter.
It had concepts of sameness. So it had some innate. Things programmed in, but then it could do things like. They discover that ABC is a group of letters in succession and then an agent can mark that. So the idea that there could be. A sequence of letters, is that a new concept that is formed or that is a concept, that is a concept that is innate sort of can you form new concepts or those?
And so in this program, all the concepts of the program were innate. So because because we were not I mean, obviously that limits set quite a quite a bit. But what we were trying to do is say, suppose you have some innate concepts.
How do you flexibly apply them to new situations and how do you make analogies? let us step back for a second. I really like that quote that you said. Without concepts, there can be no thought, without analogies. That can be no concepts. In a in a Santa Fe presentation, you said that it should be one of the mantras of AA. Yes. And that you also yourself said how to form and fluidly use concepts as the most important open problem.
And I.
Yes. How to form and fluidly use concepts is the most important open problem, and so that is what is a concept and what is an analogy, a concept is in some sense a fundamental unit of thought.
So say we have a. Concept of. A dog, OK? And a concept is embedded in a whole space of concepts so that there is certain concepts that are closer to it or farther away from it.
Are these concepts, are they really like fundamental, like you mentioned? And they look almost like axiomatically, very basic. And then there is other stuff built on top of it which include everything is are the complicated.
Like, you can certainly have formed new concepts, right? I guess that is the question. Yeah. Can you form new concepts that are commonly complex combinations of other kinds? Yes, absolutely.
And that is kind of what we we do in learning.
And then what is the role of analogies in that? So analogy is when you recognize that one situation. Is essentially the same as another situation and essentially is kind of the keyword there, because it is not the same. So if I say. Last week, I did a podcast interview in actually like three days ago in Washington, D.C., and that situation was very similar to this situation, although it was not exactly the same. You know, it was a different person sitting across from me.
We had different kinds of microphones. The questions were different. The building was different. there is all kinds of different things.
But really, it was analogous or I can say so.
So doing a podcast interview, that is kind of a constant it is a new concept. You know, I never had that concept before.
So essentially, I mean and I can make an analogy with it, like being interviewed for a news article in a newspaper. And I can say, well, you kind of play the same role that the the newspaper, the reporter played.
it is not exactly the same because maybe they actually emailed me some written questions rather than talking and the writing.
The written questions, you know, are analogous to your spoken questions.
And, you know, there is just all kinds of things somehow probably connects to conversations you have over Thanksgiving dinner and just general conversations. there is like a thread you can probably take that just stretches out in all aspects of life that connect to this podcast. I mean, sure. Conversations between humans. Sure.
And if I go and tell a friend of mine about this podcast interview, my friend might say, oh, the same thing happened to me. You know, let us say, you know, you ask me some really hard question and I have trouble answering it.
My friend could say the same thing happened to me, but it was like it was not a podcast interview.
It was not a it was a completely different situation. And yet my friend is seeing essentially the same thing. You know, we say that very fluidly.
The same thing happened to me, essentially the same thing. We do not even say that. Right. you are just imply.
Yes. Yeah.
And the view that kind of what went into, say, copycat, that that whole thing is that that that that act of saying the same thing happened to me is making an analogy.
And in some sense, that is what underlies all of our concepts.
Why do you think analogy making that you are describing is so fundamental to cognition? Like it seems like it is the main element action of what we think of as cognition. Yeah, so it can be argued that all of this. Generalization, we do have concepts. And recognizing concepts in different situations. Is done by analogy. That that is. Every time I am recognizing that, say. you are a person.
that is by analogy because I have this concept of what person is and I am applying it to you, and every time I recognize a new situation, like one of the things I talked about it in the book was that the concept of walking a dog, that that is actually making an analogy because all of the you know, the details are very different.
So so the reasoning could be reduced down to essentially analogy making to all the things we think of as like, yeah, like you said, perception.
So perception is taking raw sensory input and it is somehow integrating into our our understanding of the world, updating the understanding and all of that has just this giant mess of analogies that are being made.
I think so, yeah. If you just linger on it a little bit. Like what? What do you think it takes to engineer a process like that for us in our artificial systems?
We need to understand better, I think how. How we do it, how humans do it. And it comes down to internal models. I think, you know, people talk a lot about mental models that concepts or mental models that I can. In my head, I can do a simulation of a situation like walking a dog and that there is some work in psychology that. Promotes this idea that all of concepts are really mental simulations, that whenever you encounter a concept or situation in the world or you read about it or whatever, you do some kind of mental stimulation that allows you to predict what is going to happen to to develop expectations of what is going to happen.
So that is the kind of structure I think we need, is that kind of mental model that and in our brain, somehow these mental models are very much inter connected again.
So stuff we are talking about are essentially open problems. Right. So if I ask a question, I do not mean that you would know the answer. I am really just hypothesizing.
But how big do you think is the the network graph data structure of concepts that is in our head? Like if we are trying to build that ourselves, like it is we take it.
that is one of the things we take for granted, we think. I mean, that is why we take common sense for granted with common sense.
it is trivial, but how big of a thing of concepts is that underlies what we think of as common sense, for example? Yeah, I do not know, and I am not I do not even know what units to measure it in. And you say how big is the roof we put right with?
But, you know, we have you know, it is really hard to know. We have a.
What, 100 billion neurons or something, I do not know. And they are connected via trillions of synapses and there is all this chemical processing going on.
there is just a lot of capacity for stuff. And their information is encoded in different ways in the brain.
it is encoded in chemical interactions, is encoded in electric, like firing and firing rates.
And and nobody really knows how it is encoded. But it just seems like there is a huge amount of capacity. So I think it is it is huge. it is just enormous. And it is amazing how much stuff we know. Yeah.
And but we know and not just know like facts, but it is all integrated into this thing that we can make analogies with. Yes. there is a dream of semantic web and there is there is a lot of dreams from exper systems of building giant knowledge bases.
Do you see a hope for these kinds of approaches of building, of converting Wikipedia into something that could be used in analogy, making sure and I think people have made some progress along those lines.
I mean, people have been working on this for a long time.
But the problem is and this, I think is the problem of common sense, like people have been trying to get these common sense networks here at MIT. there is this concept net project. Right.
But the problem is that, as I said, most of the. Knowledge that we have is invisible to us, it is not in Wikipedia. it is very basic things about. You know, intuitive physics, intuitive psychology. Intuitive of metaphysics, all that stuff, if you were to create a website that described intuitive physics into the psychology, would it be bigger or smaller than Wikipedia? What do you think? I guess describe to whom. I am sorry, but that is that is really good, you know.
Yeah, that is a hard question because, you know, how do you represent that knowledge is the question. Right. I can certainly write down F equals M and O Newton's laws. And a lot of physics can be deduced from that. But that is probably not the best representation of that knowledge for for doing the kinds of reasoning we want a machine to do so. So I do not know.
it is it is impossible to say now. And people you know, the projects like there is a famous the famous psych project.
Right, that Doug Douglas Ollivant did that was trying, still going, I think is still going. And if the idea was to try and encode all of common sense knowledge, including all this invisible knowledge in some kind of logical representation, and it just never.
I think I could do any of the things that he was hoping it could do, because that is just the wrong approach, of course, that is what they always say, you know, and then the history books will say, well, the psych project finally found a breakthrough in 2058 or something.
And it you know, we are so much progress has been made in just a few decades that knows what the next breakthroughs will be. It could be it is certainly a compelling notion what the psych project stands for.
I think it was one of the earliest people to say common sense is what we need and that is what we need.
All this like expert system stuff that is not going to get you to A.I. You need common sense.
And he basically gave up his whole. Academic career to to go pursue that, and I totally admire that, but I think that the approach itself. Will not in twenty, twenty, twenty four. What do you think is wrong with the approach? What kind of approach would might be successful? Well, again, that he knows the answer. I knew that, you know, one of my talks, one of the people in the audience was a public lecture.
One of the people in the audience said, what A.I. companies are you investing in, like investment advice? I am a college professor for one thing, so I do not have a lot of extra funds to invest.
But also, like, no one knows what is going to work in Haiti. Right. that is the problem.
Let me ask another impossible question in case you have a sense in terms of data structures that will store this kind of information, do you think they have been invented yet, both in hardware and software? Or something else needs to be are we told, you know, I think something else has to be invented. I that is my guess, is the breakthrough's the most promising, would that be in hardware and software? Do you think we can get far with the current computers or do we need to do something that is saying.
I do not know if turning computation is going to be sufficient, probably, I would guess it will.
I do not see any reason why we need anything else.
But so so in that sense, we have invented the hardware we need, but we just need to make it faster and bigger and we need to figure out the right algorithms and the right sort of architecture.
TURING That the very mathematical notion when we have to build intelligence is now an engineering notion where you throw all that stuff.
Well, I guess I guess it is a it is a question. The people have brought up this question, you know, and when you asked about is our current hardware. Will our current hardware work well, turn computation says that, like our current hardware. He is in principle, a Turing machine, right? So all we have to do is make it faster and bigger. But there have been people like Roger Penrose, if you might remember that he said Turing machines cannot produce intelligence because intelligence requires continuous valued numbers.
I mean, that was sort of my reading of his argument and quantum mechanics and what else?
Whatever, you know, but I do not see any evidence for that, that we need new computation paradigms. But I do not know if we are you know, I do not think we are going to be able to scale up our current approaches to programming these computers.
What is your hope for approaches like copycat or other cognitive architectures? I have talked to the creator of SOAR, for example. I have used that car myself. I do not know if you are familiar with. Yeah, yeah.
What do you think is what is your hope of approaches like that in helping develop systems of greater and greater intelligence in the coming decades? Well, that is what I am working on now, is trying to take some of those ideas and extending it, so I think. There are some really promising approaches that are going on now that have to do with more active generative models. So this is the idea of this simulation in your head of a concept when you. If you want to when you are perceiving the new situation, you have some simulations in your head, those are generative models are generating your expectations or generating predictions.
So that is part of a perception. You have a mental model that generates a prediction and then you compare it with. Yeah. And then the difference.
And you also that that generative model is telling you where to look and what to look at and what to pay attention to. And I think it affects your perception. it is not that just you compare it with your perception.
It it becomes your perception in a way. It is kind of a mixture of of the bottom up. Information coming from the world and your top down model being imposed on the world is what becomes your perception.
So your hope is something like that can improve perception systems and that they can understand things better. Yes. Yes.
what is the what is the step? Was the analogy making stuff there?
Well, there the idea is that you have this pretty complicated conceptual space.
You know, you can talk about a semantic network or something like that with these different kinds of concept models in your brain that are connected. So so let us let us take the example of walking a dog.
We were talking about that. OK, let us say I see someone out in the street walking a cat. Some people walk their cats. I guess this seems like a bad idea, but yeah.
So my model of my you know, there is connections between my model of a dog and model of a cat.
And I can immediately see the analogy of a.
That those are analogous situations, but I can also see the differences and that tells me what to expect. So also, you know, I have a new situation.
So another example with the walking the dog thing is sometimes people I see people riding their bikes with a leash, holding a leash and the dogs running alongside.
OK, so I know that the I recognize that as kind of a dog walking situation, even though the person's not walking right in. The dog's not walking because I, I have these models that say, OK, riding a bike is sort of similar to walking or it is connected, it is a means of transportation. But I because they have their dog there, I assume they are not going to work, but they are going out for exercise. And, you know, these analogies help me to figure out kind of what is going on, what is likely.
But sort of these analogies are very human interpretable. Mm hmm. So that is the kind of space. And then you look at something like the current deep learning approaches that kind of help you to take raw sensory information and to sort of automatically build up hierarchies of of of what you call them, concepts. they are just not human interpretive concepts.
what is your what is the link here, do you hope? it is sort of the hybrid system question, how do you think that you can start to meet each other with the value of learning in this systems, the forming of analogy, making the goal of, you know, the original goal of deep learning in at least visual perception was that you would get the system to learn to extract features that at these different levels of complexity.
So maybe edge detection and that would lead into learning simple combinations of edges and then more complex shapes and then whole objects or faces and. This was based on the ideas of the neuroscientist's Hubel and Wiesel, who had seen laid out this kind of structure and brain. And I think that is that is right to some extent, of course, people have found that the whole story is a little more complex than that in the brain, of course, always is. And there is a lot of feedback.
And so I see that. As as absolutely a. A good brain inspired approach to some aspects of perception, but one thing that it is lacking. For example, is all of that feedback? Which is extremely important.
The interactive element you mentioned, the the expectation, the conceptual level going back and forth with the the the expectation, the perception and yes, going back and forth.
So. Right. So that is extremely important. And, you know, one thing about deep neural networks is that in a given situation, like, you know, they are trained, right?
They get these weights and everything.
But then now I give them a new a new image, let us say. Yes, they. Treat every part of the image in the same way. You know, they apply the same filters at each layer to all parts of the image. there is no feedback to say like, oh, this part of the image is irrelevant, right?
I should not care about this part of the image or this part of the image is the most important part. And that is kind of what we humans are able to do because we have these conceptual expectations. So there is, by the way, a little bit work in that. there is certainly a lot more in what is under the convention in natural language processing nowadays. it is it is a and that is exceptionally powerful. And it is a very just as you say, is a really powerful idea.
But again, in sort of machine learning, it all kind of operates in an automated way. that is not human.
it is not. it is not also. OK, so you are right, it is not dynamic. I mean, in the sense that as a perception of a new example is being.
Processed. Those are tensions, weights do not change, right? So, I mean, there is a. This kind of notion that there is not a memory, so you are not aggregating the idea of this mental model?
Yes, yeah, that seems to be a fundamental idea. there is not a really powerful I mean, there is some stuff with memory, but there is not a powerful way to represent the world in some sort of way.
that is deeper than I mean, it is so difficult because, you know, neural networks do represent the world.
They do have a mental model. Right. But it just seems to be shallow. it is it is hard to it is hard to criticize them at the fundamental level.
To me, at least, it is easy to it is it is easy to criticize and will look like. Exactly. you are saying mental models, sort of almost from a psycho, put a psychology head on, say, look, these networks are clearly not able to achieve what we humans do with forming mental models, but the analogy making so on. But that does not mean that they fundamentally cannot do that. it is very difficult to say that I mean this to me. Do you have a notion that the learning approaches really?
I mean, they are going to not not only are they limited today, but they will forever be limited in being able to construct such mental models.
I think the idea of the dynamic. Perception is key here, the idea that. Moving your eyes around and getting feedback, and that is something that, you know, there is been some models like that, there is certainly recurrent neural networks that operate over several times steps.
And but the problem is that the actual the recurrence. Is. You know, basically the the feedback is to the next time step is the entire hidden state of the network, which which is it turns out that that is. That does not work very well, but see, here is the thing I am saying is mathematically speaking, it has the information in that recurrence to capture everything. It just does not seem to work. Yeah. So, you know, it is like it is the same Turing machine question, right?
Yeah. Maybe theoretically a computer is anything that is during a university machine can can be intelligent, but practically the architecture might be have be very specific kind of architecture to be able to create it.
So just I guess it is sort of ask almost the same question again is how big of a role do you think deep learning needs will play or needs to play in this in perception?
I think deep learning as it is currently. As it currently exists, you know, we will play that kind of thing, we will play some role. And, uh, but I think that there is a lot more going on in perception, but who knows, you know, the definition of deep learning. I mean, it is pretty broad. it is kind of an umbrella for it. So what I mean is purely sort of neural networks. Yeah. And a feed forward neural networks, essentially.
Or there could be a recurrence. But yeah, sometimes it feels like for us I talk to Gary Marcus. It feels like the criticism of deep learning is kind of like us birds criticizing airplanes for not flying well or that they are not really flying.
Do you think deep learning, do you think it could go all the way like little things?
Do you think that, yeah, the brute force learning approach can go all the way?
I do not think so, no. I mean, I think it is an open question, but I tend to be on the neatness side that there has, that there is some things that. we have been evolved to be able to learn and that learning just can not happen without them.
So so one example here is here is an example I had in the book that that I think is useful to me, at least in thinking about this, so that this has to do with the deep mind Atari game playing program.
OK, and learn to play these Atari video games just by getting input from the pixels of the screen. And it learned to play the game, break out a thousand percent better than humans. OK, that was one of the results and it was great. And it learned this thing where it tunneled through the side of the of the bricks in the breakout game and the ball could bounce off the ceiling and then just wipe out bricks.
OK, so. There was a group who did an experiment where they took the paddle, you know, that you move with the joystick and moved it up to pixels or something like that.
And then they they looked at a deep cue learning system that had been trained on breakout and said, could it now transfer? it is learning to this new version of the game. Of course, a human could that and it could maybe that is not surprising.
But I guess the point is it had not learned the concept of a paddle. It had not learned that it had not learned the concept of a ball or the concept of tunneling. It was learning something. We caught we looking at it kind of.
Anthropomorphized it and said, oh, here is what it is doing and the way we describe it, but it actually did not learn those concepts. And so because it did not learn those concepts, it could not make this transfer. Yes.
So that is a beautiful statement. But at the same time, by moving the paddle, we also anthropomorphize flaws to inject into the system that will then flip out. How impressed we are by what I mean by that is to me, the Atari games were to me deeply impressive that that was possible at all.
So the first pause on that and people should look at that just like the game of go, which is fundamentally different to me then than what JetBlue did. Even though there is still magic, there is still three search. it is just everything deep mine is done in terms of learning, however limited, it is still deeply surprising to me. Yeah, I am not I am not trying to say that what they did was not impressive.
I think it was incredibly impressive to me. it is interesting is moving the board just another level. Another thing that needs to be learned. So like we have been able to maybe maybe been able to, through the criminal networks, learn very basic concepts that are not enough to do this general reasoning and maybe with more data.
I mean, the did the you know, the interesting thing about the examples that you talk about and beautifully is that it is often flaws of the data.
Well, that is the question. I mean, I think that is the key question is whether it is a flaw of the data or not or the metrics, because we have always the reason I brought up this example was because you were asking, do I think that, you know, learning from data could go all the way?
Yes. And this was why I brought up the example, because I think and this is not at all to. To take away from the impressive work that they did, but it is to say that when we look at what these systems learn. Do they learn the human, the things that we humans consider to be the relevant concepts and in that example, it did not? Yes, sure.
If you train it on a movie, the paddle being in different places, maybe it could deal with maybe it would learn that concept, I am not totally sure. But the question is scaling that up to more complicated worlds. To what extent could a machine that only gets this very raw data learn to divide up the world into relevant concepts? And I do not know the answer, but I would bet that that that without some innate notion that it can not do it.
Yeah. Ten years ago, I 100 percent agree with you as the most espersen system. But now I have a one like I have a glimmer of hope.
OK, that is fair enough. And I think I think that is a deep learning in the community is I still if I had to bet all my money one hundred percent deep learning will not take us all away, but there is still other still.
I was so personally sort of surprised by the targets, by go by the by the power of self play, of just game playing that I was like many other times just humbled of how little I know about what is possible. And I think fair enough. Self play is amazingly powerful. And, you know, that is that goes way back to Arthur Samuel, right.
With his checker plane program and that which was brilliant and surprising that it did so well.
So just for fun, let me ask you, on the topic of autonomous vehicles, it is the area that that I work, at least these days most closely on. And it is also an area that I think is a good example that you use as sort of an example of things. We as humans do not always realize how hard it is to do, like the concentration or the different problems that we think are easy when we first try them and then we realize how hard it is.
OK, so why you have talked about this autonomous driving being a difficult problem, more difficult than we realize humans give it credit for. Why is it so difficult? What are the most difficult parts in your view? I think it is difficult because of the world is so Open-Ended, as to what kinds of things can happen, so.
You have sort of what normally happens, which is as you drive along and nothing, nothing surprising happens and autonomous vehicles can do, the ones we have now evidently can do really well on most normal situations, as long as long as, you know, the weather is reasonably good and everything.
But if some we have this notion of educate or, you know, things in the tail of the distribution, you would call it the long tail problem, which says that there is so many possible things that can happen that was not in the training data of the machine that. It will not be able to handle it because it does not have common sense, right? it is still the paddle moved. Yeah, it is the paddle moved from the right. And so my understanding and you probably are more of an expert than I am on this, is that.
Current self-driving car vision systems have problems with obstacles, meaning that they do not know which obstacles, which quote unquote obstacles they should stop for and which ones they should not stop for. And so a lot of times I read that they tend to slam on the brakes quite a bit. And the most common accident with self-driving cars are people rear ending them because they were surprised they were not expecting the machine, the car to stop. Yeah.
So there is there is a lot of interesting questions there whether. Because you measure kind of two things, so one is the problem of perception, of understanding, of interpreting the objects that are detected correctly, and the other one is more like the policy, the action that you take or how you respond to it. So a lot of the cars breaking is a kind of notion of to clarify, there is a lot of different kind of things that are people calling autonomous vehicles.
But a lot of the L for vehicles with a safety driver are the ones like Wimoweh and crews and those companies, they tend to be very conservative and cautious.
So they tend to be very, very afraid of hurting anything or anyone and getting in any kind of accidents.
So their policy is very kind of that that results in being exceptionally responsive to anything that could possibly be an obstacle.
Right. Which which which the human drivers around it. it is unpredictable, it behaves unpredictably, that is not a very human thing to do caution, that is the thing we are good at, especially in driving. we are in a hurry, often angry and etc., especially in Boston.
So and then there is sort of another and a lot of times that is machine learning is not a huge part of that.
it is becoming more and more unclear to me how much, you know, sort of speaking to public information, because a lot of companies say they are doing deep learning and machine learning just attract good candidates. The reality is, in many cases, it is still not a huge part of the of the perception. there is light and there is other sensors that are much more reliable for optical detection. And then there is Tesla approach, which is vision only. And there is a few companies doing that test the most sort of famously pushing that forward.
And that is because the light is too expensive, right? Well, I mean. Yes, but I would say if you were to for free give to every test vehicle, I mean, Elon Musk fundamentally believes that Leider is a crutch, right? Fantasy. Said that. That. If you want to solve the problem of machine learning, Lydda is not should not be the primary sensor is the belief, OK, the camera contains a lot more information.
Mm hmm. So if you want to learn, you want that information. But if you want not hit obstacles, you want like sort of it is this weird trade off because. Yeah, so what does the vehicles have a lot of, which is really the thing.
The the the fall-back the primary fallback sensor is radar, which is a very crude version of Lider. it is a good detector of obstacles, except when those things are standing right. The stopped vehicle. Right. that is why it had problems with crashing into to stop fire trucks, stop fire trucks.
So the hope there is that the vision sensor would somehow catch that. And for there is a lot of problems with perception.
I think they are doing actually some incredible stuff in the. Almost like an active learning space where it is constantly taking edge cases and pulling back in, there is a data pipeline, another aspect. That is really important that people are studying now is called multitask learning, which is sort of breaking apart this problem, whatever the problem is in this case, driving into dozens or hundreds of little problems that you can turn into learning problems. So this giant pipeline that, you know, it is kind of interesting.
I have I have been skeptical from the very beginning, but become less and less skeptical over time. How much of driving we learned. I still think it is much farther than than the CEO of that particular company thinks it will be. But it is surprising that through good engineering and data collection and active selection of data, how you can attack that long tail. it is an interesting open question that you are absolutely right. there is a much longer tail, all these cases that we do not think about.
But it is this is a fascinating question that applies to natural language in all spaces. How big how how big is the long tail?
Right.
And I mean, not to linger on the point, but what is your sense in driving? In these practical problems of the human experience, can it be learned so the current what are your thoughts of sort of Elon Musk thought, let us forget the thing that he says will be solved in a year, but can it be solved in? In a reasonable timeline or two, fundamentally, other matters need to be invented. So I do not I think that. Ultimately driving, so is the trade off in a way, you know, being able to drive and deal with any situation that comes up does require kind of full human intelligence.
And even in humans are not intelligent enough to do it because humans I mean, most human accidents are because the human was not paying attention or the humans drunk or whatever, and not because they were not intelligent and not because they were not intelligent enough.
Right. Whereas the accidents with autonomous vehicles is because they were not intelligent enough. they are always paying attention.
they are always paying attention. So so it is a trade off, you know. And I think that it is a very fair thing to say that autonomous vehicles will be ultimately safer than humans because humans are very unsafe.
it is kind of a low bar.
But just like you said that, I think he has got a bad rap. Right, because we are really good at the common sense thing.
Yeah, we are great at the common sense thing.
we are bad at the paying attention thing, paying attention to things, especially more, you know, driving is kind of boring and we have these phones to play with and everything.
But I think. What what is going to happen is that for many reasons, not just EHI reasons, but also like legal and other reasons, that.
The definition of self driving is going to change autonomous, it is going to change, it is not going to be just.
I am going to go to sleep in the back, and you just drive me anywhere. it is going to be more.
Certain areas are going to be instrumented to have the sensors and the mapping and all the stuff you need for that, that the autonomous cars will not have to have for common sense and they will do just fine in those areas as long as pedestrians do not mess with them too much. that is another question.
But. I do not think we will have. Fully autonomous self driving in the way that, like most, the average person thinks of it for very long time.
And just to reiterate, this is the interesting open question that I think I agree with you on, is to solve fully autonomous driving. You have to be able to engineer in common sense.
Yes, I think that is an important thing to hear and think about. I hope that is wrong.
But I currently agree with you that unfortunately, you do have to have to be more specific, sort of these deep understandings of physics and yeah, of of the way this world works and also the human dynamics that you mentioned, pedestrians and cyclists, actually, that is whatever that nonverbal communication is, some people call it, there is that dynamic that is also part of this common sense. Right.
And we are pretty we humans are pretty good at predicting what other humans are going to do and how our actions impact the behaviors of so weird game theoretic dance that we are good at somehow.
And the funny thing is, because I have watched countless hours of pedestrian video and talk to people, we humans are also really bad at articulating the knowledge we have. Right. Which has been the huge challenge. Yes. So you have mentioned embodied intelligence. What do you think it takes to build a system of human level intelligence? Does it need to have a body?
I am not sure, but I am coming around to that more and more.
And what does it mean to be I do not mean to keep bringing up Yallock.
And he looms very large.
Well, he certainly has a large personality. Yes. He thinks that the system needs to be grounded, meaning it needs to sort of be able to interact with reality, but does not think it necessarily needs to have a body.
So when you think of what is the difference, I guess I want to ask, when you mean body, do you mean you have to be able to play with the world? Or do you also mean like there is a body that you that you have to preserve? that is a good question. I have not really thought about that, but I think both, I would guess because. I think you.
I think intelligence, it is so hard to to separate it from our self, our desire for self-preservation, our emotions are all that non rational stuff that kind of gets in the way of logical thinking because we.
The way. You know, we are talking about human intelligence or human level intelligence, whatever that means, a huge part of it is social.
That, you know, we were evolved to be social and to deal with other people, and that is just so ingrained in us that it is hard to separate intelligence from that. I I think, you know, I for the last 70 years or however long it is been around, it has largely been separate. there is this idea that there is like this kind of very well Cartesian.
there is this, you know, thinking thing that we are trying to create, but we do not care about all this other stuff.
And I think the other stuff is very fundamental.
So there is a idea that things like emotion get in the way of intelligence as opposed to being an integral part, integral part of it.
So, I mean, I am Russian, so romanticized the notions of emotion and suffering and all that kind of, uh, fear of mortality, those kinds of things.
So I especially sort of by the way, did you see that there is this recent thing going around the Internet of this? Some I think he is a Russian or some Slavic that had written this thing sort of anti the idea of superintelligence. I forgot maybe he is Polish anyway. So it all these arguments and when one was the argument from Slavic pessimism, my favorite.
Do you remember what the argument is? Just it is like nothing ever works.
Think exactly.
So what do you think is the role like?
that is such a fascinating idea that the what we perceive as sort of the limits of human of the human mind, which is emotion and fear and all those kinds of things are integral to intelligence. Could could you elaborate on that?
Like what? Why is that important, do you think? For human level intelligence. At least the way the humans work, it is a big part of how it affects how we perceive the world, it affects how we make decisions about the world, it affects how we interact with other people. It affects our understanding of other people, you know. For me to understand your. What you are going what you are likely to do, I need to have kind of a theory of mind, and that is.
Very much a theory of emotion and motivations and goals and and to understand that I.
You know, we have this whole system of mirror neurons.
We you know, I sort of understand your motivations through sort of simulating it myself.
So, you know, it is not something that I can prove. that is necessary, but it seems very likely so, OK, you have written the op ed in The New York Times titled We should not Be Scared by Superintelligent A.I, and it criticized a little bit, too, Russell Nick Bostrom. Can you try to summarize that article's key ideas? So it was spurred by an earlier New York Times op ed by Stewart Russell, which was summarizing his book called Human Compatable.
And the article was saying, you know, if we if we have superintelligent A.I, we need to have its values aligned with our values and it has to learn about what we really want. And he gave this example. What if we have a super intelligent A.I. and we give it the problem of solving climate change and it decides that the best way to lower the carbon in the atmosphere is to kill all the humans. OK, so to me, that just made no sense at all because a super intelligent A.I., first of all, thinking, trying to figure out what what superintelligence means.
And it does not. It seems that.
it is something that superintelligent. can not just be intelligent along this one dimension of, OK, we are going to figure out all the steps, the best optimal path to solving climate change and not be intelligent enough to figure out that humans do not want to be killed, that you could get to one without having the other.
And, you know, Bostrom in his book talks about the orthogonality hypothesis where he says he thinks that a systems.
I can not remember exactly what it is, but like a systems goals and its values do not have to be aligned. there is some orthogonality there which did not make any sense to me.
So you are saying that in any system that is sufficiently, not even superintelligent, but as a greater good intelligence, there is a holistic nature that will sort of attention that will naturally emerge that prevents it from sort of any one dimension running away? Yeah.
Yeah, exactly. So. So. You know, Ostrum had this example of the the superintelligent A.I that that makes that turns the world into paper clips because its job is to make paper clips or something, and that just as a thought experiment, did not make any sense to me.
Well, as a thought experiment or as a thing that could possibly be realized either.
So so I think that, you know, what my op ed was trying to do was say that that intelligence is more complex than these people are presenting it, that it is not like it is not so separable, the rationality.
The values, the emotions, the all of that, that it is the view that you could separate all these dimensions and build the machine that has one of these dimensions, and it is superintelligent in one dimension, but it does not have any of the other dimensions. that is what I was trying to criticize, that that that I do not believe that. So can I read a few sentences from your Shobanjo, who is always super eloquent.
So he writes. I have the same impression as Melanie that our cognitive biases are linked with our ability to learn to solve many problems, they may also be a limiting factor for a. However, this is a may in quotes, things may also turn out differently and there is a lot of uncertainty about the capabilities of future machines. But more importantly for me, the value alignment problem is a problem. Well, before we reach some hypothetical superintelligence, it is already posing a problem in the form of super powerful companies.
Whose objective function may not be sufficiently aligned with humanity, general well-being, creating all kinds of harmful side effects, that he goes on to argue that at the orthogonality in those kinds of the concerns of just aligning values with the capabilities of the system is something that might come long before we reach anything like superintelligent. So your criticism is kind of really nice to saying this idea of superintelligent systems seem to be dismissing fundamental parts of what intelligence would take. And then Yoshio kind of says, yes, but.
If we look at systems that are much less intelligent, there might be these same kinds of problems that emerge. Sure.
But I guess the example that he gives there of these corporations, that is people, right? Those are people's values. I mean, we are talking about people. The corporations are. Their values are the values of the people who run those corporations, but the idea is the algorithm. that is right. So the fundamental person, the fundamental element of what does the bad things, a human being. Yeah, but the the algorithm kind of controls the behavior of this mass of human beings.
Which algorithm would prefer for a company. that is the so for example, if it is advertisement driven company that recommends certain things and encourages engagement. So it sort of gets money by encouraging engagement and therefore the company more and more the cycle that builds an algorithm that enforces more engagement and made perhaps more division in the culture and so on and so on.
Again, I guess the question here is sort of who has the agency?
So so you might say, for instance, we do not want our algorithms to be racist.
And facial recognition, some people have criticized some facial recognition systems as being racist because they are not as good on darker skin and lighter skin.
that is right. OK, but the agency there, the the actual facial recognition algorithm is not what has the agency. it is it is not the racist thing. Right. it is it is the the I do not know, the the combination of the training data, the cameras being used or whatever.
But my understanding of and I say I totally agree with Bengoa there, that he you know, I think there are these value issues with our use of algorithms, but.
My understanding of what Russell's argument was is more that the algorithm, the machine itself has the agency now. it is the thing that is making the decisions and it is the thing that has what we would call values.
Yes.
So whether that is just a matter of degree, you know, it is hard it is hard to say. Right. Because but I would say that is sort of qualitatively different than a face recognition neural network.
And to broadly linger on that point, if you look at Elon Musk, Chrystia Russell or Bostrom, people who are worried about existential risks of AI, however far into the future, their argument goes as it eventually happens. We do not know how far, but that eventually happens. Do you share any of those concerns and what kind of concerns in general you have about that approach, anything like existential threat to humanity? So I would say, yes, it is possible, but I think there is a lot more.
Closer in existential threats, you had, as you said, like one hundred years for your time. For more than one hundred years, more than a hundred years.
And so maybe even more than 500 years. I do not I do not know.
I mean, it is so the existential threats are so far out that the future is there will be a million different technologies that we can even predict now that will fundamentally change the nature of our behavior, a reality society and so on before then.
I think so. I think so. And we have so many other. Pressing existential threats going on, nuclear weapons, even nuclear weapons, climate problems, you know. Poverty, possible pandemics. You can go on and on and I think, though, you know, worrying about existential.
Threat from A.I. is. It is not the first priority for what we should be worried about that that is kind of my view because we are so far away. But, you know, I am not. I am not necessarily criticizing Russell or Bostrom or whoever for worrying about that, and I think it is some some people should be worried about it.
it is it is certainly fine.
But I I was more sort of getting at there their view of intelligent intelligences. So it is more focusing on, like their view of superintelligence than a.
Just the fact of them worrying and the title of the article was written by The New York Times editors, I would not have called it that we should not be scared by superintelligent.
No. If you wrote maybe like we should redefine what you mean by superior.
I actually said, you know, something like superintelligence is not this is not a. Sort of coherent idea that does not that is not like something New York Times would put in and the follow up argument that your show makes also not argument, but a statement.
And I have heard him say before, and I think I agree, he kind of has a very friendly way of phrasing it as it is good for a lot of people to believe different things such as this guy.
Yeah, but it is also, practically speaking, like we should not be like while your article stands like still Russell does amazing work. Bostrom does amazing work. You do amazing work. And even when you disagree about the definition of super intelligence or the usefulness of even the term, it is still useful to have people that like use that term. Right. And then argue it is. I absolutely agree with you there. And I think it is great that, you know, and it is great The New York Times will publish all this stuff.
So it is an exciting time to be here.
What what do you think is a good test of intelligence? Is is natural language ultimately a test that you find the most compelling, like the original or the what? You know, the higher levels of the Turing test? Kind of.
Yeah, yeah, I. I still think the original idea of the Turing test is a good test for intelligence. I mean, I can not think of anything better. You know, the Turing test, the way that it is been carried out so far, has been a very impoverished, if you will. But I think a real Turing test that really goes into depth, like the one that I mentioned I talk about in the book.
I talk about Ray Kurzweil and Mitchell Corpore have this bet right.
That that, uh, twenty twenty nine, I think is the date their machine will pass the Turing test and Terrance's. And they have a very specific like how many hours expert judges and all of that. And you know, Kurzweil says yes, Skipper says no, we can. We only have like nine more years to go to see.
But I, I you know, if something a machine could pass that. I would be willing to call it intelligent, and of course, nobody will they will say that is just the language model. If it does to, you would be comfortable as a language. A long conversation that. Well, yeah, you are I mean, you are right, because I think probably to carry out that long conversation, you would literally need to have deep, commonsense understanding of the world.
I think so. And conversations enough to reveal that.
I think it is another super fun topic of complexity that you have worked on, written about. Let me ask the basic question. What is complexity? So complexity is another one of those terms like intelligence.
it is perhaps overused, but my book about complexity. Was about this wide area of complex systems, studying different systems in nature, in technology and society in which you have emergence. Kind of like I was talking about with intelligence. You know, we have the brain, which has billions of neurons, and each neuron individually could be said to be not very complex compared to the system as a whole.
But the system, the the interactions of those neurons and the dynamics creates these phenomena that we call we call intelligence or consciousness, you know, that are we consider to be very complex. So the field of complexity is trying to find general principles that underlie all these systems that have these kinds of emergent properties. And the emergence occurs from like underlying the complex system is usually simple, fundamental interactions. Yes. And the the emergence happens when there is just a lot of these things interacting.
Yes. Sort of what? And then most of the science to date, can you talk about what is reductionism? Well, reductionism is when you try and take a system and divided up into its elements. Whether those be cells or atoms or. Subatomic particles, whatever your field is, and then try and understand those elements and then try and build up an understanding of the whole system by looking at sort of the sum of all the elements.
So what is your sense whether we were talking about intelligence or these kinds of interesting, complex systems? Is it possible to understand them in a reductionist way, which is probably the approach of most of science today? Right. I do not think it is always possible to understand the things we want to understand the most, so I do not think it is possible to look at single neurons. And. Understand what we call intelligence, you know, to look at sort of summing up in the sort of the summing up is the the issue here that were you know, the one example is that the human genome.
Right. So there was a lot of work on excitement about sequencing the human genome, because the idea would be that we would be able to find genes that underlie diseases. But it turns out that. And I was very reductionist idea, you know, we figure out what all the the parts are, and then we would be able to figure out which parts because which things.
But it turns out that the parts do not because the things that we are interested in. it is like the interactions, the networks of these parts. And so that kind of reductionist approach did not yield the the explanation that we wanted. What do you what use the most beautiful, complex system that you have encountered, the most beautiful that you have been captivated by? Is it sort of. I mean, for me, that is the simplest to be celebrated. Oh, yeah.
So I was very captivated by cellular automata and worked on cellular automata for several years.
Do you find it amazing or is it surprising that such simple systems, such simple rules and cellular timbre can create sort of seemingly unlimited complexity? Yeah, that was very surprising to me.
How do you make sense of it? How does that make you feel? Is it just ultimately humbling or is there a hope to somehow leverage this into a deeper understanding and even be able to engineer things like intelligence?
it is definitely humbling. How humbling in that. I also kind of awe inspiring that.
it is that are inspiring, like part of mathematics, that these incredibly simple rules can produce this very beautiful, complex, hard to understand behavior, and that that is it is mysterious and surprising still, but exciting because it does give you kind of the hope that you might be able to engineer complexity just from from something from the beginning.
Can you briefly say what is the Santa Fe Institute, its history, its culture, its ideas, its future? Sort of. I have never I mentioned to you I have never been, but it is always been this in my mind, this mystical place where brilliant people study the edge of chaos.
Yeah, exactly.
So the Santa Fe Institute was started in 1984 and it was created by a group of scientists, a lot of them from Los Alamos National Lab, which is. About 40 minute drive from Serbia, Stewart. They were mostly physicists and chemists, but they were frustrated in their field because they felt so that their field was not approaching kind of big interdisciplinary questions like the kinds we have been talking about. And they wanted to have a place where people from different disciplines could work on these big questions without sort of being siloed into physics, chemistry, biology, whatever.
So they started this institute and this was people like George Cowan, who is a chemist in the Manhattan Project, and Nicholas Metropolis, who mathematician, physicist, Murray Gell-Mann, physicist. And so some really big names here, Carneiro and economist, Nobel Prize winning economist.
And they started having these workshops and this whole enterprise kind of grew into this cancer research institute. that is.
Itself has been kind of on the edge of chaos its whole life, because it does not have any it does not have a significant endowment, has just been kind of living on whatever funding it can raise through donations and grants and. However, it can, you know, business, business associates and so on, but it is a great place, it is a really fun place to go think about ideas from that you would not normally encounter.
So Sean Carroll is a physicist in the external faculty, and you mentioned that.
So there is some external faculty in this, people that a very small group of faculty, maybe, maybe about 10, who are there for five year terms that can sometimes get renewed. And then they have some postdocs and then they have this much larger on the order of one hundred external faculty or people like me who come and visit for various periods of time.
So what do you think is the future of the Santa Fe Institute? Like what if people are interested, like what was there in terms of the public interaction or students or so on, that that could be a possible interaction with the Santa Fe Institute or its ideas?
Yeah, so there is a there is a few different things they do.
They have a. Complex systems, summer school for graduate students and postdocs and sometimes faculty attend, too, and that is a four week, very intensive residential program where you go and you listen to lectures and you do projects and people, people really like that.
I mean, it is a lot of fun. They also have some specialty. Summer schools, there is one on computational social science, there is one on. Climate and sustainability, I think it is called. there is a few and then they have short courses where just a few days on different topics. They also have an online. Education platform that offers a lot of different courses and tutorials from SFI faculty. Including an introduction to complexity, of course, that I talk about awesome and there is a bunch of talks to an online from the guest speakers and so on, the host, a lot of yeah, they have sort of technical seminars, colloquia.
They and they have a community lecture series like Public Lectures, and they put everything on their YouTube channel.
So you can see it all. Watch it. Douglas Hofstadter, author of Gettle Mashaba, was your adviser. He mentioned a couple of times and collaborator. Do you have any favorite lessons or memories from your time working with him that continues to this day?
Yeah, but just even looking back throughout your time working with him.
So one of the things he taught me was that when you are looking at.
The complex problem, too, to idealize it as much as possible to try and figure out what are really what is the essence of this problem, and this is how like the copycat program came into being was by taking analogy making and saying how can we make this as idealised as possible with still retain really the important things we want to study. And that is really, you know, been a core theme of my research, I think. And I continue to try and do that.
And it is really very much kind of physics inspired.
Hofstetter was a Ph.D. in physics that was his background, like first principles kind of thing, like you are reduced to the most fundamental aspect of the problem. Yeah. The you can focus on solving that problem.
Yeah. And I you know, that was people used to work in these micro worlds. Right.
Like the block's world was very early, important area in A.I. and then that got criticized because they said, oh, you know, you can not scale that to the real world. And so people started working on much like more real world like problems. But now there is been kind of a return even to the block's world itself.
You know, we have seen a lot of people who are trying to work on more of these very idealized problems or things like natural language and common sense. So that is an interesting evolution of those ideas.
So the perhaps the blocks world represents the fundamental challenges of the problem of intelligence more than people realize it might.
Yeah. Is there sort of when you look back at your body of work in your life, you have worked on so many different fields, is there something that you are just really proud of in terms of ideas that you have gotten a chance to explore, create yourself? So I am really proud of my work on the copycat project, I think it is really different from what? Almost everyone has done, and I think there is a lot of ideas there to be explored and I guess one of the happiest days of my life.
You know, aside from like the births of my children was the birth of copycat, what it actually started to be able to make really interesting analogies.
And I remember that very clearly. That was very exciting time where you kind of gave life.
Yes. Artificial. So. that is right.
What in terms of what people can interact. So there is like a I think it is called Medicare, Medicaid, Medicare, and there is a Python three implementation.
If people actually want to play around with it and actually get into a study and maybe integrate into whether it is with deep learning or any other kind of work they are doing, what what would you suggest they do to learn more about it and to take it forward in different kinds of directions?
Yeah, so that there is Douglas Hofstadter's book called Fluid Concepts and Creative Analogies talks in great detail about copycat.
I have a book called Analogy Making as Perception, which is a version of my PhD thesis on it. there is also code that is available. You can get it to run. I have some links on my Web page to where people can get the code for it. And I think that that would really be the best way I get into it. And yeah, I will play with it. Well, Molineux, an honor talking to you. I really enjoyed it.
Thank you so much for your time today.
Thanks. it is been really great. Thanks for listening to this conversation with Melanie Mitchell and thank you to our presenting sponsor cash app Download. It is called Lux podcast. you will get ten dollars and ten dollars will go to First, a STEM education nonprofit that inspires hundreds of thousands of young minds to learn and to dream of engineering our future. Enjoy this podcast. Subscribe on YouTube. Give it five stars, an Apple podcast supported on Patrón or connect with me on Twitter.
And now let me give you some words of wisdom from Douglas Hofstadter and Melanie Mitchell. With our concepts that can be no thought and without analogies, there can be no concepts. And Melody adds, how to form and fluidly use concepts is the most important open problem in A.I.. Thank you for listening and hope to see you next time.
