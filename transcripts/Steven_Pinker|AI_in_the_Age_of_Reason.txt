Welcome to the Artificial Intelligence podcast. My name is Luks Friedman, I am a research scientist at MIT. Today is a conversation with Steven Pinker. he is a professor at Harvard. And before that was a professor at MIT. he is the author of many books, some of which had a big impact on the way I see the world for the better, in particular, the better angels of our nature. And his latest book, Enlightenment Now have instilled in me a sense of optimism, optimism grounded in data science and reason.
I really enjoyed this conversation. I hope you do as well. you have studied the human mind, cognition, language, vision, evolution, psychology from child to adult, from the level of individual to the level of our entire civilization. So I feel like I can start with a simple multiple-choice question.
What is the meaning of life?
Is it, A, to attain knowledge, as Plato said, be to attain power, as Nietzsche said, see to escape death, as Ernest Becker said D To propagate our genes, as Darwin and others have said. E There is no meaning, as the nihilists have said.
F knowing the meaning of life is beyond our cognitive capabilities. Steven Pinker said, Based on my interpretation 20 years ago and G none of the above, I would say it comes closest.
But I would amend that to attaining not only knowledge but fulfillment more generally, that is life, health stimulation, access to the living, cultural and social world. Now this is our meaning of life. it is not the meaning of life. If you were to ask our genes, their meaning is to propagate copies of themselves. But that is distinct from the meaning that their brain that they lead to sets for itself.
So to you, knowledge is a small subset of a large subset.
it is a large subset, but it is not the entirety of human striving because we also want to interact with people. We want to experience beauty. We want to experience the richness of the natural world. But understanding the what makes the universe ATEC is is way up there for some of us more than others. Certainly for me that that is one of the top five.
So is that a fundamental aspect? Are you just describing your own preference or is this a fundamental aspect of human nature is to seek knowledge just to in your latest book, you talk about the the power, the usefulness of rationality and reason and so on. Is that a fundamental nature of human beings or is it something we should just strive for it?
Both it is. we are capable of striving for it because it is one of the things that make us what we are Homo sapiens wise man. We are unusual among animals in the degree to which we acquire knowledge and use it to survive. We we make tools. We strike agreements via language. We extract poisons. We predict the behavior of animals. We try to get at the workings of plants.
And when I say we, I do not just mean we in the modern West, but we as a species everywhere, which is how we have managed to occupy every nation on the planet, how we have managed to drive other animals to to extinction, and the refinement of reason in pursuit of human well-being, of health, happiness, social richness, cultural richness is are our main challenge in the present that is using our intellect, using our knowledge to figure out how the world works, how we work in order to make discoveries and strike agreements that make us all better off in the long run.
Right. And you do that. Almost undeniably and in a data driven way in your recent book, but I would like to focus on the artificial intelligence aspect of things and not just artificial intelligence, but natural intelligence to. So 20 years ago in the book you have written and how the mind works, you conjecture again, my right to interpret things.
You can correct me if I am wrong, but you conjecture that human thought in the brain may be a result of and now a massive network of highly interconnected neurons. From this interconnectivity emerges thought compared to artificial neural networks, which we use for machine learning today is there is something fundamentally more complex, mysterious, even magical about the biological neural networks versus the ones we have been starting to use over the past 60 years and become to success in the past when there is something a little bit mysterious about the human neural networks, which is that each one of us who is a neural network knows that we ourselves are conscious, conscious, not in the sense of registering our surroundings or even registering our internal state, but in having subjective first person, present tense experience.
That is when I see read. it is not just different from green, but it just there is a readiness to it that I feel whether an artificial system would experience that or not, I do not know and I do not think I can know. And that is why it is mysterious. If we had a perfectly lifelike robot that was behaviorally indistinguishable from a human, would we attribute consciousness to it or are not we to attribute consciousness to it? And that is something that it is very hard to know.
But putting that aside, putting aside that that largely philosophical question, the question is, is there some difference between the human neural network and the ones that we were building in artificial intelligence will mean that we are on the current trajectory, not going to reach the point where we have got a life like robot indistinguishable from a human, because the way their neural so-called neural networks are organized are different from the way ours are organized. I think there is overlap, but I think there are some some big differences that their current neural networks, current so-called deep learning systems, are in reality not all that deep.
That is, they are very good at extracting high order statistical regularities. But most of the systems do not have a semantic level, a level of actual understanding of who did what, to whom, why, where, how things work, what causes what else do you think that kind of thing can emerge as it does so artificial?
You know, it is a much smaller the number of connections and so on than the current human biological networks. But do you think sort of to go to consciousness or to go to this higher level semantic reasoning about things?
Do you think that can emerge with just a larger network, with a more richly, weirdly interconnected network, separate and consciousness?
Because consciousness is even a matter of complex, a really weird one. Yeah, you could have you could sensibly ask the question of whether shrimp are conscious, for example. they are not terribly complex, but maybe they feel pain. So let us let us just put that what that part of it aside. Yeah.
But I think sheer size of a neural network is not enough to give it structure and knowledge, but if it is suitably engineered then then then why not. That is where neural networks natural selection did a kind of equivalent of engineering of our brains. So I do not there is anything mysterious in the sense that no no system made out of silicon could ever do what a human brain can do.
I think it is possible in principle, whether it will ever happen depends not only on how clever we are in engineering these systems, but whether even we even want to whether that is even a sensible goal.
That is, you can ask the question, is there any locomotion system that is as as good as a human? Well, we kind of want to do better than a human ultimately in terms of legged locomotion. there is no reason that humans should be our benchmark. they are they are tools that might be better in some ways.
It may just be not as it may be that we can not duplicate a natural system because at some point it is so much cheaper to use a natural system that we are not going to invest more brainpower and resources. So, for example, we do not really have a exact substitute for would we still build houses out of wood? We still build furniture out of wood. We like the look. We like the feel. it is wood has certain properties that synthetics do not. it is not that there is anything magical or mysterious about wood.
it is just that the extra steps of duplicating everything about wood is something we just have not bothered because we have wood, like I say, cotton. I mean, I am wearing cotton clothing now feels much better than than polyester. it is not that cotton has something magic in it.
And it is not that if there was that we could not ever synthesise something exactly like cotton, but at some point it is just it is just not worth it. we have got cotton. And likewise, in the case of human intelligence, the goal of making an artificial system that is exactly like the human brain is a goal that we probably no one is going to pursue to the bitter end, I suspect, because if you want tools that do things better than humans, you are not going to care whether it does something like humans.
So, for example, diagnosing cancer or predicting the weather, why set humans as your benchmark?
But in in general, I suspect you also believe that even if the humans should not be a benchmark and do not imitate humans in their system, there is a lot to be learned about how to create an artificial intelligence system by studying the human.
Yeah, I think that is right. In the same way that to build flying machines, we want to understand the laws of aerodynamics and including birds, but not mimic the birds, but the same laws.
You have a view on A.I. artificial intelligence and safety that from my perspective is refreshingly rational or perhaps more importantly, has elements of positivity to it, which I think can be inspiring and empowering as opposed to paralyzing.
For many people, including our researchers, the eventual existential threat of AI is obvious, not only possible, but obvious. And for many others, including our researchers, the threat is not obvious. So Elon Musk is famously in the highly concerned about Eykamp saying things like as far more dangerous than nuclear weapons and that I will likely destroy human civilization.
So in February, he said that if Elon was really serious about AI, that the threat of AI, he would stop building self-driving cars that he is doing very successfully as part of Tesla.
Then he said, wow, if even Pinker does not understand the difference between now I like a car and General AI when the latter literally has a million times more compute power and an open ended utility function, humanity is in deep trouble.
So first, what did you mean by the statement about Elon Musk should stop being self-driving cars if he is deeply concerned.
Not the last time that Elon Musk has fired off an intemperate tweet.
Yeah, well, we live in a world where Twitter has power. Yes. Yeah. I think the the there are two kinds of existential threat that have been discussed in connection with artificial intelligence. And I think that they are both incoherent. One of them is vague fear of A.I. takeover, that it just as we subjugated animals and less technologically advanced peoples. So if we build something that is more advanced than us, it will inevitably turn us into pets or slaves or or domesticated animal equivalents.
I think this confuses intelligence with a will to power that it so happens that in the intelligence system we are most familiar with, namely Homo sapiens, we are products of natural selection, which is a competitive process.
And so bundled together with our Problem-Solving capacity are a number of nasty traits like dominance and exploitation and maximization of power and glory and resources and influence. there is no reason to think that sheer problem solving capability will set. That is one of its goals. Its goals will be whatever we set its goals as. And as long as someone is not building a megalomaniacal artificial intelligence, then there is no reason to think that it would naturally evolve in that direction. Now, you might say, well, what if we gave it the goal of maximizing its own power source?
Well, that is a pretty stupid goal to give an autonomous system. You do not give it that goal. I mean, that is just self evidently idiotic.
So if you look at the history of the world, there is been a lot of opportunities where engineers could instill in a system destructive power and they choose not to because that is the natural process of engineering.
Well, except for weapons. I mean, if you are building a weapon, its goal is to destroy people. And so I think there are good reasons to not not build certain kinds of weapons. I think building nuclear weapons was a massive mistake.
But do you think so? Maybe pause on that, because that is one of the serious threats. Do you think that it was a mistake in a sense that it should have been stopped early on? Or do you think it is just an unfortunate event of invention that this was invented? Do you think it was possible to stop?
I guess is the question that it is hard to rewind the clock because, of course, it was invented in the context of World War two and. The fear that the Nazis might develop on first then once was initiated for that reason, it was it was hard to turn off, especially since winning the war against the Japanese and the Nazis was such an overwhelming goal of every responsible person that they were just nothing that people would not have done then to ensure victory. it is quite possible if World War Two had not happened, that nuclear weapons would not have been invented.
We can not know. But I do not think it was by any means a necessity anymore than some of the other weapons systems that were envisioned but never implemented, like planes that would disperse poison gas over cities like cropdusters or systems to try to to to create earthquakes and tsunamis in enemy countries, to weaponize the weather, weaponize solar flares, all kinds of crazy schemes that that we thought the better off. I think analogies between nuclear weapons and artificial intelligence are fundamentally misguided because the whole point of nuclear weapons is to destroy things.
The point of artificial intelligence is not to destroy things. So the analogy is is misleading.
So there is two artificial terms, as you mentioned. The first one was the intelligence.
Now, the very yeah, the system that we designed ourselves where we give it the goals, goals are external to the means to attain the goals.
If we do not design an artificial intelligence system to maximize dominance, then it will not maximize dominance. it is just that we are so familiar with homosapiens where these two traits come bundled together, particularly in men, that we are apt to confuse high intelligence with a will to power. But that is just an error. The other fear is that will be collateral damage that will give artificial intelligence a goal, like make paper clips and it will pursue that goal so brilliantly that before we can stop it, it turns us into paper clips.
we will give it the goal of curing cancer and it will turn us into guinea pigs for lethal experiments or give it the goal of world peace. And its conception of world peace is no people, therefore no fighting. And so it will kill us all. Now, I think these are utterly fanciful. In fact, I think they are actually self-defeating. They, first of all, assume that we are going to be so brilliant that we can design an artificial intelligence that can cure cancer, but so stupid that we do not specify what we mean by curing cancer in enough detail that it will not kill us in the process.
And it assumes that the system will be so smart that it can cure cancer, but so idiotic that it does not can not figure out that what we mean by curing cancer is not killing everyone. So I think that the the collateral damage scenario, the value alignment problem is is also based on a misconception.
So one of the challenges, of course, we do not know how to build either system currently or are we even close to knowing? Of course, those things can change overnight. But at this time, theorizing about it is very challenging in either direction. So that is probably at the core of the problem is without that ability to reason about the real engineering things here at hand is your imagination runs away with things. Exactly.
But let me sort of ask, what do you think was the motivation, the thought process of Elon Musk?
I build autonomous vehicles. I study autonomous vehicles. I study Tesla autopilot. I think it is one of the greatest currently application large scale applications of artificial intelligence in the world. It has potentially a very positive impact on society.
So how does a person who is creating this very good, quote unquote narrow A.I. system also seem to be so concerned about this other general A.I.?
What do you think is the motivation there? What do you think is the thing?
Well, you probably have to ask him, but there and he is notoriously flamboyant, impulsive to the as we have just seen, to the detriment of his own goals of the health of a company. So I do not know what is going on on his mind.
You probably have to ask him, but I do not think the and I do not think the distinction between special purpose AI and so-called general AI is relevant, that in the same way that special purpose AI is not going to do anything conceivable in order to attain a goal, all engineering systems have to are designed to trade off across multiple goals. Will we build cars in the first place? We did not forget to install brakes because the goal of a car is to go fast.
It occurred to people, yes, you want to go fast, but not always. So you build in brakes. Do likewise. If a car is going to be autonomous, that does not and program it to take the shortest route to the airport. it is not going to take the diagonal and mow down people and trees and fences because that is the shortest route. that is not what we mean by the short. When we program it, and that is just what an intelligent system is, by definition, it takes into account multiple constraints.
The same is true, in fact, even more true of so-called general intelligence. That is, if it is genuinely intelligent, it is not going to pursue some goal single mindedly committing every other consideration and collateral effect. that is not artificial general intelligence.
that is that is artificial stupidity. I agree with you, by the way, on the promise of autonomous vehicles for improving human welfare. I think it is spectacular. And I am surprised at how little press coverage notes that in the United States alone, something like 40000 people die every year on the highways, vastly more than are killed by terrorists.
And we spend we spent a trillion dollars on a war to combat deaths by terrorism, but half a dozen a year, I was year in, year out, 40000 people are massacred on the highways, which could be brought down to very close to zero. So I am with you on the humanitarian benefit.
Let me just mention that it is as a person who is building these cars, it is a little bit offensive to me to say that engineers would be close enough not to engineer safety into systems. I often stay up at night thinking about those 40000 people that are dying and everything I try to engineer is to save those people's lives. So every new invention that I am super excited about, every new and all the deep learning literature and CPR conferences and everything I am super excited about is all grounded in making it safe and help people.
So I just do not see how that trajectory can all of a sudden slip into a situation where intelligence will be highly negative.
You and I certainly agree on that. And I think that is only the beginning of the potential humanitarian benefits of artificial intelligence. there is been enormous attention to what are we going to do with the people whose jobs are made obsolete by artificial intelligence. But very little attention given to the fact that the jobs are going to be made obsolete are horrible jobs.
The fact that people are not going to be picking crops and making beds and driving trucks and mining coal, these are, you know, soul deadening jobs.
We have a whole literature sympathizing with the people stuck in these menial mind, deadening dangerous jobs if we can eliminate them.
This is a fantastic boon to humanity.
Now, granted, we you solve one problem and there is another one. Namely, how do we get these people a decent income?
But if we are smart enough to invent machines that can make beds and put away dishes and and handle hospital patients, well, I think we are smart enough to figure out how to redistribute income, to apportion some of the vast economic savings to the human beings who will no longer be needed to make beds.
OK, Sam Harris says that it is obvious that eventually I will be an existential risk. he is one of the people says it is obvious we do not know when the claim goes, but eventually it is obvious and because we do not know when we should worry about it. Now, this is a very interesting argument in my eyes. So how how do we think about timescale? How do we think about existential threats when we do not really know so little about the threat, unlike nuclear weapons, perhaps about this particular threat that it could happen tomorrow?
Right. So but very likely will not.
Yeah, they are likely to be one hundred years away. So how do we ignore it? Do how do we talk about it? Do we worry about it?
What how do we think about those? What is it a threat that we can imagine?
it is within the limits of our imagination, but not within our limits of understanding to sufficient to accurately predict it. But what is what is the that we are fighting?
Sorry, I, I being the existential threat, I can tell by enslaving us or turning us into paper clips.
I think the most compelling from the Sam Harris perspective would be the paperclip situation.
Yeah. I mean I think I just think it is totally fanciful. I mean, I do not build the system, do not give a do not. First of all, the code of engineering is you do not implement a system with massive control before testing it now.
Perhaps the culture of engineering will radically change then I would worry. I do not see any signs that engineers will suddenly do idiotic things like put a an electric power plant in control of a system that they have not tested first or all of these scenarios.
Not only imagine a almost a magically powered intelligence, you know, including things like cure cancer, which is probably incoherent goal because there is so many different kinds of cancer or bring about world peace. I mean, how do you even specify that as a goal? But the answer is also imagine some degree of control of every molecule in the universe, which not only is itself unlikely, but we would not start to connect these systems to infrastructure without without testing, as we would any kind of engineering system.
Now, maybe some engineers will be responsible and we need legal and regulatory and legal responsibility implemented so that engineers do not do things that are stupid by their own standards. But the I have never seen enough of a plausible scenario of existential threat to devote large amounts of brainpower to to forestall it.
So you believe in the sort of the power and mass of the engineering of reason, as you argue in your latest book of reasons, science and sort of be the very thing that guides the development of new technologies.
So it is safe and also keeps us safe the same, you know, granted the same culture of safety that currently is part of the engineering mind set for airplanes, for example. So, yeah, I do not think that that that that should be thrown out the window and that untested, all powerful system should be suddenly implemented.
But there is no reason to think they are. And in fact, if you look at the progress of artificial intelligence, it is been you know, it is been impressive, especially in the last 10 years or so.
But the idea that suddenly there'll be a step function, that all of a sudden before we know it, it will be all powerful that there'll be some kind of recursive self-improvement, some kind of foom is also fanciful, certainly by the technology that we that now impresses us, such as deep learning, where you train something on hundreds of thousands or millions of examples there, not hundreds of thousands of problems of which curing cancer is a typical example. And so the kind of techniques that have allowed A.I. to increase in the last five years are not the kind that are going to lead to this fantasy of of exponential, sudden self-improvement.
So it is I think it is kind of a magical thinking. it is not based on our understanding of how A.I. actually works. Now, give me a chance here.
So you said fanciful, magical thinking in his TED talk, Sam Harris says that thinking about A.I. killing all human civilization is somehow fun intellectually.
Now, I have to say, as a scientist engineer, I do not find it fun, but when I am having beer with my Nania friends, there is indeed something fun and appealing about it, like talking about an episode of Black Mirror, considering if a large meteor is headed towards we were just told a large meteor is headed towards Earth, something like this. And can you relate to this sense of fun and do you understand the psychology of it?
Yes, right. Good question. I personally do not find it fun. I find it kind of actually a waste of time because there are genuine threats that we ought to be thinking about, like black pandemics, like like cybersecurity vulnerabilities, like the possibility of nuclear war and certainly climate change. This is enough to fill many conversations without.
And I think I think Sam did put his finger on something, namely that there is a community, us sometimes called the rationality community that delights in using its brainpower to come up with scenarios that would not occur to mere mortals, to the less cerebral people. So there is a kind of intellectual thrill in finding new things to worry about. No one has worried about yet.
I actually think, though, that it is not only is a kind of fun that does not give me particular pleasure, but I think there can be a pernicious side to it, namely that you overcome people with such dread, such fatalism, that there is so many ways to to to to die, to annihilate our civilization, that we may as well enjoy life while we can. there is nothing we can do about it. If climate change does not do us harm, then runaway robots well.
So let us enjoy ourselves now. we have got to prioritize.
We have to look at threats that are close to certainty, such as climate change, and distinguish those ones that are merely imaginable, but with infinitesimal probabilities. And we have to take into account people's worry budget.
You can not worry about everything. And if you so dread and fear and terror and and fatalism, it can lead to a kind of numbness where they are just these problems are overwhelming and the engineers are just going to kill us all. So let us either destroy the entire infrastructure of science technology or let us just enjoy life while we can.
So there is a certain line of worry, which I am worried about a lot of things, assuming there is a certain line of worry when you cross the floor to cross that it becomes paralyzing fear as opposed to productive fear.
And that is kind of what you are highlighting. Exactly right.
And we have seen some we know that human effort is not well calibrated against risk in that because a basic tenet of cognitive psychology is that perception of risk and hence perception of fear is driven by immagine ability, not by data. And so we misallocate vast amounts of resources to avoiding terrorism, which kills on average about six Americans a year, with a one exception of 9/11. We invade countries.
We invent an entire new departments of government with massive, massive expenditure of resources and lives to defend ourselves against a trivial risk, whereas guaranteed risks.
You mentioned as one of them, you mentioned traffic fatalities and even risks that are not here, but are plausible enough to worry about like pandemics, like nuclear war, receive far too little attention.
The presidential debates, there is no discussion of how to minimize the risk of nuclear war, lots of discussion of terrorism, for example. And so we I think it is essential to calibrate our budget of fear, worry, concern, planning to the actual probability of of harm.
Yep.
So let me ask this question. So speaking of immagine ability, you said that it is important to think about reason.
And one of my favorite people who who likes to dip into the outskirts of reason, uh, through fascinating exploration of his imagination, is Joe Rogan.
Oh, yes, you. So who has the reason? Used to believe a lot of conspiracies and through reason has stripped away a lot of his beliefs in that way. So it is fascinating actually to watch him through rationality, kind of throw away the ideas of Bigfoot and 9/11.
I am not sure exactly do not know what he believes in. Yes, but you no longer know. Believed in. that is right. No, he is he is become a real force for for good.
Yep. So you were on the Joe Rogan podcast in February and had a fascinating conversation, but as far as I remember, did not talk much about artificial intelligence. I will be on his podcast in a couple of weeks.
Joe is very much concerned about existential threat.
If I am not sure if here. This is why I was hoping that you would get into that topic. And in this way, he represents quite a lot of people who look at the topic of A.I. from 10000 foot level.
So as an exercise of communication, he said it is important to be rational and reasoned. But these things let me ask if you were to coach me as a researcher about how to speak to Joe and the general public about it, what would you advise?
Well, the short answer would be to read the sections that I wrote in Enlightenment. But I no longer reason would be, I think, to emphasize and I think you are very well positioned as an engineer to remind people about the culture of engineering, that it really is safety oriented, that another discussion in enlightenment.
Now I plot rates of accidental death from various causes, plane crashes, car crashes, occupational accidents, even death by lightning strikes.
And they all plummet because the culture of engineering is how do you squeeze out the lethal risks?
Death by fire, death by drowning, death by asphyxiation.
All of them drastically declined because of advances in engineering that I got to say I did not appreciate until I saw those graphs. And it is because exactly. People like you who stay up at night thinking, oh, my God is what am I what I mean, what I am inventing likely to hurt people and to deploy ingenuity to prevent that from happening. Now, I am not an engineer, although I spent 22 years at MIT, so I know something about the culture of engineering.
My understanding is that this is the way you think if you were an engineer and it is essential that that culture not be suddenly switched off when it comes to artificial intelligence. So, I mean, that that could be a problem. But is there any reason to think it would be switched off?
I do not think so. And one, there is not enough engineers speaking up for this way for the excitement, for the positive view of human nature. And what you are trying to create is positivity. Like everything we try to invent is trying to do good for the world. But let me ask you about the psychology of negativity. It seems just objectively not considering the topic, it seems that being negative about the future makes you sound smarter than being positive about the future, regardless of topic.
Am I correct in this observation?
And if so, why do you think that is?
Yeah, I think that I think there is that that phenomenon that as Tom Lehrer, the satirist said, always predict the worst and you will be hailed as a prophet. It may be part of our overall negativity bias. We are, as a species, more attuned to the negative than the positive. We dread losses more than we enjoy gains. And that might open up a space for prophets to remind us of harms and risks and losses that we may have overlooked.
So I think there there is that asymmetry.
So you have written some of my favorite books all over the place. So starting from enlightenment now to the better angels of our nature. Blank slate, how the mind works. The one about language, language, instinct.
Bill Gates, big fan to serve your most recent book that it is my new favorite book of all time. Um, so for you as an author, what was the book early on in your life that had a profound impact on the way you saw the world?
Certainly this book, Enlightenment Now is influenced by David Deutsch's The Beginning of Infinity, a rather deep reflection on knowledge and the power of knowledge to improve the human condition. The and with bits of wisdom such as that, problems are inevitable, but problems are solvable given the right knowledge, and that solutions create new problems that have to be solved in their turn. that is, I think, a kind of wisdom about the human condition that influenced the writing of this book.
there is some books that are excellent but obscure, some of which I have on my I am on a page on my website. I read a book called The History of Force Self, published by a political scientist named James Paine on the historical decline of violence. And that was one of the inspirations for the better angels of our nature.
The what about early on? If you look back when you were maybe a teenager, I loved a book called One, Two, Three Infinity when I was a young adult.
I read that book by George Gama'a, the physicist, very accessible and humorous explanations of relativity, of no theory of dimensionality, high multiple dimensional spaces in a way that I think is still delightful 70 years after it was published.
I like that the Time Life Science series, these were books that would arrive every month that my mother subscribed to each one on a different topic. One would be on electricity, one would be on, you know, forests, one would be on evolution. And then one was on the mind. And I was just intrigued that there could be a science of mind and that that book I would cite as an influence as well.
Then later on, you fell in love with the. Studying the mind, that is one thing that grabbed you, it was one of the things I would say the I read as a college student. The book Reflections on Language by Noam Chomsky spent most of his career here at MIT. Richard Dawkins two books, The Blind Watchmaker and The Selfish Gene, were enormously influential, partly for mainly for the content, but also for the writing style, the ability to explain abstract concepts in lively prose.
Stephen Jay Gould's first collection ever since Darwin. Also an excellent example of a lively writing. George Miller, a psychologist that most psychologists are familiar with, came up with the idea that human memory has a capacity of seven plus or minus two chunks. that is probably his biggest claim to fame. He wrote a couple of books on language and communication that I would read as an undergraduate. Again, beautifully written and intellectually deep.
Wonderful. Stephen, thank you so much for taking the time to. My pleasure. Thanks a lot. Lex.
