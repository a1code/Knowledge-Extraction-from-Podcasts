The following is a conversation with Kate Darling, a researcher at MIT interested in social robotics, robot ethics and generally how technology intersects with society. She explores the emotional connection between human beings and lifelike machines, which for me is one of the most exciting topics in all of artificial intelligence, as she writes in her bio. she is a caretaker of several domestic robots, including her puleo, dinosaur robots named Yoki, Peter and Mr. Spagetti.
she is one of the funniest and brightest minds I have ever had the fortune to talk to. This conversation was recorded recently, but before the outbreak of the pandemic, for everyone feeling the burden of this crisis, I am sending love your way. This is the artificial intelligence podcast if you enjoy it. Subscribe on YouTube, review five stars and have a podcast. Supporting and patron are simply connected me on Twitter. Allex Friedman spelled F.R. IDM man. As usual, I will do a few minutes of ads now and never any ads in the middle that can break the flow of the conversation.
I hope that works for you. It does not hurt the listening experience. Quick summary of the ads to sponsors Masterclass and Express VPN, please consider supporting the podcast by signing up to master a master dot com slash works and getting express VPN and express dot com slash leks pod.
The show is sponsored by Master Class Sign Up and master class dotcoms that likes to get a discount and to support this podcast. When I first heard about Master Class, I thought it was too good to be true. For one hundred eighty dollars a year you get an all access pass to watch courses from the list. Some of my favorites, Chris Hadfield and Space Exploration Neil deGrasse Tyson on Scientific Thinking and communication will write creator of SIM City and Sims. Love those games and game design.
Carlos Santana on guitar, Garry Kasparov on chess, Danny on the ground on poker and many more. Chris Hadfield explaining how rockets work and the experience of being launched into space alone is worth the money. By the way, you can watch it on basically any device. Once again, sign up a master class. That councillor's likes to get a discount and to support this podcast. This show sponsored by Express VPN, get it at Express dot com slash blackspot to get a discount and to support this podcast.
I have been using express support for many years. I love it. it is easy to use the big power button and your privacy is protected and if you like, you can make it look like your locations anywhere else in the world. I might be in Boston now, but it can make it look like I am in New York, London, Paris or anywhere else. This has a large number of obvious benefits. Certainly it allows you to access international versions of streaming websites like the Japanese, Netflix or the UK.
Hulu Express VPN works on any device you can imagine. I use it on Linux, shout out Ubuntu twenty or for Windows Android, but it is available everywhere else to once again get an express VPN dot com slash lux pod to get a discount and to support this podcast. And now here is my conversation with Kate, darling. You taught robot ethics at Harvard. What are some ethical issues that arise? In the world with robots. Yeah, that was a reading group that I did when I like at the very beginning, first became interested in this topic.
So I think if I taught that class today, it would look very, very different.
Robot ethics, it sounds very science fiction, especially did back then. But I think that. Some of the issues that people in robot ethics are concerned with or just around the ethical use of robotic technology in general, so, for example, responsibility for harm, automated weapons systems, things like privacy and data security, things like, you know, automation and labor markets. And then personally, I am really interested in some of the social issues that come out of our social relationships with robots, one on one relationship with robots.
Yeah, I think most of the stuff we have to talk about is like one on one social stuff. that is what I love. And I think that is what your you love as well. And there are expert in but a societal level. there is like there is a presidential candidate now, Andrew Yang, running concern about automation and robots and in general taking away jobs. He has a proposal of UBI universal basic income of everybody gets a thousand bucks. Yeah.
As a way to sort of save you if you lose your job from automation to allow you time to discover what it is that you would like to or even love to do.
Yes.
So I lived in Switzerland for 20 years and universal basic income has been more of a topic.
they are separate from the whole robots and jobs issues. So it is so interesting to me to see kind of these Silicon Valley people latch onto this concept that came from a very kind of left wing socialist, you know, kind of a different place in Europe.
But on the automation and labour markets topic, I think that it is very so sometimes in those conversations, I think people overestimate where robotic technology is right now. And we also have this fallacy of constantly comparing robots to humans and thinking of this as a one to one replacement of jobs.
So even like Bill Gates a few years ago said something about, you know, maybe we should have a system that taxes robots for taking people's jobs.
And it just I mean, I am sure that was taken out of context. You know, he is a really smart guy, but that sounds to me like kind of viewing it as a one to one replacement versus viewing this technology as kind of a supplemental tool. That, of course, is going to shake up a lot of stuff. it is going to change the job landscape.
But I do not see, you know, robots taking all the jobs in the next 20 years.
that is just not how it is going to work. Right.
So maybe drifting into the land of more personal relationships with robots, interaction and so on. I got to warn you, I go I may ask some silly philosophical questions, I apologize. Oh, please do. OK. Do you think humans will abuse robots in their interactions? So you have had a lot of and we will talk about a sort of anthropomorphising version and and work, you know, this this intricate dance, emotional dance between humans and robot.
But this seems to be also a darker side where people when they treat.
The other, as servants especially, they can be a little bit abusive or a lot abusive, do you think about that? I do worry about that.
Yeah, I do think about that.
So, I mean, one of my one of my main interests is the fact that people subconsciously treat robots like living things and even though they know that they are interacting with a machine and what it means in that context to behave, you know, violently. I do not know if you could say abuse because you are not actually, you know, abusing the inner mind of the robot. The robot is then does not have any feelings as far as you know.
Well, yeah, it also depends on how we define feelings and consciousness. But I think that is another area where people kind of overestimate where we currently are with the technology. Like the robots are not even as smart as insects right now. And so I am not worried about abuse in that sense. But it is interesting to think about what does people's behavior towards these things mean for our own behavior? Is it desensitizing the people to be verbally abusive to a robot or even physically abusive?
And we do not know whether it is a similar connection from like if you play violent video games, what connection does that have to desensitization to violence?
that is actually I have not even read literature on that. I wonder about that. Because everything I have heard, people do not seem to any longer be so worried about violent video games, correct?
we have seen the research on it is it is a difficult thing to research. So it is sort of inconclusive. But we seem to have gotten the sense, at least as a society, that people can compartmentalize when it is something on a screen and you are like, you know, shooting a bunch of characters or running over people with your car, that does not necessarily translate to you doing that in real life.
We do, however, have some concerns about children playing violent video games. And so we do restrict it there. I am not sure that is based on any real evidence either, but it is just the way that we have kind of decided, you know, we want to be a little more cautious there.
Now, the reason I think robots are a little bit different is because there is a lot of research showing that we respond differently to something in our physical space than something on a screen. We will treat it much more viscerally, much more like a physical actor.
And so it is it is totally possible that this is not a problem. And it is the same thing as violence in videogames. You know, maybe, you know, restrict it with kids to be safe. But adults can do what they want. But we just need to ask the question again, because we do not have any evidence at all yet.
Maybe there is an intermediate place to. I did my research on Twitter. By research, I mean, scrolling through your Twitter feed, you mentioned that you were going at some point to an animal law conference. So I have to ask, do you think there is something that we can learn from animal rights that guys are thinking about robots?
Oh, I think there is so much to learn from that. I am actually writing a book on it right now. that is why I am going to this conference. So I am I am writing a book that looks at the history of animal domestication and how we have used animals for work, for weaponry, for companionship. And, you know, one of the things the book, the book tries to do is move away from this fallacy that I talked about of comparing robots and humans, because I do not think that is the right analogy.
But I do think that on a social level, even on a social level, there is so much that we can learn from looking at that history, because throughout history we have treated most animals like tools, like products, and then some of them we have treated differently.
And we are starting to see people treat robots in really similar ways. So I think it is a really helpful predictor to how we are going to interact with the robots.
Do you think we will look back at this time like one hundred years from now and see what we do to animals is like similar the way we view like the Holocaust with the World War two?
that is a great question. I mean, I hope so. I am not. Convinced that we will, but I often wonder, you know, what are my grandkids going to view as, you know, abhorrent that my generation did, that they would never do?
And I am like, well, what is the big deal? You know, it is it is a fun question to ask yourself. It always seems that there is atrocities that we discover later.
So the things that at the time people did not see, as you know, look at everything from slavery to any kinds of abuse throughout history to the kind of insane wars that were happening to the way war was carried out and rape and the kind of violence that was happening during war in that we now, you know, we see his atrocities, but at the time perhaps did not as much. And so now. I have this intuition that I have this worry, maybe you are going to probably criticize me, but I do anthropomorphize robots.
I have I do not see a fundamental philosophical difference in a robot and a human being.
In terms of once the capabilities are matched. So the fact that we are really far away does not in terms of capabilities and that from from natural language processing, understanding generation to just reasoning and all that stuff, I think once you saw it, I see this is a very gray area and I do not feel comfortable with the kind of abuse that people throw at robots. Subtle. But I can see it becoming I can see basically a civil rights movement for robots in the future.
Do you think you put it in the form of a question? Do you think robots should have some kinds of rights?
Well, it is interesting because I came at this originally, from your perspective, I was like, you know what? there is no fundamental difference between technology and like human consciousness. Like, we can probably recreate anything. We just do not know how yet. And so there is no reason not to give machines the same rights that we have once. Like you say, they are kind of on an equivalent level.
But I realize that that is kind of a far future question I still think we should talk about, because I think it is really interesting.
But I realize that it is actually we might need to ask the robot race question even sooner than that while the machines are still, you know, quote unquote, really, you know, dumb and not on our level because of the way that we perceive them.
And I think one of the lessons we learn from looking at the history of animal rights and one of the reasons we may not get to a place in 100 years where we view it as wrong to, you know, eat or otherwise, you know, use animals for our own purposes is because historically we have always protected those things that we relate to the most.
So one example is whales. No one gave a shit about the whales. Am I allowed to swear?
You swear as much as you want freedom.
Yeah. No one gave a shit about the whales until someone recorded them singing. And suddenly people were like, oh, this is a beautiful creature and now we need to save the whales.
And that started the whole Save the Whales movement in the 70s. So. I am as much as I am, and I think a lot of people want to believe that we care about consistent biological criteria, that is not historically how we formed our alliances is a what?
Why do we why do we believe that all humans are created equal killing of a human being, no matter who the human being is?
that is what I meant by equality is bad.
And then because I am connecting that to robots and I am wondering whether mortality. So the killing act is what makes something that is the fundamental first. Right. So I am I am currently allowed to take a shotgun and shoot a Roomba.
I think I am not sure, but I am pretty sure it is not considered murder, right, or even shutting them off. So that is that is where the line appears to be right. Is its mortality a critical thing here?
I think here, again, like the animal analogy is really useful because you are also allowed to shoot your dog, but people will not be happy about it.
So we give we do give animals certain protections from like.
You know, you are not allowed to torture your dog and set it on fire, at least in most states and countries, you know, but you are still allowed to treat it like a piece of property and a lot of other ways. And so we draw these, you know, arbitrary lines all the time. And, you know, there is a lot of philosophical thought on why viewing humans as something unique.
Is not is just speciesism and not, you know, based on any criteria that would actually justify making a difference between us and other species, do you think in general people.
Most people are good. Do you think do you think there is evil and good in all of us? that is revealed through our circumstances and through our interactions. I like to view myself as a person who believes that there is no absolute evil and good and that everything is, you know, gray. But I do think it is an interesting question, like when I see people being violent towards robotic objects.
You said that bothers you because the robots might someday, you know, be smart. And is that what really bothers me?
Because it reveals. So I personally believe because I have studied Wagemans, I am Jewish, I studied the Holocaust and World War Two, except, well, I personally believe that most of us have evil in us. That. What bothers me is the abuse of robots reveals that evil in human beings. Yeah, and I think it does not bother me. it is I think it is an opportunity for roboticists to make help people find the better angels of our nature right now. That abuse is not just a fun side thing.
that is a you revealing a dark part that you should not there should be hidden deep inside.
Yeah, I mean, you laugh, but some of our research does indicate that maybe people's behavior towards robots reveals something about their tendencies for empathy, generally, even using very simple robots that we have today that clearly do not feel anything.
So, you know, Westworld is maybe, you know, not so far off. And it is like, you know, depicting the bad characters as willing to go around and shoot and rape the robots and the good characters is not wanting to do that, even without assuming that the robots have consciousness.
So there is an opportunity. it is an opportunity to almost practice empathy. The robots is an opportunity to practice empathy. I agree with you. Some people would say, why are we practicing empathy on robots instead of on our fellow humans or on animals that are actually alive and experience the world?
And I do not agree with them because I do not think empathy is a zero sum game. And I do think that it is a muscle that you can train and that we should be doing that.
But some people disagree. So the interesting thing I have heard, you know, raising kids a sort of. Asking them or telling them to be nice to the smart speakers, to Alexa and so on, saying please and so on during the requests, I do not know if I am a huge fan of that idea because that is towards the idea of practicing empathy.
I feel like politeness. I am always polite to all the all the systems that we build, especially anything that speech interaction based like when we talk to the car, I always have a pretty good detector for please to I feel like there should be a room for encouraging empathy in those interactions. Yeah, OK, so I agree with you, so I am going to play devil's advocate here. So what is the what is the devil's advocate argument there?
The devil's advocate argument is that if you are the type of person who has abusive tendencies or needs to get some sort of like behavior like that out needs an outlet for it, that it is great to have a robot that you can scream at so that you are not screaming at a person.
And we just do not know whether that is true, whether it is an outlet for people or whether it just kind of, as my friend once said, trains their cruelty muscles.
It makes them more cruel in other situations.
Oh, boy. Yeah. And that expands to other topics which that I do not know. You know, there is a topic of sex, which is weird, one that I tend to avoid from robotics perspective.
And most of the general public does not. They talk about sex robots and so on. Is that an area you have touched at all? Research was. I like the way, because that is what people imagine sort of any kind of interaction between humans and robots shows any kind of compassion. They immediately think from a product perspective in the near term is sort of expansion of what pornography is and all that kind of stuff.
Yeah, that is kind of you to like characterize it as though they are thinking rationally about product. I feel like sex robots are just such a titillating news hook for people that they become like the story.
And it is really hard to not get fatigued by it when you are in the space because you tell someone you do human robot interaction. Of course, the first thing they want to talk about is sex robots. He said, yeah, it happens a lot. And it is it is unfortunate that I am so fatigued by it, because I do think that there are some interesting questions that become salient when you talk about sex with robots. See what I think would happen when people get sex robots, like if some guys like guys get female sex robots, where I think there is an opportunity for is an actual like like they will actually interact.
What I am trying to say, they will not outside of the sex will be the most fulfilling part, like the interaction is like the folks who there is movies and the street who pay a prostitute and then end up just talking to her the whole time. They feel like there is an opportunity. it is like most guys and people in general joke about the sex act. But really people are just lonely inside.
they are looking for a connection. Many of them.
And it would be unfortunate if that it is that connection is established through the sex industry.
I feel like it should go into the front door of like people are lonely and they want a connection.
Well, I also feel like we should kind of deep, you know, destigmatize the sex industry because, you know, even prostitution, like they are prostitutes that specialize in disabled people who do not have the same kind of opportunities to explore their sexuality.
So it is I feel like we should, like, destigmatize all of that generally.
Yeah, but yeah, that connection, that loneliness is an interesting topic that you bring up, because while people are constantly worried about robots replacing humans and oh, if people get sex robots and the sex is really good, then they will not want their partner or whatever.
But we rarely talk about robots actually filling a hole where there is nothing. Yeah.
And what benefit that can provide to people.
Yeah. I think that is an exciting there is a whole there is a giant hole that is unfillable by humans is asking too much of your people, your friends and people you are in a relationship with in your family to fill that hole because you know, it is exploring.
The full, you know, exploring the full complexity and richness of who you are. Who are you really like people? Your family does not have enough patience to really sit there and listen to who are you really? And I feel like there is an opportunity to really make that connection with robots.
I just feel like we are complex as humans and we are capable of lots of different types of relationships. So whether that is with family members, with friends, with our pets or with robots, I feel like there is space for all of that. And all of that can provide value in a different way. Yeah, absolutely, so I am jumping around currently, most of my work is autonomous vehicles, so the most popular topic among the general public is the trolley problem.
So most, most, most roboticists, the kind of hate this question. But what do you think of this thought experiment? What do you think we can learn from it outside of the silliness of the actual application of it to the autonomous vehicle? I think it is still an interesting ethical question. And that in itself, just like much of the interaction with robots, has something to teach us. But from your perspective, do you think there is anything there?
Well, I think you are right that it does have something to teach us because. But but I think what people are forgetting and all of these conversations is the origins of the trolley problem and what it was meant to show us, which is that there is no right answer and that sometimes our moral intuition that comes to us instinctively is not actually what we should follow if we care about creating systematic rules that apply to everyone.
So I think that as the philosophical concept, it could teach us at least that. But that is not how people are using it right now like we have. And these are friends of mine and like I love them dearly and their project as a lot of value.
But if we are viewing the moral machine project as what we can learn from the trolley problems, the moral machine is I am sure you are familiar. it is this website that you can go to and it gives you different scenarios like, oh, you are in a car, you can decide to run over, you know, these two people or this child, you know, what do you choose? Do you choose the homeless person? You choose the person who is jaywalking. And so it pits these like moral choices against each other and then tries to crowdsource the quote unquote, correct answer, which is really interesting and I think valuable data.
But I do not think that is what we should base our rules in autonomous vehicles on, because it is exactly what the trolley problem is trying to show, which is your first instinct might not be the correct one if you look at rules that then have to apply to everyone and everything.
So how do we encode these ethical choices in interaction with robots? So, for example, the autonomous vehicles, there is a serious ethical question of do I protect myself?
Does my life have higher priority than the life of another human being, because that changes certain control decisions that you make, so if your life matters more than other human beings, then you would be more likely to swerve out of your current lane.
So currently, the automated emergency braking systems that just break, they do not ever swerve.
Right. So swerving into oncoming traffic or or no. Just in a different lane can because significant harm to others. But it is possible that it causes less harm to you. So that is a difficult ethical question.
Do you. You do you do you have a hope that. Like the trolley problem is not supposed to have a right answer, right? Do you hope that when we have robots at the table will be able to discover the right answer for some of these questions? Well, what is happening right now, I think, is this this question that we are facing of, you know, what ethical rules should we be programming into the machines is revealing to us that our ethical rules are much less programmable than we probably thought before.
And so that is a really valuable insight. I think that that these issues are very complicated and that in in a lot of these cases, it is you can not really make that call, like not even as a legislator.
And so what is going to happen in reality, I think, is that, you know. Car manufacturers are just going to try and avoid the problem and avoid liability in any way possible or like they are going to always protect the driver because who is going to buy a car if it is programmed to kill someone, kill, kill you instead of someone else? So that is what is going to happen in reality. But what did you mean by once we have robots at the table, like, do you mean when they can help us figure out what to do?
No, I mean when robots are part of the ethical decisions. So, no, no, not they help us. Well. Uh oh, you mean when it is like I run over a robot or a person, right, that kind of thing. So what went on?
So when you it is exactly what you said, which is when you have to encode the ethics into an algorithm, you start to try to really understand what are the fundamentals of the decision making process you make to make certain decisions, such as you, um, like capital punishment, should you take a person's life or not to punish them for a certain crime?
Sort of. You can use you can develop an algorithm to make that decision.
Right. And the hope is that the act of making that algorithm. However you make it so, there is a few approaches that will help us actually get to the core of what is right and what is wrong under our current societal standards.
But is not that what is happening right now? And we are realizing that we do not have a consensus on what is right and wrong. I mean, in politics in general?
Well, like when we are thinking about these trolley problems and autonomous vehicles and how to program ethics into machines and how to, you know, make. Make A.I. algorithms fair and equitable, we are realizing that this is so complicated and it is complicated in part because there is does not seem to be a one right answer in any of these case.
Do hope for like one of the ideas of the moral machine is that crowdsourcing can help us converge towards like democracy can help us converge towards the right answer. Do you have a hope for crowdsourcing?
Well, yes and no. So I think that in general, you know, I have a legal background and policymaking is often about trying to suss out, you know, what rules does this society's particular society agree on and then trying to codify that. So the law makes these choices all the time and then tries to adapt according to changing culture. But in the case of the moral machine project, I do not think that people's choices on that website necessarily necessarily reflect what laws they would want in place if given.
I think you would have to ask them a series of different questions in order to get what their consensus is. I agree with that.
That has to do more with the artificial nature of I mean, they are showing some cute icons on a screen that is that is almost so if you, for example, who do a lot of work in virtual reality, as if you make if you put those same people into virtual reality where they have to make that decision, the decision would be very different.
I think I agree with that. that is one aspect. And the other aspect is it is a different question to ask someone, would you run over the homeless person or the doctor in this scene or do you want cars to always run over the homeless people?
I say, yeah. So let us talk about anthropomorphism to me. Anthropomorphism, if I can pronounce it correctly, is is one of the most fascinating phenomena from both engineering perspective and psychology perspective, machine learning perspective and robotics in general. Can you step back and define anthropomorphism how you see it?
In general terms, in your in your work. Sure, so anthropomorphism is this tendency that we have to project human like traits and behaviors and qualities onto non humans.
And we often see it with animals like, well, we will project emotions on animals that may or may not actually be there.
We often see that we are trying to interpret things according to our own behavior when we get it wrong. But we do it with more than just animals. We do it with objects, you know, teddy bears. We see, you know, faces in the headlights of cars. And we do it with robots very, very, extremely.
Do think that can be engineered, can never be used to enrich an interaction. Oh yeah. And they I systemone and the human. Oh yeah.
For sure. And do you see it being used that way often.
Like. I do not. I have not seen whether it is Aleksa or any of the smart speaker systems often trying to optimize for the anthropomorphised and. You said you have not seen I have not seen that they keep moving away from that, I think they are afraid of that, that they actually so I only recently found out.
But did you know that Amazon has like a whole team of people who are just there to work on Alexa's personality?
So I know. Depends what you mean by personality. I did not know I did not know that exact thing. But I do know that the how the voice is perceived is worked on a lot. Well, if it is a pleasant feeling about the voice, but that has to do more with the texture of the sound and the audio and so on. But personality is more like. it is like, what is your favorite beer when you ask there and the personality team is different for every country to like, there is a different personality for a German Aleksa than there is for American Aleksa.
that is it.
I think it is very difficult to, you know, use the really, really harness the anthropomorphism with these voice assistance, because the voice interface is still very primitive. And I think that in order to get people to really suspend their disbelief and treat a robot like it is alive, less is sometimes more. You want them to project onto the robot and you want the robot to not disappoint their expectations for how it is going to answer behave in order for them to have this kind of illusion.
And with Alexa, I do not think we are there yet or Siri that just they are just not good at that. But if you look at some of the more animal like robots, like the baby seal that they use with the dementia patients, it is a much more simple design. does not try to talk to you, can not disappoint you in that way. It just makes little movements and sounds and people stroke it and it responds to their touch. And that is like a very effective way to harness people's tendency to kind of treat the robot like a living thing.
Yeah, so you bring up some interesting ideas in your paper chapter, I guess, anthropomorphic framing, human robot interaction that I read the last time schedule. This was a long time ago.
What are some good and bad cases of anthropomorphism in your perspective?
Like one of the good ones and bad?
Well, I should start by saying that, you know, while design can really enhance the anthropomorphism, it does not take a lot to get people to treat a robot like it is alive, like people will.
Over eighty five percent of rumbas have a name, which I do not know the numbers for your regular type of vacuum cleaner, but they are not that high.
Right. So people will feel bad for the Roomba when it gets stuck, they will send it in for repair and want to get the same one back and that that one is not even designed to, like, make you do that.
So I think that some of the cases where it is maybe a little bit concerning that anthropomorphism is happening is when you have something that is supposed to function like a tool and people are using it in the wrong way. And one of the concerns is military robots wear. So, gosh, 2000. Like early twenties, which is a long time ago, iRobot, the Rumbo company, made this robot called the pack bot that was deployed in Iraq and Afghanistan with the bomb disposal units that were there.
And the soldiers became very emotionally attached to the robots.
And that is fine until a soldier risks his life to save a robot, which you really do not want. But they were treating them like pets, like they would name them. They would give them funerals with gun salutes. They would get really upset and traumatized when the robot got broken. So you in situations where you want a robot to be a tool in particular, when it is supposed to do a dangerous job, they do not want a person doing it. It can be hard when people get emotionally attached to it.
that is maybe something that you would want to discourage. Another case for concern is maybe when companies try to leverage the emotional attachment to exploit people.
So if it is something that is not in the consumer's interest, trying to sell them products or services or exploit an emotional connection to keep them paying for a cloud service for a social robot or something like that might be, I think that is a little bit concerning as well.
Yeah, the emotional manipulation, which probably happens behind the scenes now with some like social networks and so on, but making it more explicit. what is your favorite robot? Like fictional or real, no real real robot, which you have felt a connection with or not like not not anthropomorphic connection, but I mean, like you sit back and sit down. This is an impressive. System. Wow, so two different robots, so the the Pleo baby dinosaur robot that is no longer sold, that came out in 2007.
That one I was very impressed with. It was.
But but from an anthropomorphic perspective, I was impressed with how much I bonded with it, how much I like wanted to believe that it had this inner life.
Can you describe the can you describe what it is? How big is it? What can actually do. Yeah.
Pleo is about the size of a small cat. It had a lot of like motors that gave it this kind of lifelike movement. It had things like touch sensors and an infrared camera. So it had all these cool little technical features, even though it was a toy. And the thing that really struck me about it was that it could mimic pain and distress really well.
So if you held it up by the tail, it had a tilt sensor that told it what direction it was facing and it would start to squirm and cry out. If you hit it too hard, it would start to cry. So it was very impressive in design.
And was the second robot that you were. You said there might have been two that you liked. Yeah. So the Boston Dynamics robots are just impressive feat of engineering. Have you met them in person?
Yeah, I recently got a chance to go visit. And, you know, I was always one of those people who watched the videos and was like, this is super cool. But also it is a product video. Like, I do not know how many times that they had to shoot this to get it right. But visiting them, I you know, I am pretty sure that I was very impressed, let us put it that way. Yeah.
In terms of the control, I think that was a transformational moment for me when I am at Spot Mini in person because. OK, maybe this is a psychology experiment, but I anthropomorphized the crap out of it, so I immediately it was like my best friend, right?
I think it is really hard for anyone to watch sport move and not feel like it has the agency.
Yeah, this movement, especially the arm on sport, many really ofay obviously looks like a head.
Yeah.
That and they say, no, we mean it that way, but obviously it looks exactly like that. And so it is almost impossible to not think of it as almost like the baby dinosaur, but slightly larger. And this movement of the of course the intelligence is their whole idea is that it is not supposed to be intelligent. it is a platform on which you build higher intelligence. it is actually a really, really dumb it is just a basic movement platform. Yeah, but even dumb robots can we can immediately respond to them in this visceral way.
What are your thoughts about Sophea, the robot, this kind of mix of some basic natural language processing and basically an art experiment?
Yeah, an art experiment is a good way to characterize it. I am much less impressed with Sofia than I am with Boston Dynamics. She said she likes you. She says she admires you. Is yeah.
She followed me on Twitter at some point. Yeah.
And she tweets about how much she likes you. So. So what does that mean? I have to be nice or not. I was emotionally manipulative.
You know, how how do you think of the whole thing that happened with Sofia is quite a large number of people kind of immediately had a connection and thought that maybe were far more advanced with robotics than we are or actually did not even think much. I was surprised how little people cared.
That they kind of assumed that, well, of course, I can do this. Yeah. And then they if they assume that I felt they should be more impressed. Yeah.
Well, you know what I mean. Like, really overestimate where we are. And so when something I do not even I do not even think the fear was very impressive or is very impressive.
I think she is kind of a puppet. To be honest. But yeah, I think people have are a little bit influenced by science fiction and pop culture to think that we should be further along than we are.
So what is your favorite robots in movies and fiction? Wally, Wally. What do you like about Wally, the humor, the cuteness?
The the perception control systems operating, and while that makes it over, just in general, the design of Wally the robot, I think that animators figured out, you know, starting in like the nineteen forties how to create characters that do not look real but look like something that is even better than real, that we really respond to and think is really cute. They figured out how to make them move and look in the right way. And Wally is just such a great example of that.
You think eyes, big eyes or big something that kind of ayash. So it is always playing on some.
Aspect of the human face, right, often, yeah, so big eyes. Well, I think one of the one of the first, like, animations to really play with this was Bambi, and they were not originally going to do that. They were originally trying to make the deer look as lifelike as possible, like they brought deer into the studio and had a little zoo there so the animators could work with them.
And then at some point they are like, if we make really big eyes and like a small nose and like big cheeks, kind of more like a baby face, then people like it even better than if it looks real.
Do you think the future of things like Aleksa in the home has a possibility to take advantage of that, to build on that, to create these systems that are better than real, that created close human connection?
I can pretty much guarantee you without having any knowledge that those companies are working on that on that design behind the scenes. Like, I can not be sure.
I totally disagree with you freely. So that is what I am interested in. I would like to build such a company. I know a lot of those folks and they are afraid of that because you do not. Well, how do you make money off of it? But even just like making Alexa look a little bit more interesting than just like a cylinder would do so much.
it is an interesting thought, but I do not think people from Amazon perspective are looking for that kind of connection.
They want you to be addicted to the services provided by Alexa, not to the device, so that the device itself, it is felt that you can lose a lot because if you create a connection and then it creates more opportunity for frustration for for negative stuff than it does for positive stuff.
I think the way they think about it, that is interesting. I agree that there is it is very difficult to get right and you have to get it exactly right.
Otherwise, you wind up with Microsoft's Clippy. OK, easy.
Now, what is what is your problem with Collopy? You like Clippy, your friend?
Yeah, I was just I just just talked to the we just had this argument and they said Microsoft CTO and he said he said he is not bringing Clippy back, they are not bringing Clippy back. And that is very disappointing. I think it was Clippy was the greatest assistance we have ever built. It was a horrible attempt, of course, but it is the best we have ever done because there was a real attempt to have it like an actual personality. And I mean, it was obviously technology was way not there at the time of being able to be a recommender system for assisting you in anything and typing in word or any kind of other application, but still was an attempt of personality that was legitimate that I thought was brave.
Yes. Oh, yes. OK, you know, you have convinced me I will be slightly less hard unclipped. And I know I have like an army of people behind me who awesome is Clippy.
So really, I want to meet these people. Who are these people? it is the people who like to hate stuff when it is there and and miss it when it is gone.
So everyone knows. Exactly. All right.
So Anqi and Djibo, the two companies, two amazing companies, the social robotics companies that have recently been closed down. Yes.
So why do you think it is so hard to create a personal robotics company? So making a business out of essentially something that people would anthropomorphize have a deep connection with, why is it so hard to make it work, the business case, not there or what is it? I think it is a number of different things. I do not think it is going to be this way forever.
I think at this point in time it takes so much work to build something that only barely meets people's like minimal expectations because of science fiction and pop culture, giving people this idea that we should be further than we already are.
Like when people think about a robot assistant in the home, they think about Rosie from the Jetsons or something like that. And Anqi and and you did such a beautiful job with the design and getting that interaction just right. But I think people just wanted more they wanted more functionality. I think you are also right that, you know, the business case is not really there because there has not been a killer application that is useful enough to get people to adopt the technology in great numbers.
I think what we did see from the people who did, you know, get Djibo is a lot of them became very emotionally attached to it. But that is not I mean, it is kind of like the Palm Pilot back in the day.
Most people are like, why do I need this? Why would I? They do not see how they would benefit from it until they have it or some other company comes in and makes it a little better. Yeah, how how far away are we? Do you think I mean, how hard is this problem? it is a good question and I think it has a lot to do with people's expectations. And those keep shifting depending on what science fiction is popular.
But also it is two things. it is people's expectation and people's need for an emotional connection. Yeah, and I believe the need is pretty high.
Yes. But I do not think we are aware of it. that is right.
there is like I really think this is like the life as we know it.
So we just kind of gotten used to it of really I hate to be dark because I have close friends, but we have gotten used to really never being close to anyone. All right.
And we are deeply I believe this is happening. I think we are deeply lonely, all of us, even those in deep, fulfilling relationships. In fact, what makes us so fulfilling, I think, is that they at least tap into that deep loneliness a little bit. But I feel like there is more opportunity to explore that.
That does not it does not interfere with the human relationships you have. It expands more on the that. Yeah. The rich, deep, unexplored complexity. that is all of us weird apes.
OK, I think you are right.
Do you think it is possible to fall in love with a robot? Oh, yeah, totally.
Do you think it is possible to have a long term committed, monogamous relationship with the robot? Well, yeah, there are lots of different types of long term, committed, monogamous relationships, I think monogamous implies like you are not going to see other humans sexually or like you basically on Facebook have to say I am in a relationship with this person, this robot I just do not like.
Again, I think this is comparing robots to humans when I would rather compare them to pets like you get a robot, it fulfills, you know, this loneliness that you have in maybe not the same way as a pet, maybe in a different way. That is even, you know, supplemental in a different way.
But, you know, I am not saying that people will not, like, do this, be like, oh, I want to marry my robot or I want to have like a sexual relationship, monogamous relationship with my robot. But I do not think that that is the main use case for them. But you think that there is still a gap between human and Pat? So between husband and pet, there is a different relationship in engineering, so that that is a gap that can be closed through.
I think it could be closed someday, but why would we close that? Like, I think it is so boring to think about recreating things that we already have when we could when we could create something that is different.
I know you are thinking about the people who do not have a husband and what can we give them?
Yeah, but I guess what I am getting at is. Maybe not so like the movie her. Yeah, right, so. A better husband, well, maybe better in some ways, like it is I do think that robots are going to continue to be a different type of relationship, even if we get them like very human looking or when, you know, the voice interactions we have with them feel very like natural and human, like, I think there is still going to be differences.
And there were in that movie, too, like towards the end, guy goes off the rails is just a movie so that your intuition is. But that because because you kind of said do things right.
So one is why would you want.
To basically replicate the husband, yeah, and the other is kind of implying that it is kind of hard to do so like anytime you try, you might build something very impressive, but it will be different.
I guess my question is about human nature is how hard is it to satisfy that role of the husband?
So removing any of the sexual stuff aside is the is more like the mystery, the tension, the dance of relationships.
You think with robots, that is difficult to build listening to. I think that.
Well, it also depends on are we talking about robots now in 50 years in like indefinite amount of time where I am thinking like five or ten years, five or ten years, I think that robots at best will be like a it is more similar to the relationship we have with our pets than relationship that we have with other people.
I got it.
So what do you think it takes to build a system that exhibits greater and greater levels of intelligence, like impresses us with this intelligence, you know, a Roomba. So you talk about anthropomorphised and that does not I think intelligence is not required.
In fact, intelligence probably gets in the way sometimes, like you mentioned, but.
What do you think it takes to create a system where we sense that it has a human level intelligence is something that probably something conversational human level, how hard do you think that problem is? it would be interesting to hear your perspective.
Not just purely to talk to a lot of people. How hard is the conversation with agents? Yeah, how hard is it to pass the Turing test?
But my sense is it is it is easier than just solving it is easier than solving the pure and natural language processing problem, because I feel like you can cheat.
Yeah. So, yeah, so how how hard is it to pass the Turing test, in your view? Well, I think, again, it is all about expectation management. If you set up people's expectations to think that they are communicating with what was it, a 13 year old boy from the Ukraine? that is right. And then they are not going to expect perfect English. they are not going to expect perfect understanding of concepts or even like being on the same wavelength in terms of like conversation flow.
So it is much easier to pass in that case. Do you think? You kind of alluded this to with audio. Do you think it needs to have a body? I think that we definitely have so we treat physical things with more social agency because we are very physical creatures, I think a body can be useful.
Does it get in the way, is there a negative aspects like, yeah, there can be. So if you are trying to create a body that is too similar to something that people are familiar with, like I have this robot cat at home that Hasbro makes and it is very disturbing to watch because I am constantly assuming that it is going to move like a real cat and it does not because it is like a one hundred dollar piece of technology.
So it is very disappointing and it is very hard to treat it like it is alive. So you can get a lot wrong with the body, too.
But you can also use tricks, same as, you know, the expectation management of the 13 year old boy from the Ukraine.
If you pick an animal that people are not intimately familiar with, like the baby dinosaur, like the BBC, all that people have never actually held in their arms, you can get away with much more because they do not have these preformed expectations.
Yeah, no. You having a TED talk or something that clicked for me that nobody actually knows what a dinosaur looks like. So you can actually get away with a lot more.
That was great. Do you think he needs. So what do you think about consciousness and mortality being displayed in the robot? So not actually. Having consciousness, but having these kind of human elements that are much more than just the interaction, much more than just like you mentioned, with a dinosaur moving kind of in interesting ways, but really being worried about its own death and really acting as if it is aware and self-aware and identity. Have you seen that done in robotics?
Would you think about doing that?
Is that a is that a powerful good thing?
Well, it is a I think it can be a design tool that you can use for different purposes. So I can not say whether it is inherently good or bad, but I do think it can be a powerful tool.
The fact that the, you know, PLEO mimics distress when you quote unquote hurt it is is a really powerful tool to get people to engage with it in a certain way.
I had a research partner that I did some of the empathy work with Nandy, and he had built a robot for himself that had like a life span and that would stop working after a certain amount of time just because he was interested in, like, whether he himself would treat it differently.
And we know from, you know, Tamagotchi those like those little games that that we used to have that were extremely primitive, that like people respond to like this idea of mortality.
And, you know, you can get people to do a lot with a little design tricks like that. Now, whether it is a good thing depends on what you are trying to get them to do. Have a deeper relationship, have a deeper connection, so a relationship if it is for their own benefit. That sounds great.
OK, well, you see, you do that for a lot of other reasons. I see. So what kind of stuff are you worried about, though? Is it mostly about manipulation of your emotions for like advertising and so on? Things like that? Yeah. Or data collection or I mean, you could think of governments misusing this to extract information from people.
it is, you know, just just like any other technological tool just raises a lot of questions.
What if you if you look at Facebook, if you look at Twitter and social networks, there is a lot of concern of data collection.
Now, what is from the legal perspective or in general, how do we prevent the violation of sort of these companies crossing a line?
it is a gray area, but crossing a line they should not in terms of manipulating, like we are talking about manipulating our emotion, manipulating our behavior, using tactics that are not so savory.
Yeah, it is it is really difficult because. We are starting to create technology that relies on data collection to provide functionality, and there is not a lot of incentive, even on the consumer side, to curb that, because the other problem is that the harms are not tangible. they are not really apparent to a lot of people because they kind of trickle down on a societal level.
And then suddenly we are living in like 1984, which, you know, sounds extreme. But I read that book was very prescient. And I am not worried about, you know, these systems. You know, I I have, you know, Amazon echo at home and tell Alexa all sorts of stuff. And and it helps me because, you know, Alexa knows what brand of diaper we use and so I can just easily order it again. So I do not have any incentive to, like, ask a lawmaker to curb that.
But when I think about that data then being used against, you know, low income people to target them for, you know, scam loans or education programs, that is then a societal effect that I think is very severe.
And, you know, legislators should be thinking about the gray area is. The removing ourselves from consideration of like of explicitly defining objectives and more saying, well, we want to maximize engagement in our social network.
Yeah. And and then just because you are not actually doing a bad thing, it makes sense. You want people to to keep a conversation going, to have more conversations, to keep coming back again and again, to have conversations. And whatever happens after that, you are kind of not exactly directly responsible. you are only indirectly responsible. So I think it is a really hard problem.
Yeah, I know you are optimistic about us ever being able to solve it.
You mean the problem of capitalism? Well, it is because the problem is that the companies are acting in the companies interests and not in people's interests. And when those interests are aligned, that is great. But it is the completely free market does not seem to work because of this information asymmetry.
But it is hard to know how to say you try to do the right thing. I guess what I am trying to say is it is not obvious for these companies what the good thing for society is to do.
I do not think they sit there and with I do not know, with a glass of wine and a cat like petting a cat, evil cat.
And there is two decisions. And one of them is good for society. One is good for the other for the profit. And they choose the profit.
I think they actually there is a lot of money to be made by doing the right thing for society like that, because just Google, Facebook have so much cash that they actually watch, especially Facebook will significantly benefit from making decisions that are good for society.
it is good for their brand. Right.
So but I do not know if they know what society that is the we I do not think we know what is good for society in terms of how.
Yeah.
How we manage the conversation on Twitter or how we design the we are talking about robots like should it should we emotionally manipulate you into having a deep connection with Alexa or not?
Yeah, yeah, yeah. We do have optimism that we will be able to solve some of these questions. Well, I am going to say something that is controversial like in my circles, which is that I do not think that companies who are reaching out to ethicists and trying to create interdisciplinary ethics boards, I do not think that that is totally just trying to whitewash the problem and and so that they look like they have done something. I think that a lot of companies actually do, like you say, care about what the right answer is.
They do not know what that is and they are trying to find people to help them find them. Not in every case.
But I think I you know, it is much too easy to just vilify the companies as like you said, sitting there with their cat going for one million dollars.
that is not what happens. A lot of people are well-meaning, even within companies.
Um, I think that what we do absolutely need is more interdisciplinarity, both within companies, but also within the policymaking space, because we are you know, we have hurtled into the world where technological progress is much faster.
It seems much faster than it was. And things are getting very complex.
And you need people who understand the technology, but also people who understand what the societal implications are and people who are thinking about this in a more systematic way to be talking to each other. there is no other solution. I think you have also done work on intellectual property.
So if you look at the algorithms of these companies are using like YouTube, Twitter or Facebook, so on. I mean, that is kind of those are mostly secretive, the recommender systems behind behind this album. Do you think about it, IP and the transparency of algorithms like this? Like what? The responsibility of these companies to open source the algorithms or at least reveal to the public what how these algorithms work?
So I personally do not work on that. There are a lot of people who do, though, and there are a lot of people calling for transparency. In fact, Europe's even trying to legislate transparency. Maybe they even have at this point where, like if if an algorithmic system makes some sort of decision that affects someone's life, that you need to be able to see how that decision was made.
I you know, it is it is a it is a tricky balance because obviously companies need to have, you know, some sort of competitive advantage and you can not take all that away or you stifle innovation. But, yeah, for some of the ways that these systems are already being used, I think. It is pretty important that people understand how they work.
What are your thoughts in general on intellectual property in this weird age of software, AI robotics?
Oh, that is broken. I mean, the system is just broken. So can you describe.
Actually, I do not even know what intellectual property is in the space of software, what it means to mean.
So I believe I have a patent on a piece of software from my Ph.D..
You believe you do not know we went through a whole process. Yeah, I, I do get the spam emails like. Well, from your patent for you. Yes.
Much like a thesis. So but that is useless, right. Or not. Where does it stand in this age. What, what is, what is the right way to do it. what is the right way to protect and own ideas when it is just code and and this mishmash of something that feels much softer than a piece of machinery? Yeah, yeah.
I mean, it is hard because, you know, there are different types of intellectual property in there, kind of these blunt instruments. they are like it is like patent law is like a wrench, like it works really well for an industry like the pharmaceutical industry.
But when you try and apply it to something else, it is like, I do not know, I will just like hit this thing with the wrench and hope it works.
So software, you know, software, you have a couple of different options. Software I like any code that is written down in some tangible form is automatically copyrighted. So you have that protection. But that does not do much because if someone takes the basic idea that the code is executing and just does it in a slightly different way, they can get around the copyright. So there is not a lot of protection then you can patent software. But that is I mean. Getting a patent costs, I do not know if you remember what year cost or was it an institution?
Yes, it is a university. Yeah, they it was insane. There were so many lawyers, so many meetings. And it made me feel like it must have been hundreds of thousands of dollars. It was crazy. it is it is insane the cost of getting a patent. And so this idea of like protecting the, like, inventor in their own garage came up with great ideas.
Kind of that is the thing of the past. it is all just companies trying to protect things and it costs a lot of money. And then with code, it is oftentimes like, you know, by the time the patent is issued, which can take like five years, you know, probably your code is obsolete at that point.
So it is it is a very again, a very blunt instrument that does not work well for that industry. And so, you know, at this point, we should really have something better. But we do not do, like, opensource. Yeah, it is almost as good for society. I think all of us should open source code.
Well, so at the Media Lab at MIT, we have an open source default, because what we have noticed is that people will come in, they will write some code and they will be like, how do I protect this? And we are like, that is not your problem right now. Your problem, is not it? Someone's going to steal your project. Your problem is getting people to use it at all. Right?
Like there is so much stuff out there, like we do not even know if you are going to get traction for your work. And so open sourcing can sometimes help, you know, get people's work out there, but ensure that they get attribution for it, for the work that they have done. So I am a fan of it in a lot of contexts. Obviously, it is not like a one size fits all solution.
So what I glean from your Twitter is your mom I saw a quote, a reference to baby bot. What have you learned about robotics and A.I. from raising a human baby? But.
Well, I think that my child has made it more apparent to me that the systems we are currently creating are not like human intelligence, there is not a lot to compare there.
it is just he he has learned and developed in such a different way than a lot of the A.I. systems we are creating that. that is not really interesting to me to compare. But what is interesting to me is how these systems are going to shape the world that he grows up in. And so I am like even more concerned about kind of the societal effects of developing systems that rely on massive amounts of data collection, for example.
So as you are going to be allowed to use like Facebook or Facebook, it is over.
Kids do not use that Snapchat or do they use Instagram shots over to I do not know, I just read that tick tock is over, which I have never even seen. So I do not know.
I know we are old, we do not know Twitter, and we just I am going to start gaming and streaming my my gameplay. So what do you see as the future of.
Personal robotics, social robotics, interaction with the robots, like, what are you excited about, if you were to sort of philosophize about what might happen the next five, 10 years, that would be cool to see.
Oh, I really hope that we get kind of a home robot that makes it that the social robot and not just elect the like. it is you know, I really love the products. I thought Giba was had some really great aspects.
So I am hoping that a company crack's that made you so OK. It was wonderful talking to you today. Like, thank you so much.
it is fun. Thanks for listening to this conversation with Kate, darling, and thank you to our sponsors, Expressible and Masterclass. Please consider supporting the podcast by signing up to Master Class, a master class that works and getting Express VPN and Express VPN dot com slash leks pod. If you enjoy this podcast, subscribe on YouTube. Review five stars and a podcast supporter on Patrn or simply connect with me on Twitter. lex Friedemann. And now let me leave you with some tweets from Kate, darling, first tweet as the pandemic has fundamentally changed who I am.
I now drink the leftover milk in the bottom of the cereal bowl, second tweet is I came on here to complain that I had a really bad day and saw that a bunch of you are hurting to love to everyone. Thank you for listening and hope to see you next time.
