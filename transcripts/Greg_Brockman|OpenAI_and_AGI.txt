The following is a conversation with Greg Brockmann. he is the co-founder and CEO of Open Eye, a world class research organization, developing ideas and A.I. with a goal of eventually creating a safe and friendly artificial general intelligence, one that benefits and empowers humanity. Opening is not only a source of publications, algorithms, tools and data sets.
Their mission is a catalyst for an important public discourse about our future, with both narrow and general intelligence systems. This conversation is part of the Artificial Intelligence Podcast at MIT and beyond if you enjoy it. Subscribe on YouTube, iTunes or simply connect with me on Twitter at Lex Friedman spelled F.R. I.D.. And now here is my conversation with Greg Brockman. So in high school and right after you wrote a draft of a chemistry textbook, saw that that covers everything from basic structure, the atom to quantum mechanics.
So it is clear you have an intuition and a passion for both the the physical world with chemistry and now robotics to the digital world with a deep learning, reinforcement learning and so on.
Do you see the physical world in the digital world is different? And what do you think is the gap?
A lot of it actually boils down to iteration speed, right? That I think that a lot of what really motivates me is is building things right is the you know, think about mathematics, for example, where you think really hard about a problem. You understand that you are right down to this very obscure form they call proof. But then this is a humanities library, right? it is there forever. there is some truth that we have discovered. You know, maybe only five people in your field will ever read it, but somehow you kind of move humanity forward.
And so I actually used to really think that I was going to be a mathematician. And then I actually started writing this chemistry textbook. One of my friends told me you will never publish it because you do not have a Ph.D. So instead I decided to build a website and try to promote my ideas that way. And then I discovered programming. And you know that programming you think hard about a problem. You understand that you write down in a very obscure form that we call a program.
But then once again, it is in humanities library. Right. And anyone could get the benefit from it. And the scalability is massive. And so I think that the thing that really appeals to me about the digital world is that you can have this this this insane leverage. Right. A single individual with an idea is able to affect the entire planet. And that is something I think is really hard to do if you are moving around physical atoms.
But you said mathematics. So if you look at the what things over here our mind, do you ultimately see it as just math is just information processing or is there some other magic as you have seen if you have seen the biology and chemistry and so on?
Yeah, I think it is really interesting to think about humans is just information processing systems. And that, it seems like, is actually a pretty good way of describing a lot of kind of how the world works are a lot of what we are capable of. To think that that, you know, again, if you just look at technological innovations over time, that in some ways the most transformative innovation that we have had has been the computer. Right. In some ways, the Internet, you know, that what is the Internet done?
Right. The Internet is not about these physical cables. it is about the fact that I am suddenly able to instantly communicate with any other human on the planet. I am able to retrieve any piece of knowledge that in some ways the human race has ever had and that those are these insane transformations.
Do you see the our society as a whole, the collective as another extension of the intelligence of the human being? So if you look at the human beings information processing system, you mentioned the Internet, the networking. Do you see us all together as a civilization, as a as a kind of intelligent system?
Yeah, I think this is actually a really interesting perspective to take and to think about that. You sort of have this collective intelligence of all of society. The economy itself is the super human machine that is optimizing something. Right. And it is also in some ways, a company has a will of its own right.
That you have all these individuals who are all pursuing their own individual goals and thinking really hard and thinking about the right thing to do. But somehow the company does something that is this emergent thing and that is it is a really useful abstraction. And so I think that in some ways, you know, we think of ourselves as the most intelligent things on the planet and the most powerful things on the planet. But there are things that are bigger than us that are these systems that we all contribute to.
And so I think actually, you know, it is a it is interesting to think about if you have read Isaac Asimov Foundation, right. That that there is this concept of psychohistory in there, which is effectively this, that if you have trillions or quadrillions of of beings, then maybe you could actually predict what that being that that huge macro being will do and almost independent of what the individuals want.
And I actually have a second angle on this I think is interesting, which is thinking about technological determinism.
One thing that I actually think a lot about with with opening I right is that we are kind of coming on onto this insanely transformational technology of general intelligence.
Right. That will happen at some point.
And there is a question of how can you take actions that will actually steer it to go better rather than worse? And that I think one question I need to ask is, as a scientist, as an inventor, as a creator, what impact can you have in general? Right. You look at things like the telephone invented by two people on the same day, like what does that mean? Like what does that mean about the shape of innovation? And I think that what is going on is everyone's building on the shoulders of the same giants.
And so you can kind of you can not really hope to create something no one else ever would. You know, if Einstein was not born, someone else would have come up with relativity. You know, he changed the timeline a bit, right. That maybe it would take another 20 years, but it would not be that fundamentally humanity would never discover these these fundamental truths.
So there is some kind of invisible momentum that some people like Einstein are opening eyes, plugging into that anybody else can also plug into. And ultimately, that wave takes us into a certain direction. that is that is right. that is right, and, you know, this kind of seems to play out and a bunch of different ways that there is some exponential that is being written and that the exponential itself, which one it is, changes. Think about Moore's Law. An entire industry set its clock to it for 50 years.
Like, how can that be right? How is that possible? And yet somehow it happened. And so I think you can not hope to ever invent something that no one else will. Maybe you can change the timeline a little bit, but if you really want to make a difference, I think that the thing that you really have to do, the only real degree of freedom you have is set the initial conditions under which a technology is born.
And so you think about the Internet, right, that there are lots of other competitors trying to build similar things and the Internet one, and that the initial conditions where that was created by this group that really valued people being able to be, you know, anyone being able to plug in this very academic mindset of of being open and connected. And I think that the Internet for the next 40 years really played out that way. You know, maybe today things are starting to shift in a different direction.
But I think that those initial conditions were really important to determine the next 40 years worth of progress.
that is really beautifully put. Another example of that I think about you know, I recently looked at it.
I looked at Wikipedia, the formation of Wikipedia, and I wonder what the Internet would be like if Wikipedia had ads. You know, there is an interesting argument that why they chose not to make it put advertising on Wikipedia. I think it is I think Wikipedia is one of the greatest resources we have on the Internet. And it is extremely surprising how well it works and how well it was able to aggregate all this kind of good information. And essentially the creator Wikipedia, I do not know, there is probably some debates there, but set the initial conditions and it carried itself forward.
that is really interesting. So you are the way you are thinking about ajai or artificial intelligence is you are focused on setting the initial conditions for the progress.
that is right. that is powerful. OK, so look into the future. If you create an AGI system like one that can answer Turing test natural language, what do you think would be the interactions you would have with it?
What do you think are the questions you would ask? Like what would be the first question you would ask it for him? that is right.
I think that at that point, if you have really built a powerful system that is capable of shaping the future of humanity, the first question that you really should ask is how do we make sure that this plays out well? And so that is actually the first question that I would ask a powerful ajai system is so you would not ask your colleague, you would not ask like Illia, you would ask the system.
Oh, we have already had the conversation with Iliya. Right. And everyone here. And so you want as many perspectives and piece of wisdom as you can for for answering this question. So I do not think you necessarily defer to whatever your powerful system tells you, but you use it as one input to try to figure out what to do. But I guess fundamentally what it really comes down to is if you have built something really powerful and you think about it, think about, for example, the creation of of shortly after the creation of nuclear weapons.
Right. The most important question in the world was what is the world order going to be like? How do we set ourselves up in a place where we are going to be able to survive as a species with ajai?
I think the question slightly different, right, that there is a question of how do we make sure that we do not get the negative effects. But there is also the positive side, right? You imagine that. You know, like like what will not it be like like what will be capable of? And I think one of the core reasons that in Ajai can be powerful and transformative is actually due to technological development. Right. If you have something that is capable, as capable as a human and that it is much more scalable, that you absolutely want that thing to go read the whole scientific literature and think about how to create cures for all the diseases.
Right. You want to think about how to go and build technologies to help us create material abundance and to figure out societal problems that we have trouble with, like how are we supposed to clean up the environment?
And, you know, maybe you want this to go and invent a bunch of little robots that'll go out and be biodegradable and turn ocean debris into harmless molecules.
And I think that that that positive side is something that I think people miss sometimes when thinking about what an ajai will be like.
And so I think that if you have a system that is capable of all of that, you absolutely want its advice about how do I make sure that we are using your your capabilities in a positive way for humanity.
So what do you think about that psychology that looks at all the different possible trajectories of an ajai system, many of which perhaps the majority of which are positive and nevertheless focuses on the negative trajectories? And you get to interact with folks who get to think about this maybe within yourself as well. You look at Sam Harris and so on. It seems to be sorry to put it this way, but almost more fun to think about the negative possibilities, whatever that is deep in our psychology.
What do you think about that and how do we deal with it? Because we want to help us.
So I think there is kind of two problems entailed in that question. The first is more of the question of how can you even picture what a world with a new technology will be like? Now, imagine we are in nineteen fifty and I am trying to describe Uber to someone.
Apps and the Internet. Yeah, I mean, yeah, that is that is going to be extremely complicated, but it is imaginable. it is imaginable, right.
But and now imagine being in 1950 and predicting Uber. Right. And you need to describe the Internet. You describe GPS. You need to describe the fact that everyone's going to have this phone in their pocket.
And so I think that that just the first truth is that it is hard to picture how a transformative technology will play out in the world. we have seen that before with technologies that are far less transformative than ajai will be. And so I think that that one piece is that is just even hard to imagine and to really put yourself in a world where you can predict what that that positive vision would be like.
And, you know, I think the second thing is that it is I think it is always easier to support the negative side than the positive side. it is always easier to destroy than create. And, you know, in in a physical sense, a more just in an intellectual sense. Right. Because, you know, I think that with creating something, you need to just get a bunch of things right and to destroy you just need to get one thing wrong.
Yeah. And so I think that what that means is that I think a lot of people's thinking dead ends as soon as they see the negative story.
But that being said, I actually actually have some hope. Right. I think that the that the positive vision is something that I think can be is something that we can we can talk about. And I think that just simply saying this fact of. Yeah, like, that is positive. there is negatives. Everyone likes to dwell on the negative people that respond well to that message and say, ha, you are right, there is a part of us that we are not talking about, not thinking about.
And that is actually something that is that is that is, I think really been a key part of how we think about ajai at open. I right. You can kind of look at it as like, OK, like opening up talks about the fact that there are risks and yet they are trying to build the system. Like, how do you square that those two facts?
So do you share the intuition that some people have?
I mean, from Sam Harris to even Elon Musk himself, that it is tricky as you develop ajai to keep it from slipping into the existential threats, into the negative.
what is your intuition about how hard is it to keep a development on the positive track?
And what is your intuition there?
To answer the question, you can really look at how we structure open A.I. So we really have three main arms that we have capabilities which is actually doing the technical work and pushing forward what these systems can do. there is safety, which is working on technical mechanisms to ensure that the systems we build are aligned with human values. And then there is policy which is making sure that we have governance mechanisms answering that question of, well, who is values. And so I think that the technical safety one is the one that people kind of talk about the most.
Right. And you talk about like think about, you know, all of the dystopic AI movies. A lot of that is about not having good technical safety in place. And what we have been finding is that, you know, I think that actually a lot of people look at the technical safety problem and think it is just intractable. Right. This question of what do humans want? How am I supposed to write that down? Can I even write down what I want?
No way. And then they stop there. But the thing is, we have already built systems that are able to learn things that humans can not specify, you know, even the rules for how to recognize if there is a cat or dog in an image. Turns out it is intractable to write that down, and yet we are able to learn it and that what we are seeing with systems, we build it open and they are still in early proof of concept stage is that you are able to learn human preferences.
you are able to learn what humans want from data. And so that is kind of the core focus for our technical safety team. And I think that they are actually we have had some pretty encouraging updates in terms of what we have been able to make work.
So you have an intuition and a hope that from data, you know, looking at the value alignment problem from data, we can build systems that align with the collective better angels of our nature. So align with the ethics and the morals of human beings to even say this in a different way.
I mean, think about how how do we align humans, right? Think about like a human baby can grow up to be an evil person or a great person. And a lot of that is from learning from data. Right. That you have some feedback as a child is growing up. They get to see positive examples. And so I think that that just like that, the only example we have of a general intelligence that is able to learn from data to align with human values and to learn values, I think we should not be surprised that we can do the same sorts of of techniques or whether the same sort of techniques end up being how we we saw value for ages.
So let us go even higher. I do not know if you have read the book Sapience, but there is an idea that you know that as a collective, as us human beings have kind of developed together and ideas that we hold, there is no in that context, objective truth. We just kind of all agree to certain ideas and hold them as a collective.
Did you have a sense that there is in the world of good and evil, do you have a sense to the first approximation, there are some things that are good and that you could teach systems to behave to be good.
So I think this actually blends into our 13 right. Which is the policy team. And this is the one aspect I think people really talk about way less than they should. Right. Because imagine that we build super powerful systems that we have managed to figure out all the mechanisms for these things to do whatever the operator wants. The most important question becomes who is the operator? What do they want and how is that going to affect everyone else? Right. And and I think that this question of what is good, what are those values?
I mean, I think you do not even have to go to those those very grand existential places to start to realize how hard this problem is. You just look at different countries and cultures across the world and that there is there is a very different conception of how the world works. And, you know, what what what kinds of ways that society wants to operate. And so I think that that that the really core question is, is actually very concrete. And I think it is not a question that we have ready answers to.
Right.
Is how do you have a world where all the different countries that we have the United States, China, Russia and, you know, the hundreds of other countries out there are able to continue to not just operate in the way that they see fit, but in the world that emerges in these where you have these very powerful systems operating alongside humans ends up being something that empowers humans more, that makes like human existence be a more meaningful thing, and that people are happier and wealthier and able to live more fulfilling lives.
it is not an obvious thing for how to design that world once you have that very powerful system.
So if we take a little step back and we are having like a fascinating conversation and open as in many ways a tech leader in the world, and yet we are thinking about these big existential questions, which is fascinating, really important. I think you are a leader in that space. And it is a really important space of just thinking how I affect society in a big picture view.
So Oscar Wilde said we are all in the gutter, but some of us are looking at the stars. And I think Open has a charter that looks to the stars, I would say, to create intelligence, to create general intelligence, make it beneficial, safe and collaborative. Can you tell me?
How that came about, how a mission like that and the path to creating a mission like that opening I was found.
Yeah. So I think that in some ways it really boils down to taking a look at the landscape right now.
If you think about the history of A.I., that basically for the past 60 or 70 years, people have thought about this goal of what could happen if you could automate human intellectual labor.
Right. Imagine you could build a computer system that could do that. What becomes possible?
We have a lot of sci fi that tells stories of various dystopias and, you know, increasingly of movies like her that tell you a little bit about maybe more of a little bit utopian vision. You think about the impacts that we have seen from being able to have bicycles for our minds and computers, and that I think that the that the impact of computers and the Internet has just far outstripped what anyone really could have predicted. And so I think that it is very clear that if you can build an ajai, it will be the most transformative technology that humans will ever create.
And so what it boils down to then is a question of, well, is there a path? Is there hope? Is there a way to build such a system? And I think that for 60 or 70 years that people got excited and that, you know, ended up not being able to deliver on the hopes that the people had pinned on them. And I think that then, you know, that after, you know, two two winters of development, the people I, you know, I think kind of almost stopped daring to dream.
Right. Really talking about ajai or thinking about ajai became almost this taboo in the community.
But I actually think that people took the wrong lesson from history. And if you look back starting in 1959 is when the Perceptron was released. And this is basically, you know, one of the earliest neural networks, it was released to what was perceived as this massive overhype. So in The New York Times in 1959, you have this article saying that, you know, the Perceptron will one day recognize people, call their names, instantly translate speech between languages and people at the time looked at this and said, this is your system, can not do any of that.
And basically spent 10 years trying to discredit the whole Perceptron direction. And it succeeded and all the funding dried up. And, you know, people kind of went in other directions. And, you know, the 80s there was a resurgence. And I would always heard that the resurgence in the 80s was due to the invention of back propagation and these these algorithms that got people excited. But actually, the causality was due to people building larger computers that you can find these these articles from the 80s saying that the democratization of computing power suddenly meant you could run these larger neural networks.
And then people started to do all these amazing things back propagation algorithm was invented. And you know that the neural nets people were running were these tiny little like 20 neuron neural nets. Right. Right. Like, what are you supposed to learn with 20 neurons?
And so, of course, they were not able to get great results. And it really was not until 2012 that this approach that is almost the most simple natural approach that people have come up with in the 50s. Right. In some ways, even in the 40s before there were computers with Pitts' McCallan, their neuron. Suddenly, this became the best way of solving problems, right, and I think there are three core properties that deep learning has that I think are very worth paying attention to.
The first is generality with a very small number of deep learning tools, deep, deep neural net, maybe some some, you know, RL and it solves this huge variety of problems, speech recognition, machine translation, game playing, all of these problems, a small set of tools. So there is the generality. there is a second piece, which is the competence. You want to solve any of those problems throughout 40 years worth of normal computer vision research replaced with deep neural net.
it is going to work better. And there is a third piece, which is the scalability. Right? The one thing that has been shown time and time again is that you if you have a larger neural network through more compute, more data, add it, it will work better. Those three properties together feel like essential parts of building a general intelligence.
Now, it does not just mean that if we scale up what we have, that we will have an ajai right.
There are clearly missing pieces. There are missing ideas. We need to have answers for reasoning. But I think that the core here is that for the first time, it feels that we have a paradigm that gives us hope the general intelligence can be achievable. And so as soon as you believe that everything else becomes comes into focus. Right. If you imagine that you may be able to and you know that the timeline, I think, remains uncertain. But I think that that, you know, certainly within our lifetimes and possibly within a much shorter period of time than than people would expect, if you can really build the most transformative technology that will ever exist, you stop thinking about yourself so much.
Right. And you start thinking about just like how do you have a world where this goes well and that you need to think about the practicalities of how do you build an organization and get together a bunch of people and resources and to make sure that people feel motivated and ready to do it.
But I think that then you start thinking about, well, what if we succeed and how do we make sure that when we succeed that the world is actually the place that we want ourselves to exist? And, you know, almost the results in Bell sense of the word. And so that is kind of the broader landscape and opening. I was really formed in 2015 with that high level picture of ajai might be possible sooner than people think and that we need to try to do our best to make sure it is going to go well.
And then we spent the next couple of years really trying to figure out what does that mean, how do we do it? And, you know, I think that typically with a company, you start out very small CEO and co-founder, and you build a product, you get some users, you get product market fit. You know, at some point you raise some money, you hire people, you scale, and then you go down the road.
Then the big companies realize you exist and try to kill you. And for opening I it was basically everything and exactly the opposite order.
Let me just pause for a second. He said a lot of things. Let me just admire the jarring aspect of what open A.I. stands for, which is daring to dream. I mean, you said it is pretty powerful. You caught me off guard because I think that is very true.
The the the step of just daring to dream about the possibilities of creating intelligence in a positive and a safe way. But just even creating intelligence is a much needed, refreshing catalyst for the AI community. So that is that is the starting point. OK, so then formation of open.
I, uh, what I would just say that, you know, when we were starting opening AI, that kind of the first question that we had is, is it too late to start a lab with a bunch of the best people possible?
That was an actual question was are those those are the core question of of you know, we have this dinner in July of twenty, twenty fifteen. And there is that was that was really what we spent the time talking about. And, you know, because you think about kind of where I was, is that transition from being an academic pursuit to an industrial pursuit. And so a lot of the best people were in these big research labs and that we want to start our own one that, you know, no matter how much resources we could accumulate, would be, you know, pale in comparison to the big tech companies.
And we knew that. And it was a question of are we going to be actually able to get this thing off the ground? You need a critical mass. You can not just do you and a co-founder build the product.
Right. You really need to have a group of, you know, five to 10 people. And we kind of concluded it was not obviously impossible. So it seemed worth trying.
Well, you are also a dreamer, so who knows, right? that is right.
OK, so speaking of that, competing with the with the big players, let us talk about some of the some of the tricky things as you think through this process of growing, of seeing how you can develop these systems at a at scale that competes.
So you recently formed OpenAir LP and you cap profit company that now carries the name OpenAir. So open your eyes.
Now, this official company, the original non-profit company, still exists and carries the open air and nonprofit name. So can you explain what this company is, what the purpose of its creation is, and how did you arrive at the decision? Yep.
To create it open I the whole entity and open the LP as a vehicle is trying to accomplish the mission of ensuring that artificial general intelligence benefits everyone. And the main way that we are trying to do that is by actually trying to build general intelligence ourselves and make sure the benefits are distributed to the world. that is the primary way. we are also fine if someone else does this right, it does not have to be us.
If someone else is going to build an AGI and make sure that the benefits do not get locked up in one company or, you know, one one, one with one set of people, like, we are actually fine with that. And so those ideas are baked into.
Our charter, which is kind of the the foundational document that, ah, that describes kind of our values and how we operate, but it is also really baked into the structure of open ILP And so the way that we set up opening ILP is that in the case where we succeed, right.
If we actually build what we are trying to build. Then investors are able to get a return and but that return is something that is capped. And so if you think of AIG in terms of the value that you could really create, you are talking about the most transformative technology ever created. it is going to create orders of magnitude more value than any existing company and that all of that value will be owned by the world like legally title to the nonprofit to fulfill that mission.
And so that is that is the structure.
So the mission is a powerful one. And it is it is one that I think most people would agree with.
it is how we would hope A.I. progresses.
And so how do you tie yourself to that mission?
How do you make sure you do not deviate from that mission, that, you know, other incentives that are profit driven, what do not interfere with the mission?
So this is actually a really core question for us for the past couple of years, because, you know, I say that, like, the way that our history went was that for the first year we were getting off the ground. Right. We had this high level picture, but we did not know exactly how we wanted to accomplish it. And really two years ago, when we first started realizing in order to build ajai, we are just going to need to raise way more money than we can as a nonprofit.
And you are talking many billions of dollars.
And so the first question is, how are you supposed to do that and stay true to this mission?
And we looked at every legal structure out there, and you could have none of them are quite right for what we wanted to do. And I guess it should not be too surprising if you are going to do some, like, crazy, unprecedented technology that you are going have to come up with some crazy, unprecedented structure to do it in. And a lot of a lot of our conversation was with people at opening. I write the people who really joined because they believe so much in this mission and thinking about how do we actually raise the resources to do it and also stay true to to what we stand for.
And the place you have got to start is to really align on what is it that we stand for? What are those values? what is really important to us? And so I would say that we spent about a year really compiling the opening charter and that that determines and if you even look at the first the first line item in there, it says that, look, we expect we are going to have to marshal huge amounts of resources, but we are going to make sure that we minimize conflicts of interest with the mission.
And that kind of aligning on all of those pieces was the most important step towards figuring out how do we structure a company that can actually raise the resources to do what we need to do.
I imagine open I the decision to create open Alpay was a really difficult one, and there was a lot of discussions, as you mentioned, for a year. And there is different ideas, perhaps detractors within open I sort of different paths that you could have taken. What were those concerns? What were the different paths considered? What was that process of making that decision like?
But so if you look actually at the opening charter that there is almost two paths embedded within it. There is we are primarily trying to build ajai ourselves, but we are also OK if someone else does it. And this is a weird thing for a company.
it is really interesting, actually. Yeah. That there is an element of competition that you do want to be the one that does it. But at the same time, you OK somebody else's.
And we will talk about that a little bit. That tradeoff, that is the dance.
that is really interesting. And I think this was the core tension as we were designing opening ILP. And really the opening strategy is how do you make sure that both you have a shot at being a primary actor, which really requires building an organization, raising massive resources and really having the will to go and execute on some really, really hard vision. Right. You need to really sign up for a long period to go and take on a lot of pain and a lot of risk.
And to do that, normally you just import the startup mindset. Right. And that you think about, OK, like how do we how to execute everyone?
You have this very competitive angle, but you also have the second angle of saying that, well, the true mission is not for opening it to build CGI. The true mission is for Ajai to go well for humanity. And so how do you take all of those first actions and make sure you do not close the door on outcomes that would actually be positive and infill the mission? And so I think it is a very delicate balance. Right. And I think that going hundred percent, one direction or the other is clearly not the correct answer.
And so I think that even in terms of just how we talk about opening I and think about it, there is just like like one thing that is always in the back of my mind is to make sure that we are not just saying opening. Eyes goal is to build ajai. Right. That it is actually much broader than that. Right. That first of all, you know, it is not just aget safe ajai that is very important. But secondly, our goal is not to be the ones to build it.
Our goal is to make sure it goes well for the world. And so I think that figuring out how do you balance all of those and to to get people to really come to the table and compile the the like a single document that that encompasses all of that was not trivial.
So part of the challenge here is your mission is, I would say beautiful, empowering and a beacon.
I hope for people in the research community and just people think about I so your decisions are scrutinized more than I think, a regular profit driven company. Do you feel the burden of this in the creation of the charter and just in the way you operate?
Yes.
So why do you lean into the burden by creating such a charter? Why not keep it quiet?
I mean, it just boils down to the to the mission.
Like I am here and everyone else is here because we think this is the most important mission. I dare to dream. All right.
So what do you think?
You can be good for the world or create an ajai system that is good when you are a for profit company? From my perspective, I do not understand why profit interferes with the positive impact on society. I do not understand why Google makes most of its money from ads. can not also do good for the world or other companies, Facebook, anything. I do not I do not understand why those have to interfere. You can profit is not the thing, in my view that affects the impact of a company.
What affects the impact of the company is the charter is the culture, is the you know, the people inside and profit is the thing that just fuels those people. So what are your views there?
Yeah, so I think it is a really good question. And there is there is some some, you know, real like long standing debates in human society that are wrapped up in it.
The way that I think about it is just think about what what are the most impactful nonprofits in the world? What are the most impactful for profits in the world, right? Yes, much easier to list the for profits. that is right. And I think that there is there is some real truth here that the system that we set up, the system for, kind of how, you know, today's world is organized is one that that really allows for huge impact.
And that that kind of part of that is that you need to be that for profits are are self-sustaining and able to to kind of build on their own momentum. And I think that is a really powerful thing. it is something that when it turns out that we have not set the guardrails correctly, causes problems. Right. Think about logging companies that go into the forest. You know, the rainforest. that is really bad. We do not want that. And it is actually really interesting to me that kind of this this question of how do you get positive benefits out of a for profit company, it is actually very similar to how do you get positive benefits out of an AGI, right.
That you have this, like, very powerful system. it is more powerful than any human and it is kind of autonomous in some ways. You know, it is superhuman and a lot of axes. And somehow you have to set the guardrails to get good things to happen. But when you do, the benefits are massive. And so I think that that when when I think about nonprofit versus for profit, I think it is just not enough happens in nonprofits. they are very pure, but it is just kind of, you know, it is just hard to do things.
they are in for profits in some ways, like too much happens.
But if it kind of shaped in the right way, it can actually be very positive.
And so with our help, we are picking a road in between. Now, the thing I think is really important to recognize is that the way that we think about opening Alpay is that in the world where it actually happens, right. In a world where we are successful, we build the most transformative technology ever, the amount of value we are going to create will be astronomical. Mm hmm. And so then in that case, that the the cap that we have will be a small fraction of the value we create and the amount of value that goes back to investors and employees looks pretty similar to what would happen in a in a pretty successful startup.
And that is really the case that we are optimizing for, right, that we are thinking about in the success case, making sure that the value we create does not get locked up. And I expect another, you know, for profit companies that it is possible to do something like that.
I think it is not obvious how to do it right. I think that as a for profit company, you have a lot of fiduciary duty to your shareholders and that there are certain decisions that you just cannot make in our structure. we have set it up so that we have a fiduciary duty to the charter, that we always get to make the decision that is right for the charter rather than even if it comes at the expense of our own stakeholders. And and so I think that when I think about what is really important, it is not really about nonprofit versus for profit.
it is really a question of if you build ajai and you kind of, you know, communities now in this new age, who benefits, whose lives are better. And I think that what is really important is to have an answer that is everyone. Yeah.
Which is one of the core aspects of the charter.
So one concern people have not just with openness, but with Google, Facebook, Amazon, anybody really that that is creating impact at scale is how do we avoid, as your charter says, avoid enabling the use of Ajai to unduly concentrate power?
Why would not a company like open, I keep all the power of any system to itself, the charter, the charter. So how does the charter? Externalise itself in day to day, so I think that the first to zoom out right there, the way that we structure the company so that the power for sort of dictating the actions that opening it takes ultimately rests with the board or the board of the nonprofit. And the board is set up in certain ways, certain certain restrictions that you can read about in the opening ILP blog post.
But effectively, the board is the is the governing body for opening ILP and the board has a duty to fulfill the mission of the nonprofit. And so that is kind of how we tie how we thread all these things together. Now, there is a question of day to day. How do people, the individuals who in some ways are the most empowered ones, you know, the board sort of gets to call the shots at the high level, but the people who are actually executing are the employees or the people here on a day to day basis who have the, you know, the keys to the technical kingdom.
And there I think that the answer looks a lot like, well, how does any company's values get actualized? Right. I think that a lot of that comes down to that. You need people who are here because they really believe in that mission and they believe in the charter and that they are willing to take actions that maybe are worse for them but are better for the charter. And that is something that is really baked into the culture. And honestly, I think it is you know, I think that is one of the things that we really have to work to preserve as time goes on.
And that is a really important part of how we think about hiring people and bringing people into open air.
So there is people here, there is people here who could speak up and say, like, hold on a second, this is totally against what we stand for culture wise.
Yeah, yeah, for sure. I mean, I think that that we actually have I think that is like a pretty important part of of how we operate and how we have even again with designing the charter and designing open Alpay in the first place, that there has been a lot of conversation with employees here and a lot of times where employees said, wait a second, this seems like it is going in the wrong direction and let us talk about it. And so I think one thing that is that is I think a really and, you know, here is here is actually one thing that I think is very unique about us as a small company is that if you are at a massive tech giant, that is a little bit hard for someone who is a line employee to go and talk to the CEO and say, I think that we are doing this wrong.
And, you know, you look at companies like Google that have had some collective action from employees to, you know, make ethical change around things like Mavin. And so maybe there are mechanisms that other companies that work but here are super easy for anyone to pull me aside, to pull Sammis ideology aside and people do it all the time.
One of the interesting things in the charter is this idea that it would be great if you could try to describe or untangle switching from competition to collaboration. And late stage development was really interesting, this dance between competition and collaboration. How do you think about that?
Yeah, assuming you can actually do the technical side of development. I think there is going to be two key problems with figuring out how do you actually deploy it, make it go? Well, the first one of these is the run up to building the first ajai. You look at how self-driving cars are being developed and it is a competitive race.
And the thing that always happens in a competitive race is that you have huge amounts of pressure to get rid of safety.
And so that is one thing we are very concerned about, right, is that people, multiple teams figuring out we can actually get there. But, you know, if we took the slower path that is more guaranteed to be safe, we will lose. And so we are going to take the fast path. And so the more that we can both ourselves be in a position where we do not generate that competitive race, where we say if the race is being run and that, you know, someone else is further ahead than we are, we are not going to try to to leapfrog.
we are going to actually work with them. We will help them succeed. As long as what they are trying to do is to fulfill our mission, then we are good. We do not have to build it yourselves. And I think that is a really important commitment from us. But it can not just be unilateral. Right. I think it is really important that other players were serious about building ajai, make similar commitments. Right. And I think that that, you know, again, to the extent that everyone believes that aid should be something to benefit everyone, then it actually really should not matter which company builds it.
And we should all be concerned about the case where we just race so hard to get there that something goes wrong.
So what role do you think government, our favorite entity, has in setting policy and rules about this domain, from research to the development to early stage to late stage and energy development?
So I think that, first of all, is really important. The government's in their right in some way, shape or form. You know, at the end of the day, we are talking about building technology that will shape how the world operates and that there needs to be government as part of that answer. And so that is why we have we have done a number of different congressional testimony as we interact with a number of different lawmakers. And, you know, right now, a lot of our message to them is that it is not the time for regulation, it is the time for measurement that our main policy recommendation is that people and, you know, the government does this all the time with bodies like next spend time trying to figure out just where the technology is, how fast it is moving, and can really become literate and up to speed with respect to what to expect.
So I think that today the answer really is about about about measurement. And I think that there will be a time and place where that will change. And I think it is a little bit hard to predict exactly what what exactly that trajectory should look like.
So there will be a point at which regulation federal in the United States, the government steps in and and helps be the I do not want to say the adult in the room to make sure that there is strict rules, maybe conservative rules that nobody can cross.
Well, I think there is there is kind of maybe two two angles to it. So today, with narrow applications that I think there are already existing bodies that are responsible and should be responsible for regulation. You think about, for example, with self-driving cars that you want the you know, the National Highway I. Netzer. Exactly. To be very good. That that makes sense. Right. That basically what we are saying is that we are going to have these technological systems that are going to be performing applications that humans already do great.
We already have ways of thinking about standards and safety for those. So I think actually empowering those regulators today is also pretty important. And then I think for for ajai, you know, that there is going to be a point where we will have better answers. And I think that maybe a similar approach of first measurement and start thinking about what the rules should be, I think it is really important that we do not prematurely squash, you know, progress.
I think it is very easy to kind of smother the abiding field. And I think that is something to really avoid. But I do not think it is the right way of doing it is to say, let us just try to be ahead and not involve all these other stakeholders. So you have recently released a paper on two language modeling, but did not release the full model because you had concerns about the possible negative effects of the availability of such model. it is outside of just that decision is super interesting because of the discussion as at a societal level.
The discourse it creates is fascinating in that aspect. But if you think about the specifics here at first, what are some negative effects that you envisioned? And, of course, what are some of the positive effects?
Yeah, so again, I think to zoom out like the way that we thought about GP2 is that with language modeling, we are clearly on a trajectory right now where we scale up our models and we get qualitatively better performance. Right to itself was actually just a scale up of a model that we have released in the previous June. Right. We just ran it at a much larger scale and we got these results. we are suddenly starting to write coherent prose, which was not something we would seen previously.
And what are we doing now? Well, we are going to scale up to by 10x, by 100 X by thousand X, and we do not know what we are going to get. And so it is very clear that the model that we that we released last June, you know, I think it is kind of like it is a good academic to say it is not something that we think is something that can really have negative applications or, you know, to the extent that we can do the positive of people being able to play with it is is, you know, far, far outweighs the possible harms you.
Fast forward to not to, but you 20 and you think about what that is going to be like. And I think that the capabilities are going to be substantive. And so there needs to be a point in between the two where you say this is something where we are drawing the line and that we need to start thinking about the safety aspects.
And I think for too, we could have gone either way. And in fact, when we had conversations internally that we had a bunch of pros and cons and it was not clear which one, which one outweighed the other. And I think that when we announced that, hey, we decide not to release this model, then there was a bunch of conversation where various people said, it is so obvious that you should have just released it. There are other people said it is so obvious you should not have released it.
And I think that that almost definitely means that holding it back was the correct decision. Right. If it is if there is if it is not obvious whether something is beneficial or not, you should probably default to caution. And so I think that that the overall landscape for how we think about it is that this decision could have gone either way. there is great arguments in both directions. But for future models down the road and possibly sooner than you would expect because, you know, scaling these things up does not actually take that long.
Those ones you are definitely not going to want to release into the wild. And so I think that we almost feel that this is a test case.
And to see can we even design, you know, how do you have a society where how do you have a system that goes from having no concept of responsible disclosure, where the mere idea of not releasing something for safety reasons is unfamiliar to a world where you say, OK, we have a powerful model, let us at least think about it, let us go through some process and think about the security community.
It took them a long time to design responsible disclosure. You know, you think about this question of, well, I have a security exploit. I send it to the company. The company is like tries to prosecute me or just just ignores it. What do I do? Right. And so, you know, the alternatives of oh, I just just always publish your exploits. That does not seem good either. Right. And so it really took a long time and took this this it was bigger than any individual right is really about building a whole community that believe that.
OK, we will have this process where you sent to the company. You know, if they do not act at a certain time, then you can go public and you are not a bad person. you have done the right thing. And I think that in a high part of the responsibility to just proves that we do not have any concept of this. So that is the high level picture. And so I think that I think this was this was a really important move to make.
And we could have maybe delayed it for TV3. But I am really glad we did it for you, too. And so now you look contribute to itself and you think about the substance of, OK, what a potential negative applications. So you have this model that is been trained on the Internet, which, you know, it is also going to be a bunch of very biased data, a bunch of, you know, very offensive content in there. And you can ask it to generate content for you on basically any topic.
Right. You just give it a prompt and they will just start start writing and our content, like you see on the Internet, you know, even down to you like saying advertisement in the middle of some of its generations.
And you think about the possibilities for generating fake news or abusive content.
And, you know, it is interesting seeing what people have done with, you know, we released a smaller version of GP2 and that people have done things like try to generate I, you know, take my own Facebook message history and generate more Facebook messages like me and people generating fake politician content. Or, you know, there is a bunch of things there where you at least have.
To think, is this going to be good for the world? there is the flip side, which is I think that there is a lot of awesome applications that we really want to see, like creative applications in terms of if you have sci fi authors that can work with this tool and come up with cool ideas like that, seems that seems awesome.
If we can write better sci fi through the use of these tools and we have actually had a bunch of people writing to us asking, hey, can we use it for, you know, a variety of different creative applications?
The positive are actually pretty easy to imagine there, if you know the usual and the applications are really interesting.
But let us go there. it is kind of interesting to think about a world where we look at Twitter.
Where they just fake news, but smarter and smarter bots being able to spread it in an interesting, complex networking way in information that just floods out as regular human beings with our original thoughts.
So what are your views of this world? Twenty. Right.
What do you how do we think about again, it is like one of those things about in the 50s trying to describe the the Internet or the smartphone. What do you think about that world, the nature of information? And one possibility is that we will always try to design systems that identify robot versus human and will do so successfully. And so we will authenticate that we are still human. And the other world is that we just accept the fact that we are swimming in a sea of fake news and just learn to swim there.
Well, have you ever seen the, you know, popular meme of of robot with a physical physical arm and pen clicking? The I am not a robot button.
Yeah, I think I think the truth is that that really trying to distinguish between robot and human is a losing battle.
Ultimately, you think it is a losing battle? I think it is a losing battle ultimately. Right. I think that that is that in terms of the content, in terms of the actions that you could take, I mean, think about how capturers have gone. Right. The capturers used to be a very nice, simple use of this image. All of our OCR is terrible. You put a couple of of artifacts in it. You know, humans are going to be able to tell what what it is an AI system would be able to do today, like I could barely do.
Captious Yeah. And I think that this is just kind of where we are going, I think captures where we are a moment in time thing. And as A.I. systems become more powerful, that they are being human capabilities that can be measured in the very easy, automated way that the AIS will not be capable of, I think that is just like it is just an increasingly hard technical battle.
But it is not that all hope is lost. Right.
And you think about how do we already authenticate ourselves. Right. You know, we have systems. We have Social Security numbers. If you are in the US or, you know, you have you have, you know, ways of identifying individual people and having real world identity tied to to digital identities seems like a step towards, you know, authenticating the source of content rather than the content itself. Now, there are problems with that.
How can you have privacy and anonymity in a world where the only content you can really trust is we are the only way you can trust content is by looking at where it comes from. And so I think that building out good reputation networks may be maybe one possible solution. But, yeah, I think that this this question is it is not an obvious one.
And I think that we, you know, maybe sooner than we think we will be in a world where, you know, today I often will read a tweet and be like, I feel like a real human wrote this or do I feel like this is like genuine? I feel like I can kind of judge the content a little bit.
And I think in the future it just will not be the case. You look at, for example, the FCC comments on net neutrality. It came out later that millions of those were auto generated and that the researchers were able to do various statistical techniques to do that.
What do you do in a world where the statistical techniques do not exist? it is just impossible to tell the difference between humans and ice. And in fact, the the the most persuasive arguments are written by by AI, all that stuff. it is not sci fi anymore. you will get to making a great argument for why recycling is bad for the world.
You got to read that, huh? you are right.
Yeah, that is that is quite interesting. I mean, ultimately, it boils down to the physical world being the last frontier of proving he said, like basically networks of people, humans vouching for humans in the physical world and somehow the authentication ends there.
I mean, if I had to ask you, I mean, you are way too eloquent for a human. So if I had to ask you to authenticate, they prove how do I know you are not a robot and how do you know I am not a robot?
Yeah, I think that is so far. Where were this in this space, this conversation we just had the physical movements we did is the biggest gap between us and A.I. systems is the physical manipulation. So maybe that is the last frontier. Well, here is another question is, is you know why?
Why is why is solving this problem important? Right. Like what aspects are really important to us? I think that probably where we will end up is will hone in on what do we really want out of knowing if we are talking to a human. And and I think that, again, this comes down to identity. And so I think that that the Internet of the future, I expect to be one that will have lots of agents out there that will interact with you.
But I think that the question of is this, you know, a real flesh and blood human or is this an automated system? It may actually just be less important.
let us actually go there. it is too as impressive. And that is a big twenty. Why is it so bad that all my friend. Are GP2 20, why is it so why is it so important on the Internet, do you think, to interact with only human beings?
Why can not we live in a world where ideas can come from models trained on human data?
Yeah, I think this is I think it is actually a really interesting question. This comes back to the how do you even picture a world with some new technology? And I think that that one thing that I think is important is, is, you know, I would say honesty. And I think that if you have, you know, almost the Turing Test style sense sense of of technology, you have eyes that are pretending to be humans and deceiving you.
I think that is you know, that feels like a bad thing. Right? I think that it is really important that we feel like we are in control of our environment. Right. That we understand who we are interacting with and if it is an eye or a human, that that is not something that we are being deceived about.
But I think that the flip side of can I have as meaningful of an interaction with an A.I. as I can with a human? Well, I actually think you can turn to sci fi. And her, I think, is a great example of asking this very question. One thing I really love about her is it really starts out almost by asking how meaningful are human virtual relationships. Right.
And and then you have a human who has a relationship with an AI and that you really start to be drawn into that. Right. That all of your emotional buttons get triggered in the same way as if there was a real human that was on the other side of that phone.
And so I think that this is one way of thinking about it is that I think that we can have meaningful interactions and that if there is a funny joke, sometimes it does not really matter if it was written by a human A.I. But what you do not want, and I think we should really draw hard lines is deception. And I think that as long as we are in a world where, you know, why, why do we build A.I. systems at all? Right.
The reason we want to build them is to enhance human lives, to make humans be able to do more things, to have human humans feel more fulfilled. And if we can build AI systems that do that, you know, sign me up.
So the process of language modeling. How far do you think it takes us?
let us look and move her. Do you think dialogue, natural language, conversation as formulated by the Turing Test, for example, do you think that process could be achieved through this kind of unsupervised language modeling?
So I think the Turing test in its real form is not just about language. Right? it is really about reasoning. To write that to really pass the Turing test, I should be able to teach calculus to whoever's on the other side and have it really understand calculus and be able to, you know, go in and solve new calculus problems.
And so I think that to really solve the Turing test, we need more than what we are seeing with language models. We need some way of plugging in reasoning. Now, how different will that be from what we already do? that is an open question, right? Might be that we need some sequence of totally radical new ideas, or it might be that we just need to kind of shape our existing systems in a slightly different way.
But I think that in terms of how far our language model will go, it is already gone way further than many people would have expected. Right. I think that things like and I think there is a lot of really interesting angles to poke in terms of how much does to understand physical world like, you know, you read a little bit about fire underwater in in Djibouti, too. So it is like, OK, maybe he does not quite understand what these things are.
But at the same time, I think that you also see various things like smoke coming from flame and, you know, a bunch of these things that GBC to it has no body, it has no physical experience.
it is just statically read data. And I think that I think that the answer is like, we do not know yet these questions, though, we are starting to be able to actually ask them to physical systems that the real systems that exist. And that is very exciting.
Do you think what is your intuition? Do you think if you just scale language modeling, like significantly scale, that reasoning can emerge from the same exact mechanisms?
I think it is unlikely that if we just scale GBG to that will have reasoning and the full fledged way. And I think that there is like, you know, the type signatures a little bit wrong. Right? Like there is something we do with that we call thinking right. Where we spend a lot of compute, like a variable amount of compute to get to better answers. Right. I think a little bit harder. I get better answer and that that kind of type signature is not quite encoded in LGBT.
Right. Will kind of like it is been a long time and it is like evolutionary history making in all this information, getting very, very good at this predictive process.
And then at runtime I just kind of do one forward pass and so and I am able to generate stuff. And so, you know, there might be small tweaks to what we do in order to get the type signature. Right. For example. Well, you know, it is not really one forward pass, right. You know, you generate some assemble and so maybe you generate like a whole sequence of thoughts and you only keep, like, the last bit or something.
Right. But I think that at the very least, I would expect you have to make changes like that. Yeah.
Yeah. Just exactly how we you said. Think is the process of generating thought by thought in the same kind of way, like you said, keep the last bit the thing that we converge towards.
Yeah, I think there is there is another piece which is which is interesting, which is this out of distribution generalisation. Right. That like thinking somehow that is a do that. Right, that we have not experienced a thing and yet somehow we just kind of keep refining our mental model of it. This is, again, something that feels tied to whatever reasoning is.
And maybe it is a small tweak to what we do. Maybe it is many ideas and will take us many decades.
Yeah. So the assumption there generalization out of distribution is that it is possible to create new new ideas.
You know, it is possible that nobody's ever created new ideas and then was scaling to did you 20 you would you would essentially generalize to all possible thoughts as he was going to have to just to play devil's advocate.
I mean, how many how many new new story ideas have we come up with? that is Shakespeare, right?
Yeah, exactly.
it is just all different forms of love and drama and so on. OK, not sure if you read, but a lesson. A recent blog post by Sutton.
I have.
He basically says something that echoes some of the ideas that you have been talking about, which is, he says the biggest lesson that can be read from 70 years of research is that general methods of leverage computation are ultimately going to ultimately win out. Do you agree with this basically of and open in general about the ideas you are exploring, about coming up with methods? Well, there is tippity to modeling or whether it is open air five playing Dota or a general method is better than the more fine tuned expert tuned.
Method. Yeah, so I think that well, one thing that I think was really interesting about the reaction to that blog post was that a lot of people have read this as saying that compute is all that matters and it is a very threatening idea. Right. And I do not think it is a true idea either. it is very clear that we have algorithmic ideas that have been very important for making progress and to really build ajai. You want to push as far as you can on the computational scale and you want to push as far as you can on human human ingenuity.
And so I think you need both. But I think the way that you phrased the question is actually very good, right. That it is really about what kind of ideas should we be striving for. And absolutely, if you can find a scalable idea for more computing to put more data into it, it gets better like that. that is the real Holy Grail.
And so I think that the answer to the question, I think is yes, that that that is really how we think about it. And that part of why we are excited about the power of deep learning, the potential for building ajai is because we look at the systems that exist in the most successful AI systems and we realize that you scale those up, they are going to work better. And I think that that scalability is something that really gives us hope for being able to build transformative systems.
So I will tell you, this is partially an emotional you know, I think that response that people often have as computers so important for state of the art performance, you know, individual developers, maybe a 13 year old sitting somewhere in Kansas or something like that. You know, they are sitting they might not even have a GPU and or maybe have a single GPU or ten eighty or something like that.
And there is this feeling like, well, how can I possibly compete or contribute to this world of I if scale so important.
So if you can comment on that and in general, do you think we need to also in the future focus on democratizing compute resources more, more or as much as we democratize the algorithms?
Also, the way that I think about it is that there is the space of of possible progress. there is a space of ideas and sort of systems that will work, that will move us forward. And there is a portion of that space and to some extent increasingly significant portion of that space that does just require massive compute resources.
And for that that I think that the answer is kind of clear, that part of why we have the structure that we do is because we think it is really important to be pushing the scale and to be, you know, building as large clusters and systems.
But there is another portion of the space that is not about the large scale compute that these ideas that and again, I think that for the ideas to really be impactful and really shine who they should be, ideas that if you scale them up, would work way better than they do at small scale, but you can discover them without massive computational resources. And if you look at the history of of recent developments, you think about things like the gown or the VA, that these are ones that I think you could come up with them without having.
And, you know, in practice, people did come up with them without having massive, massive computational resources. I just talked to a good fellow.
But the thing is, the initial Gane produced pretty terrible results. Right?
So only because it was not a very specific it was only because they are smart enough to know that this is quite surprising in general, anything that they know. Do you see a world or is that too optimistic? And Dreamer like to imagine that the compute resources are something that is owned by governments and provided as utility?
Actually, to some extent, this this question reminds me of of blog posts from one of my former professors at Harvard, this guy, Matt, Matt Welsh, who is a systems professor. I remember sitting in his tenure talk. Right. And you know that he had literally just gotten tenure. He went to Google for the summer and then decided he was not going back to academia. Right. And that kind of in his blog post, he makes this point that, look, as a systems researcher, that I come to these cool system ideas.
Right. And kind of a little proof of concept. And the best thing I could hope for is that the people at Google or Yahoo!
Which was around at the time, will implement it and actually make it work at scale. Right. that is like the dream for me, right? I built the little thing and they the big thing that is actually working.
And for him, he said, I am done with that. I want to be the person who is who is actually doing the building and deploying. And I think that there is a similar dichotomy here. Right? I think that there are people who really actually find value. And I think it is a valuable thing to do to be the person who produces those ideas. Right. Who build the proof of concept.
And yeah, you do not get to generate the coolest possible Gane images, but you invented the gane. Right. And so that there is there is there is a real tradeoff there. And I think that is a very personal choice. But I think there is value in both sides.
So do you think creating ajai something or some new models would we would see echoes of the brilliance even at the prototype level, so you would be able to develop those ideas without scale, the initial sew seeds.
So to take a. And, you know, I always like to look at the examples that exist, right, look at real precedent. And so take a look at the June twenty eighteen model that we released that we scaled up to turn to. And you can see that at Smallscale, it set some records. This was the original Deepti. We actually had some some cool generations that were not nearly as amazing and really stunning as the two ones. But it was promising.
It was interesting. And so I think it is the case that with a lot of these ideas that you see promise that small scale. But there is an asterisk here, a very big asterisk, which is sometimes we see. Behaviors that emerge that are qualitatively different from anything we saw at small scale and that the original inventor of whatever algorithm looks at and says, I did not think I could do that. This is what we saw in Dota. Right.
So Pippo was was created by John Coleman, who is a researcher here. And and with with Doda. We basically just ran a at massive, massive scale and some tweaks in our store in order to make it work.
But fundamentally it is p0 at the core.
And we were able to get this long term planning, these these behaviors to really play out on a time scale that we just thought was not possible.
And John looked at that and was like, I did not think it could do that. that is what happens when you get three orders of magnitude more scale tested at.
Yeah, but it still has the same flavors of, you know, at least echoes of the expected billions, although I suspect with Deepti is scaled more and more, you might get surprising things. So, yeah, you are right. it is interesting that it is it is difficult to see how far an idea will go when it is scaled.
it is an open question also to that point with with Dota and people like I mean, here is here is a very concrete one, right? it is like it is actually one thing it is very surprising about Dota, but I think people do not really pay that much attention to is the degree of generalization our distribution that happens.
Right, that you have this A.I. that is trained against other bots for its entirety, the entirety of existence, decided to take a step back and you can not talk through it.
You know, a story of Dota, a story of leading up to open five and that passed. And what was the process of self playing. So out of training. Yeah, yeah.
Yeah. So so with his daughter Dota, it is a complex video game. And we started training. We started trying to solve Dota because we felt like this was a step towards the real world relative to other games like chess or go. Right. Those very cerebral games where you just kind of have this bored, very discrete moves.
Dota starts to be much more continuous time that you have this huge variety of different actions, that you have a 45 minute game with all these different units. And it is got a lot of messiness to it that really has not been captured by previous games. And famously, all of the hardcoded bots for Dota were terrible.
it is just impossible to write anything good for it because it is so complex. And so this seemed like a really good place to push what is the state of the art in reinforcement learning? And so we started by focusing on the one versus one version of the game. And and we are able to to solve that. we are able to beat the world champions. And that the that the learning, you know, the skill curve was this crazy exponential. Right. It was like constantly we were just scaling up that we were fixing bugs.
And, you know, that you look at the at the skill curve and it was really very, very smooth. One was actually really interesting to see how that, like, human iteration loop yielded very steady, exponential progress.
And to one side note, first of all, it is an exceptionally popular video game. The side effect is that there is a lot of incredible human experts at that video. Again, so the benchmark that you are trying to reach is very high. And the other can you talk about the approach that was used initially and throughout the training, these agents to play this game?
Yep. And so the approach that we used is self play. And so you have two agents that do not know anything. They battle each other. They discover something a little bit good, and now they both know it and they just get better and better and better without bound.
And that is a really powerful idea, right, that we then went from the one versus one version of the game and scaled up to four versus five. Right. So you think about kind of like with basketball, where you have this, like, team sport and you need to do all this coordination. And we were able to push the same idea, the same self play to to really get to the professional level at the full five versus five version of the game.
And and the things I think are really interesting here is that these agents, in some ways, they are almost like an insect like intelligence, right. Where, you know, they have a lot in common with how an insect is trained. Right. Insect kind of lives in this environment for very long time. You know, the answer to this insect, I have been around for a long time and had a lot of experience that gets baked into into into this agent.
And, you know, it is not really smart in the sense of a human right. it is not able to go in, learn calculus, but it is able to navigate its environment extremely well and able to handle unexpected things in the environment that is never seen before pretty well. And we see the same sort of thing with our Doda bots. Right? they are able to within this game, they are able to play against humans, which is something that never existed in its evolutionary environment, totally different play styles from humans versus the bots, and yet it is able to handle extremely well.
And that is something that I think was very surprising to us, was something that does not really emerge from what we have seen with Pippo at smaller scale, the kind of scale we are running, the stuff that was so, you know, I could take one hundred thousand CPU cores running with like hundreds of DPAs is probably about, you know, like, you know, something like hundreds of of years of experience going into this bot every single real day. And so that scale is massive and we start to see very different kinds of behaviors out of the algorithms that we all know and love.
You mentioned beat the world expert one by one, and then you did not were not able to win five five this year.
Yeah. At the best players in the world. So what is what is the comeback story? what is first of all, talk through that does exceptionally exciting event and what is what is the following months and this year look like.
Yeah. Yeah.
So well, one thing that is interesting is that, you know, we lose all the time, right. Because we because we so the Dota team at opening night, we played the bot against better players than our system all the time. Or at least we used to write like, you know, the first time we lost publicly was we went up on stage at the International and we played against some of the best teams in the world. And we ended up losing both games.
But we gave them a run for their money. Right. The both games were kind of 30 minutes. Twenty five minutes. And they went back and forth, back and forth, back and forth. And so I think that really shows that we are at the professional level. And that kind of looking at those games, we think that the queen could have gone a different direction and we could have could have had some wins. That was actually very encouraging for us.
And, you know, it is interesting because the international was at a fixed time. Right? So we knew exactly what day we were going to be playing and we pushed as far as we could as fast as we could. Two weeks later, we had about that had an 80 percent win rate versus the one that played at T'ai. So the march of progress, you know, you should think of as a snapshot rather than as an end state. And so, in fact, Will will be announcing our our finals pretty soon.
I actually think that we will announce our final match prior to this podcast being released.
So there should be we will be playing or playing against the the world champions. And, you know, for us, it is really less about like that. The way that we think about what is upcoming is the final milestone, the final competitive milestone for the project.
Right. That our goal in all of this is not really about beating humans at Dota. Our goal is to push the state of the art and reinforcement learning. And we have done that right. And we have actually learned a lot from our system. And that we have, you know, I think a lot of exciting next steps that we want to take. And so a final showcase of what we built. we are going to do this match. But for us, it is not really the success or failure to see, you know, do do we have the clean up go in our direction or against.
Where do you see the field of deep learning heading in the next few years? What do you see the work and reinforcement learning, perhaps hiding and.
More specifically, with Open Eye, all the exciting projects that you are working on. What is 12 19 hold for the massive scale scale?
I will put an asterisk on that and just say, you know, I think that it is about eight years plus scale. You need both.
So that is a really good point. So the question in terms of ideas, you have a lot of projects that are exploring different areas of intelligence. And the question is, when you when you think of scale, do you think about growing the scale of those individual projects or do you think about adding new projects and solitude?
If you are thinking of adding new projects or if you look at the past, what is the process of coming up with new projects, new ideas, you know?
So we really have a lifecycle of project here. So we start with a few people just working on a small scale idea. And language is actually a very good example of this. That is really, you know, one person here who was pushing on language for a long time. I mean, then you get signs of life. Right? And so this is like let us say, you know, with with the original GBG, we had something that was interesting and we said, OK, it is time to scale this right.
it is time to put more people on it, put more computational resources behind it. And and then we just kind of keep pushing and keep pushing at the end state is something that looks like Dota or robotics, where you have a large team of, you know, 10 or 15 people that are running things at very large scale and that you are able to really have material engineering and and and, you know, sort of machine learning science coming together to make systems that work and get material results that just would have been impossible otherwise.
So we do that whole life cycle. we have done a number of times, you know, typically end to end, probably to two years or so to do it. You know, the organization's been around for three years or so. Maybe we will find that we also have longer lifecycle projects, but we work up to those we have.
So so one one team that we were actually just starting, Éliane, I are kicking off a new team called the Reasoning Team, and that this is to really try to tackle how do you get neural networks to reason. And we think that this will be a long term project and one that we are very excited about in terms of reasoning.
Super exciting topic. What do you what kind of benchmarks, what kind of tests of reasoning do you envision what would if you said back, whatever drink and you would be impressed that the system is able to do something, what would that look like? they are improving. they are improving.
So some kind of logic and especially mathematical logic.
I think so. Right. And I think that there is there is there is kind of other problems that are dual to think improving in particular.
You know, you think about programming. I think about even like security analysis of code that these all kind of capture the same sorts of core reasoning and being able to do some out of distribution generalization. It would be quite exciting if OpenAir reasoning team was able to prove that P equals NP would be very nice.
It would be very, very exciting, especially if it turns out the pig will then be. That'll be interesting, too.
Maybe just it would be ironic and humorous.
Uh, so what problem stands out to you as the most exciting and challenging, impactful to the work for us as a community in general and for open eye this year? He mentioned reasoning. I think that is that is a heck of a problem. Yeah.
So I think reasoning is an important one. I think it is going to be hard to get good results in twenty nineteen. You know, again, just like we think about the life cycle takes time I think for twenty nineteen language modeling seems to be kind of on that ramp. Right.
it is at the point that we have a technique that works. We want to scale one hundred thousand actually. What happens. Awesome.
Do you think we are living in a simulation.
I think it is I think it is hard to have a real opinion about it. You know, it is actually interesting. I separate out things that I think can have like, you know, yield materially different predictions about the world from ones that are just kind of, you know, fun to speculate about. And I kind of view simulation. it is more like, is there a flying teapot between Mars and Jupiter? Like, maybe, but it is a little bit hard to know what that would mean for my life.
So there is something actionable. So some of the best work opening has done is in the field of reinforcement learning and some of the success of reinforcement learning come from being able to simulate the problem and trying to solve.
So do you have a hope for reinforcement for the future, reinforcement learning and for the future of simulation, like what we are talking about autonomous vehicles or any kind of system. Do you see that scaling up or will it be able to simulate systems and hence be able to create a simulator that echoes our real world and proving once and for all, even though you are denying it, that we are living in a simulation?
It seems that for questions. Right. So, you know, kind of at the core there of like, can we use simulation for self driving cars?
Take a look at our robotic system. Dactyl, right. That was trained in simulation using the Doda system, in fact. And it transfers to a physical robot. And I think everyone looks at our data system, OK, it is just a game. How are you ever going to escape to the real world? And the answer is, well, we did it with the physical robot that no one can program. And so I think the answer, a simulation goes a lot further than you think if you apply the right techniques to it.
Now, there is a question of, you know, ah, the being in that simulation going to going to wake up and have consciousness. I think that one seems a lot harder to take and reason about.
I think that, you know, you really should think about like where where exactly does human consciousness come from and our own self awareness.
And, you know, is it just that, like, once you have like a complicated enough neural net, you have to worry about the agents feeling pain. And, you know, I think there is like interesting speculation to do there. But but, you know, again, I think it is a little bit hard to know for sure.
Let me just keep the speculation, do you think, to create intelligence, general intelligence, you need one consciousness and two a body. Do you think any of those elements are needed or is intelligence something that is that is orthogonal to those?
I will stick to the kind of like the non grand answer first. Right.
So the non grand answer is just to look at, you know, what are we already making work you will get to? A lot of people would have said that even get these kinds of results. You need real world experience. You need a body, you need grounding. How are you supposed to reason about any of these things? How are you supposed to, like, even kind of know about smoke and fire and those things? If you have never experienced them and you do shows that you can actually go way further than that kind of reasoning would predict?
So I think that in terms of do need conscience, do we need a body, it seems the answer is probably not right, that we probably just continue to push kind of the systems we have. They already feel, general, they are not as competent or as general or able to learn as quickly as an ajai would. But, you know, there are at least like kind of Proteau ajai in some way, and they do not need any of those things now.
Now, let us move to the grand answer, which is, you know, if our neural net nets conscious already, would we ever know? How can we tell?
Right. Yeah. Here, here is where the speculation starts to become become, you know, at least interesting or fun and maybe a little bit disturbing, depending on where you take it. But it certainly seems that when we think about animals that there is some continuum of consciousness. You know, my cat, I think is is conscious in some way. Right. You know, not as conscious as a human. And you could imagine that you could build a little consciousness metre.
Right. You point a cat, gives you a little breathing point and assuming and gives you a much bigger reading.
What would happen if you point to one of those at a the neural net? And if you are training this massive simulation, do the neural nets feel pain?
You know, it becomes pretty hard to know that the answer is no and it becomes pretty hard to to really think about what that would mean if the answer were yes.
And it is very possible. You know, for example, you could imagine that maybe the reason that humans are have consciousness is because it is a it is a convenient computational shortcut. Right. If you think about it, if you have a being that wants to avoid pain, which seems pretty important to survive in this environment and wants to like, you know, eat food, then that may be the best way of doing it is to have a being that is conscious. Right.
That, you know, in order to succeed in the environment, you need to have those properties and how are you supposed to implement them? And maybe this this consciousness is a way of doing that. If that is true, then actually maybe we should expect that really competent reinforcement learning agents will also have consciousness.
But, you know, that is a big if. And I think there are a lot of other arguments that can make another directions.
I think it is a really interesting idea that even GP2 has some degree of consciousness, that something is actually not as crazy to think of, as useful to think about, as as we think about what it means to create intelligence of a dog, intelligence of the cat and the intelligence of human. So last question, do you think. We will ever fall in love, like in the movie her with an artificial intelligence system or an artificial intelligence system falling in love with a human.
I hope so. If there is any better way to end, it is on love. So Greg, thanks so much for talking to me. Thank you for having me.
